title,abstract,day,month,year
Bundling and Tumbling in Bacterial-inspired Bi-flagellated Soft Robots for Attitude Adjustment,"We create a mechanism inspired by bacterial swimmers, featuring two flexible flagella with individual control over rotation speed and direction in viscous fluid environments. Using readily available materials, we design and fabricate silicone-based helical flagella. To simulate the robot's motion, we develop a physics-based computational tool, drawing inspiration from computer graphics. The framework incorporates the Discrete Elastic Rod method, modeling the flagella as Kirchhoff's elastic rods, and couples it with the Regularized Stokeslet Segments method for hydrodynamics, along with the Implicit Contact Model to handle contact. This approach effectively captures polymorphic phenomena like bundling and tumbling. Our study reveals how these emergent behaviors affect the robot's attitude angles, demonstrating its ability to self-reorient in both simulations and experiments. We anticipate that this framework will enhance our understanding of the directional change capabilities of flagellated robots, potentially stimulating further exploration on microscopic robot mobility.",19,1,2024
Low-cost modular devices for on-road vehicle detection and characterisation,"Detecting and characterising vehicles is one of the purposes of embedded systems used in intelligent environments. An analysis of a vehicle characteristics can reveal inappropriate or dangerous behaviour. This detection makes it possible to sanction or notify emergency services to take early and practical actions. Vehicle detection and characterisation systems employ complex sensors such as video cameras, especially in urban environments. These sensors provide high precision and performance, although the price and computational requirements are proportional to their accuracy. These sensors offer high accuracy, but the price and computational requirements are directly proportional to their performance. This article introduces a system based on modular devices that is economical and has a low computational cost. These devices use ultrasonic sensors to detect the speed and length of vehicles. The measurement accuracy is improved through the collaboration of the device modules. The experiments were performed using multiple modules oriented to different angles. This module is coupled with another specifically designed to detect distance using previous modules speed and length data. The collaboration between different modules reduces the speed relative error ranges from 1 to 5, depending on the angle configuration used in the modules.",26,1,2024
Cross-platform impact of social media algorithmic adjustments on public discourse,"In the hypertrophic and uncharted information world, recommender systems are gatekeepers of knowledge. The evolution of these algorithms is usually an opaque process, but in February 2023, the recommender system of X, formerly Twitter, was altered by its chairman (Elon Musk) transparently, offering a unique study opportunity. This paper analyses the cross-platform impact of adjusting the platform's recommender system on public discourse. We focus on the account of Elon Musk and, for comparison, the account of the President of the United States (Joe Biden). Our results highlight how algorithm adjustments can boost content visibility, user engagement, and community involvement without increasing the engagement or involvement probabilities. We find that higher visibility can increase the influence on social dialogue but also backfire, triggering negative community reactions. Finally, our analysis offers insights to detect future less transparent changes to recommender systems.",29,1,2024
Cantera-Based Python Computer Program for Solving Steam Power Cycles with Superheating,"One of the main sources of electricity generation is power plants that use water (steam) to rotate turbines, which drive large electric generators. The steam can be generated from renewable or non-renewable energy sources, such as geothermal energy and nuclear fuels. Having an analysis tool for modeling the performance of such steam power plants can greatly help in reaching optimum designs, leading to less fuel consumption, reduced pollution, and cheaper electricity. It is further advantageous if such modeling tool is free to access, does not require many inputs from the user, and gives results in a very short time. These remarks establish a motivation for the current study. This article documents a computer code written in the Python programming language for numerically analysing the main processes in a steam power cycle with superheating. The code utilizes built-in thermodynamic properties for water in the open-source software package ""Cantera"". A validation case with a benchmarking example in the literature using an independent source of water properties suggests that the developed code is correct. The code can be viewed as an extension to the Python examples for thermodynamic and power generation applications. Cantera can handle both subcritical and supercritical types of superheating. In the subcritical superheating, the steam absolute pressure does not exceed 220.9 bar. In the supercritical superheating, water becomes in a special condition called supercritical fluid, with absolute pressures above 220.9 bar.",27,1,2024
Steel Plate Fault Detection using the Fitness Dependent Optimizer and Neural Networks,"Detecting faults in steel plates is crucial for ensuring the safety and reliability of the structures and industrial equipment. Early detection of faults can prevent further damage and costly repairs. This chapter aims at diagnosing and predicting the likelihood of steel plates developing faults using experimental text data. Various machine learning methods such as GWO-based and FDO-based MLP and CMLP are tested to classify steel plates as either faulty or non-faulty. The experiments produced promising results for all models, with similar accuracy and performance. However, the FDO-based MLP and CMLP models consistently achieved the best results, with 100% accuracy in all tested datasets. The other models' outcomes varied from one experiment to another. The findings indicate that models that employed the FDO as a learning algorithm had the potential to achieve higher accuracy with a little longer runtime compared to other algorithms. In conclusion, early detection of faults in steel plates is critical for maintaining safety and reliability, and machine learning techniques can help predict and diagnose these faults accurately.",26,1,2024
Scheduling of Distributed Applications on the Computing Continuum: A Survey,"The demand for distributed applications has significantly increased over the past decade, with improvements in machine learning techniques fueling this growth. These applications predominantly utilize Cloud data centers for high-performance computing and Fog and Edge devices for low-latency communication for small-size machine learning model training and inference. The challenge of executing applications with different requirements on heterogeneous devices requires effective methods for solving NP-hard resource allocation and application scheduling problems. The state-of-the-art techniques primarily investigate conflicting objectives, such as the completion time, energy consumption, and economic cost of application execution on the Cloud, Fog, and Edge computing infrastructure. Therefore, in this work, we review these research works considering their objectives, methods, and evaluation tools. Based on the review, we provide a discussion on the scheduling methods in the Computing Continuum.",20,1,2024
Self-healing Nodes with Adaptive Data-Sharding,"Data sharding, a technique for partitioning and distributing data among multiple servers or nodes, offers enhancements in the scalability, performance, and fault tolerance of extensive distributed systems. Nonetheless, this strategy introduces novel challenges, including load balancing among shards, management of node failures and data loss, and adaptation to evolving data and workload patterns. This paper proposes an innovative approach to tackle these challenges by empowering self-healing nodes with adaptive data sharding. Leveraging concepts such as self-replication, fractal regeneration, sentient data sharding, and symbiotic node clusters, our approach establishes a dynamic and resilient data sharding scheme capable of addressing diverse scenarios and meeting varied requirements. Implementation and evaluation of our approach involve a prototype system simulating a large-scale distributed database across various data sharding scenarios. Comparative analyses against existing data sharding techniques highlight the superior scalability, performance, fault tolerance, and adaptability of our approach. Additionally, the paper delves into potential applications and limitations, providing insights into the future research directions that can further advance this innovative approach.",19,1,2024
TALICS$^3$: Tape Library Cloud Storage System Simulator,"High performance computing data is surging fast into the exabyte-scale world, where tape libraries are the main platform for long-term durable data storage besides high-cost DNA. Tape libraries are extremely hard to model, but accurate modeling is critical for system administrators to obtain valid performance estimates for their designs. This research introduces a discrete event tape simulation platform that realistically models tape library behavior in a networked cloud environment, by incorporating real-world phenomena and effects. The platform addresses several challenges, including precise estimation of data access latency, rates of robot exchange, data collocation, deduplication/compression ratio, and attainment of durability goals through replication or erasure coding. The suggested simulator has the capability to compare the single enterprise configuration with multiple commodity library (RAIL) configurations, making it a useful tool for system administrators and reliability engineers. They can use the simulator to obtain practical and reliable performance estimates for their long-term, durable, and cost-effective cold data storage architecture designs.",18,1,2024
Reconfigurable nonreciprocal heat transport with natural bulk materials,"Non-reciprocity is increasingly scrutinised in contemporary physics and engineering, especially in the realm of heat transport. This concept opens up novel avenues for directional heat transport and thermal regulation. Nonetheless, the development of non-reciprocal thermal metamaterials confronts three primary challenges: a constrained operational temperature range and structural scale, considerable energy dissipation, and the limitation of non-reciprocal effects due to external parameters that are not freely adjustable. To surmount these hurdles, we propose a reconfigurable approach to non-reciprocal heat transport. The design features a simple asymmetrical structure, with a flat panel made of natural, evenly-distributed material, positioned vertically on one side of a central horizontal board.We employ natural convection on the vertical plate to facilitate non-reciprocal heat transport. The reconfigurability of non-reciprocal heat transport is achieved by adjusting the number of vertical plates and their thermal conductivity. Theoretical computations of the rectification ratio are employed to quantify and forecast the non-reciprocal effect, corroborated by simulations and empirical studies. We also establish correlations between the rectification ratio and various parameters, including the vertical plates' positioning and height, ambient temperature, and the temperature differential between heat sources. Control over multiple parameters can efficaciously widen the scope of non-reciprocal control and streamline experimental procedures. Furthermore, our research exclusively utilises the natural convection of air for non-reciprocal heat transport, obviating the need for Supplementary energy sources and markedly enhancing energy efficiency.",6,1,2024
Experimental Evaluation of the PHP's cURL Library Performance,"cURL (libcurl) is a popular and widely used library distributed with the php interpreter. It allows php applications to connect to and communicate with external resources (servers) by using wide variety of communication protocols. In most cases it is the preferred way of consuming external REST web services. Programmers usually use it for granted without even thinking of any performance issues. During an experimental analysis of the Hadoop's WebHDFS API throughput, it has been noted that read (download) speed from WebHDFS reduces with increasing the file size. However, this issue does not happen when writing to WebHDFS. Since the communication between the php application and the WebHDFS API is handled by the php's cURL library, then the cause of the download speed decrease could be either the cURL library itself or the API. This paper presents a series of experimental analyses aiming to determine the cause of the download speed decrease in previous experiments - whether it is the WebHDFS API or the php's cURL library. Both parties are tested in multiple ways separately and independently of each other. Results clearly prove (in two different ways) that the cause of the download speed decrease is the php's cURL library itself, not the consumed API.",5,1,2024
Dynamic Model Switching for Improved Accuracy in Machine Learning,"In the dynamic landscape of machine learning, where datasets vary widely in size and complexity, selecting the most effective model poses a significant challenge. Rather than fixating on a single model, our research propels the field forward with a novel emphasis on dynamic model switching. This paradigm shift allows us to harness the inherent strengths of different models based on the evolving size of the dataset.Consider the scenario where CatBoost demonstrates exceptional efficacy in handling smaller datasets, providing nuanced insights and accurate predictions. However, as datasets grow in size and intricacy, XGBoost, with its scalability and robustness, becomes the preferred choice.Our approach introduces an adaptive ensemble that intuitively transitions between CatBoost and XGBoost. This seamless switching is not arbitrary; instead, it's guided by a user-defined accuracy threshold, ensuring a meticulous balance between model sophistication and data requirements. The user sets a benchmark, say 80% accuracy, prompting the system to dynamically shift to the new model only if it guarantees improved performance.This dynamic model-switching mechanism aligns with the evolving nature of data in real-world scenarios. It offers practitioners a flexible and efficient solution, catering to diverse dataset sizes and optimising predictive accuracy at every juncture. Our research, therefore, stands at the forefront of innovation, redefining how machine learning models adapt and excel in the face of varying dataset dynamics.",31,1,2024
Immersed in Reality Secured by Design -- A Comprehensive Analysis of Security Measures in AR/VR Environments,"Virtual reality and related technologies such as mixed and augmented reality have received extensive coverage in both mainstream and fringe media outlets. When the subject goes to a new AR headset, another AR device, or AR glasses, the talk swiftly shifts to the technical and design details. Unfortunately, no one seemed to care about security. Data theft and other forms of cyberattack pose serious threats to virtual reality systems. Virtual reality goggles are just specialist versions of computers or Internet of Things devices, whereas virtual reality experiences are software packages. As a result, AR systems are just as vulnerable as any other Internet of Things (IoT) device we use on a daily basis, such as computers, tablets, and phones. Preventing and responding to common cybersecurity threats and assaults is crucial. Cybercriminals can exploit virtual reality headsets just like any other computer system. This paper analysis the data breach induced by these assaults could result in a variety of concerns, including but not limited to identity theft, the unauthorized acquisition of personal information or network credentials, damage to hardware and software, and so on. Augmented reality (AR) allows for real-time monitoring and visualization of network activity, system logs, and security alerts. This allows security professionals to immediately identify threats, monitor suspicious activities, and fix any issues that develop. This data can be displayed in an aesthetically pleasing and intuitively structured format using augmented reality interfaces, enabling for faster analysis and decision-making.",30,1,2024
Predicting SSH keys in Open SSH Memory dumps,"As the digital landscape evolves, cybersecurity has become an indispensable focus of IT systems. Its ever-escalating challenges have amplified the importance of digital forensics, particularly in the analysis of heap dumps from main memory. In this context, the Secure Shell protocol (SSH) designed for encrypted communications, serves as both a safeguard and a potential veil for malicious activities. This research project focuses on predicting SSH keys in OpenSSH memory dumps, aiming to enhance protective measures against illicit access and enable the development of advanced security frameworks or tools like honeypots. This Masterarbeit is situated within the broader SmartVMI project, and seeks to build upon existing research on key prediction in OpenSSH heap dumps. Utilizing machine learning (ML) and deep learning models, the study aims to refine features for embedding techniques and explore innovative methods for effective key detection based on recent advancements in Knowledge Graph and ML. The objective is to accurately predict the presence and location of SSH keys within memory dumps. This work builds upon, and aims to enhance, the foundations laid by SSHkex and SmartKex, enriching both the methodology and the results of the original research while exploring the untapped potential of newly proposed approaches. The current thesis dives into memory graph modelization from raw binary heap dump files. Each memory graph can support a range of embeddings that can be used directly for model training, through the use of classic ML models and graph neural network. It offers an in-depth discussion on the current state-of-the-art in key prediction for OpenSSH memory dumps, research questions, experimental setups, programs development, results as well as discussing potential future directions.",24,1,2024
The Security Performance Analysis of Blockchain System Based on Post-Quantum Cryptography -- A Case Study of Cryptocurrency Exchanges,"The current blockchain system for cryptocurrency exchanges primarily employs elliptic curve cryptography (ECC) for generating key pairs in wallets, and elliptic curve digital signature algorithms (ECDSA) for generating signatures in transactions. Consequently, with the maturation of quantum computing technology, the current blockchain system faces the risk of quantum computing attacks. Quantum computers may potentially counterfeit signatures produced by ECDSA. Therefore, this study analyzes the vulnerabilities of the current blockchain system to quantum computing attacks and proposes a post-quantum cryptography (PQC)-based blockchain system to enhance security by addressing and improving each identified weakness. Furthermore, this study proposes PQC-based wallets and PQC-based transactions, utilizing PQC digital signature algorithms to generate PQC-based signatures for the inputs in PQC-based transactions, thereby preventing signatures from being counterfeited by quantum computing. Experimental results demonstrate that the efficiency of the Dilithium algorithm, a PQC digital signature algorithm, in producing wallets, generating signatures, and verifying signatures surpasses that of ECDSA in the current blockchain system. Furthermore, the Dilithium algorithm also exhibits a higher security level.",23,1,2024
The Division Problem of Chances,"In frequently repeated matching scenarios, individuals may require diversification in their choices. Therefore, when faced with a set of potential outcomes, each individual may have an ideal lottery over outcomes that represents their preferred option. This suggests that, as people seek variety, their favorite choice is not a particular outcome, but rather a lottery over them as their peak for their preferences.We explore matching problems in situations where agents' preferences are represented by ideal lotteries. Our focus lies in addressing the challenge of dividing chances in matching, where agents express their preferences over a set of objects through ideal lotteries that reflect their single-peaked preferences.We discuss properties such as strategy proofness, replacement monotonicity, (Pareto) efficiency, in-betweenness, non-bossiness, envy-freeness, and anonymity in the context of dividing chances, and propose a class of mechanisms called URC mechanisms that satisfy these properties. Subsequently, we prove that if a mechanism for dividing chances is strategy proof, (Pareto) efficient, replacement monotonic, in-between, non-bossy, and anonymous (or envy free), then it is equivalent in terms of welfare to a URC mechanism.",20,1,2024
"Quantifying Lifetime Productivity Changes: A Longitudinal Study of 325,000 Late-Career Scientists","This study focuses on persistence in research productivity over the course of an individual's entire scientific careers. We track 'late-career' scientists (N=324,463) in 16 STEMM disciplines (science, technology, engineering, mathematics, and medicine) from 38 OECD countries for up to five decades. We examine the details of their mobility patterns between the top, middle, and bottom productivity classes. Methodologically, we turn a large-scale publication and citation bibliometric dataset into a comprehensive, longitudinal data source for research on careers in science. The global science system emerges as highly immobile: 60% of global top performers continue their careers as top performers and half of global bottom performers as bottom performers. Jumpers-Up and Droppers-Down are extremely rare in science. Our regression analyses show that productivity is highly path dependent: for all disciplines examined, there is a single most important predictor of being a top performer: being a top performer at an earlier career stage.",19,1,2024
Impact of Topography and Climate on Post-fire Vegetation Recovery Across Different Burn Severity and Land Cover Types through Machine Learning,"Wildfire significantly disturb ecosystems by altering forest structure, vegetation ecophysiology, and soil properties. Understanding the complex interactions between topographic and climatic conditions in post-wildfire recovery is crucial. This study investigates the interplay between topography, climate, burn severity, and years after fire on vegetation recovery across dominant land cover types (evergreen forest, shrubs, and grassland) in the Pacific Northwest region. Using Moderate Resolution Imaging Spectroradiometer data, we estimated vegetation recovery by calculating the incremental enhanced vegetation index (EVI) change during post-fire years. A machine learning technique, random forest (RF), was employed to map relationships between the input features (elevation, slope, aspect, precipitation, temperature, burn severity, and years after fire) and the target (incremental EVI recovery) for each land cover type. Variable importance analysis and partial dependence plots were generated to understand the influence of individual features. The observed and predicted incremental EVI values showed good matches, with R2 values of 0.99 for training and between 0.89 and 0.945 for testing. The study found that climate variables, specifically precipitation and temperature, were the most important features overall, while elevation played the most significant role among the topographic factors. Partial dependence plots revealed that lower precipitation tended to cause a reduction in vegetation recovery for varying temperature ranges across land cover types. These findings can aid in developing targeted strategies for post-wildfire forest management, considering the varying responses of different land cover types to topographic, climatic, and burn severity factors.",16,1,2024
"A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming","This paper offers an insightful examination of how currently top-trending AI technologies, i.e., generative artificial intelligence (Generative AI) and large language models (LLMs), are reshaping the field of video technology, including video generation, understanding, and streaming. It highlights the innovative use of these technologies in producing highly realistic videos, a significant leap in bridging the gap between real-world dynamics and digital creation. The study also delves into the advanced capabilities of LLMs in video understanding, demonstrating their effectiveness in extracting meaningful information from visual content, thereby enhancing our interaction with videos. In the realm of video streaming, the paper discusses how LLMs contribute to more efficient and user-centric streaming experiences, adapting content delivery to individual viewer preferences. This comprehensive review navigates through the current achievements, ongoing challenges, and future possibilities of applying Generative AI and LLMs to video-related tasks, underscoring the immense potential these technologies hold for advancing the field of video technology related to multimedia, networking, and AI communities.",30,1,2024
VN-Net: Vision-Numerical Fusion Graph Convolutional Network for Sparse Spatio-Temporal Meteorological Forecasting,"Sparse meteorological forecasting is indispensable for fine-grained weather forecasting and deserves extensive attention. Recent studies have highlighted the potential of spatio-temporal graph convolutional networks (ST-GCNs) in predicting numerical data from ground weather stations. However, as one of the highest fidelity and lowest latency data, the application of the vision data from satellites in ST-GCNs remains unexplored. There are few studies to demonstrate the effectiveness of combining the above multi-modal data for sparse meteorological forecasting. Towards this objective, we introduce Vision-Numerical Fusion Graph Convolutional Network (VN-Net), which mainly utilizes: 1) Numerical-GCN (N-GCN) to adaptively model the static and dynamic patterns of spatio-temporal numerical data; 2) Vision-LSTM Network (V-LSTM) to capture multi-scale joint channel and spatial features from time series satellite images; 4) a GCN-based decoder to generate hourly predictions of specified meteorological factors. As far as we know, VN-Net is the first attempt to introduce GCN method to utilize multi-modal data for better handling sparse spatio-temporal meteorological forecasting. Our experiments on Weather2k dataset show VN-Net outperforms state-of-the-art by a significant margin on mean absolute error (MAE) and root mean square error (RMSE) for temperature, relative humidity, and visibility forecasting. Furthermore, we conduct interpretation analysis and design quantitative evaluation metrics to assess the impact of incorporating vision data.",26,1,2024
Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action Classification,"Tactile signals collected by wearable electronics are essential in modeling and understanding human behavior. One of the main applications of tactile signals is action classification, especially in healthcare and robotics. However, existing tactile classification methods fail to capture the spatial and temporal features of tactile signals simultaneously, which results in sub-optimal performances. In this paper, we design Spatio-Temporal Aware tactility Transformer (STAT) to utilize continuous tactile signals for action classification. We propose spatial and temporal embeddings along with a new temporal pretraining task in our model, which aims to enhance the transformer in modeling the spatio-temporal features of tactile signals. Specially, the designed temporal pretraining task is to differentiate the time order of tubelet inputs to model the temporal properties explicitly. Experimental results on a public action classification dataset demonstrate that our model outperforms state-of-the-art methods in all metrics.",21,1,2024
Security-Sensitive Task Offloading in Integrated Satellite-Terrestrial Networks,"With the rapid development of sixth-generation (6G) communication technology, global communication networks are moving towards the goal of comprehensive and seamless coverage. In particular, low earth orbit (LEO) satellites have become a critical component of satellite communication networks. The emergence of LEO satellites has brought about new computational resources known as the \textit{LEO satellite edge}, enabling ground users (GU) to offload computing tasks to the resource-rich LEO satellite edge. However, existing LEO satellite computational offloading solutions primarily focus on optimizing system performance, neglecting the potential issue of malicious satellite attacks during task offloading. In this paper, we propose the deployment of LEO satellite edge in an integrated satellite-terrestrial networks (ISTN) structure to support \textit{security-sensitive computing task offloading}. We model the task allocation and offloading order problem as a joint optimization problem to minimize task offloading delay, energy consumption, and the number of attacks while satisfying reliability constraints. To achieve this objective, we model the task offloading process as a Markov decision process (MDP) and propose a security-sensitive task offloading strategy optimization algorithm based on proximal policy optimization (PPO). Experimental results demonstrate that our algorithm significantly outperforms other benchmark methods in terms of performance.",20,1,2024
Computation of leaky waves in layered structures coupled to unbounded media by exploiting multiparameter eigenvalue problems,"We present a semi-analytical approach to compute quasi-guided elastic wave modes in horizontally layered structures radiating into unbounded fluid or solid media. This problem is of relevance, e.g., for the simulation of guided ultrasound in embedded plate structures or seismic waves in soil layers over an elastic half-space. We employ a semi-analytical formulation to describe the layers, thus discretizing the thickness direction by means of finite elements. For a free layer, this technique leads to a well-known quadratic eigenvalue problem for the mode shapes and corresponding horizontal wavenumbers. Rigorously incorporating the coupling conditions to account for the adjacent half-spaces gives rise to additional terms that are nonlinear in the wavenumber. We show that the resulting nonlinear eigenvalue problem can be cast in the form of a multiparameter eigenvalue problem whose solutions represent the wave numbers in the plate and in the half-spaces. The multiparameter eigenvalue problem is solved numerically using recently developed algorithms.",18,1,2024
A community-led calibration of the Zr isotope Reference Materials: NIST candidate RM 8299 and SRM 3169,"As the field of Zr stable isotopes is rapidly expanding from the study of mass-independent to that of mass-dependent isotope effects, a variety of Zr standards have appeared in the literature. While several of these standards have been proposed as the ideal isotope reference material (iRM) against which all data should be reported, none of them have been shown to meet the compositional and/or conflict-of-interest-free distribution requirements put forth by the community. To remedy this situation, we report on a community-led effort to develop and calibrate a scale defining iRM for Zr isotopes: NIST RM 8299. Developed in partnership with the National Institute of Standards and Technology (NIST) from the widely used SRM 3169 Zr Standard Solution, the candidate RM 8299 was calibrated through an inter-laboratory study involving three laboratories. Our data show that RM 8299 meets all requirements of an ideal iRM. It is an isotopically homogeneous, high-purity reference material, that is free of isotope anomalies, and whose composition is identical to that of a major geological reservoir (Ocean Island Basalts). Furthermore, RM 8299 will be curated and distributed by NIST, a neutral, conflict-of-interest free organization, and was produced in sufficient quantities to last multiple decades. We recommend that all Zr isotope data to be reported against RM 8299. Our results also show that SRM 3169 lots #130920 and #071226 have identical composition to RM 8299. Therefore, using RM 8299 as the scale defining iRM will enable direct comparison of all future data with the vast majority of the existing literature data, both for mass-independent and mass-dependent isotope effects. To facilitate conversion of d94/90Zr values reported against other Zr standards, we provide high-precision conversion factors to the RM 8299 scale obtained using the double-spike method.",30,1,2024
On Abstract Nonlinear Integro-Dynamic Equations in Time Scale,"In this paper, we investigate the existence of the asymptotically almost automorphic solution of the following type of abstract nonlinear integro-dynamic equation \begin{eqnarray*} y^{\Delta}(s) &=&Ay(s)+\mathcal{F}\left(s,y(s),\int\limits_{t_0}^{s}{\mathcal{H}(s,\tau,y(\tau))}\Delta\tau\right),~ s\in\mathbb{T}^k, y(0)&=&y_0 \end{eqnarray*} in the Banach space of continuous function on a time scale $\mathbb{T}$. We apply the Krasnoselskii fixed point theorem to show the existence of an almost automorphic solution of the above dynamic equation.",1,1,2024
An Efficient Evolutionary Algorithm for Diversified Top-k (Weight) Clique Search Problems,"In many real-world problems and applications, finding only a single element, even though the best, among all possible candidates, cannot fully meet the requirements. We may wish to have a collection where each individual is not only outstanding but also distinctive. Diversified Top-k (DTk) problems are a kind of combinatorial optimization problem for finding such a promising collection of multiple sub-structures, such as subgraphs like cliques and social communities. In this paper, we address two representative and practical DTk problems, DTk Clique search (DTkC) and DTk Weight Clique search (DTkWC), and propose an efficient algorithm called Diversified Top-k Evolutionary AlgorithM (DiverTEAM) for these two problems. DiverTEAM consists of a local search algorithm, which focuses on generating high-quality and diverse individuals and sub-structures, and a genetic algorithm that makes individuals work as a team and converge to (near-)optima efficiently. Extensive experiments show that DiverTEAM exhibits an excellent and robust performance across various benchmarks of DTkC and DTkWC.",19,1,2024
Biomimicry in Radiation Therapy: Optimizing Patient Scheduling for Improved Treatment Outcomes,"In the realm of medical science, the pursuit of enhancing treatment efficacy and patient outcomes continues to drive innovation. This study delves into the integration of biomimicry principles within the domain of Radiation Therapy (RT) to optimize patient scheduling, ultimately aiming to augment treatment results. RT stands as a vital medical technique for eradicating cancer cells and diminishing tumor sizes. Yet, the manual scheduling of patients for RT proves both laborious and intricate. In this research, the focus is on automating patient scheduling for RT through the application of optimization methodologies. Three bio-inspired algorithms are employed for optimization to tackle the complex online stochastic scheduling problem. These algorithms include the Genetic Algorithm (GA), Firefly Optimization (FFO), and Wolf Optimization (WO). These algorithms are harnessed to address the intricate challenges of online stochastic scheduling. Through rigorous evaluation, involving the scrutiny of convergence time, runtime, and objective values, the comparative performance of these algorithms is determined. The results of this study unveil the effectiveness of the applied bio-inspired algorithms in optimizing patient scheduling for RT. Among the algorithms examined, WO emerges as the frontrunner, consistently delivering superior outcomes across various evaluation criteria. The optimization approach showcased in this study holds the potential to streamline processes, reduce manual intervention, and ultimately improve treatment outcomes for patients undergoing RT.",16,1,2024
The prominent and heterogeneous gender disparities in scientific novelty: evidence from biomedical doctoral theses,"Scientific novelty is the essential driving force for research breakthroughs and innovation. However, little is known about how early-career scientists pursue novel research paths, and the gender disparities in this process. To address this research gap, this study investigates a comprehensive dataset of 279,424 doctoral theses in biomedical sciences authored by US Ph.D. graduates. Spanning from 1980 to 2016, the data originates from the ProQuest Dissertations & Theses Database. This study aims to shed light on Ph.D. students' pursuit of scientific novelty in their doctoral theses and assess gender-related differences in this process. Using a combinatorial approach and a pre-trained Bio-BERT model, we quantify the scientific novelty of doctoral theses based on bio-entities. Applying fractional logistic and quantile regression models, this study reveals a decreasing trend in scientific novelty over time and heterogeneous gender disparities in doctoral theses. Specifically, female students consistently exhibited lower scientific novelty levels than their male peers. When supervised by female advisors, students' theses are found to be less novel than those under male advisors. The significant interaction effect of female students and female advisors suggests that female advisors may amplify the gender disparity in scientific novelty. Moreover, heterogeneous gender disparities in scientific novelty are identified, with non-top-tier universities displaying more pronounced disparities, while the differences at higher percentile ranges were comparatively more minor. These findings indicate a potential underrepresentation of female scientists pursuing novel research during the early stages of their careers. Notably, the outcomes of this study hold significant policy implications for advancing the careers of female scientists.",19,1,2024
Benchmarking formalisms for dynamic structure system Modeling and Simulation,"Modeling and simulation of complex systems is key to explore systems dynamics. Many scientific approaches were developed to represent dynamic structure systems but most of these approaches are efficient for some kinds of systems and inefficient for others. Which approach can be adopted for different dynamic structure systems categories is a topic of interest for many researchers and until now has not been fully resolved. Therefore it is essential to explore the existing approaches, understand them, and identify gaps. To fulfil this goal, we identified criteria at stake for a smooth flow from model creation to its simulation for dynamic structure systems. Using these criteria, we benchmark the existing modeling formalisms focusing more on DEVS extensions, and use the results to identify approaches gaps and discuss them.",25,1,2024
Machine Learning in Proton Exchange Membrane Water Electrolysis -- Part I: A Knowledge-Integrated Framework,"In this study, we propose to adopt a novel framework, Knowledge-integrated Machine Learning, for advancing Proton Exchange Membrane Water Electrolysis (PEMWE) development. Given the significance of PEMWE in green hydrogen production and the inherent challenges in optimizing its performance, our framework aims to meld data-driven models with domain-specific insights systematically to address the domain challenges. We first identify the uncertainties originating from data acquisition conditions, data-driven model mechanisms, and domain expertise, highlighting their complementary characteristics in carrying information from different perspectives. Building upon this foundation, we showcase how to adeptly decompose knowledge and extract unique information to contribute to the data augmentation, modeling process, and knowledge discovery. We demonstrate a hierarchical three-level framework, termed the ""Ladder of Knowledge-integrated Machine Learning"", in the PEMWE context, applying it to three case studies within a context of cell degradation analysis to affirm its efficacy in interpolation, extrapolation, and information representation. This research lays the groundwork for more knowledge-informed enhancements in ML applications in engineering.",24,1,2024
Federated Unlearning for Human Activity Recognition,"The rapid evolution of Internet of Things (IoT) technology has spurred the widespread adoption of Human Activity Recognition (HAR) in various daily life domains. Federated Learning (FL) is frequently utilized to build a global HAR model by aggregating user contributions without transmitting raw individual data. Despite substantial progress in user privacy protection with FL, challenges persist. Regulations like the General Data Protection Regulation (GDPR) empower users to request data removal, raising a new query in FL: How can a HAR client request data removal without compromising other clients' privacy? In response, we propose a lightweight machine unlearning method for refining the FL HAR model by selectively removing a portion of a client's training data. Our method employs a third-party dataset unrelated to model training. Using KL divergence as a loss function for fine-tuning, we aim to align the predicted probability distribution on forgotten data with the third-party dataset. Additionally, we introduce a membership inference evaluation method to assess unlearning effectiveness. Experimental results across diverse datasets show our method achieves unlearning accuracy comparable to \textit{retraining} methods, resulting in speedups ranging from hundreds to thousands.",17,1,2024
"Topologies in the Internet of Medical Things (IoMT), literature review","The bibliographic review is a fundamental phase in a research project, and it must guarantee that the most relevant information in the field of study is obtained. Our main objective was to know the works related to the Internet of medical things, from now on (IoMT). We analyzed a total of 535 articles searched in association for Computing Machinery in Adelante ACM, Web of Science and Scopus the search domain was IoMT, we established 3 parameters, (problematic, artifact and artifact evaluation), this according to the Research of Design Science in Adelante DSR, is a research approach for the construction of artifacts to provide a useful solution to a problem in each domain. The equation (Internet of things AND mesh) resulted in 535, (Internet of things AND medicine) a total of 417 and finally (Internet of medical things AND mesh) with 8, this means that there is a lot to investigate in this research domain. The advantages identified in this type of topology is to carry messages from one node to another by different paths, there can be absolutely no interruption in communications, each server has its own communications with all other servers. Health and IT issues have been drastically influenced by the large data from IoMT devices. In this paper, we conducted a review of the scientific literature and mapped research trends on the IoMT paradigm in the health domain. Finally, this paper expands on the literature, and the findings of this study can serve as a basis for future studies.",29,1,2024
Discovering the Power of Artificial Cardiac Conduction System (ACCS): Harmony in Bio-inspired Metaheuristic,"This work proposes a novel bio-inspired metaheuristic called Artificial Cardiac Conduction System (ACCS) inspired by the human cardiac conduction system. The ACCS algorithm imitates the functional behaviour of the human heart that generates and sends signals to the heart muscle, initiating it to contract. Four nodes in the myocardium layer are participating in generating and controlling heart rate, such as the sinoatrial, atrioventricular, bundle of His, and Purkinje fibers. The mechanism of controlling the heart rate through these four nodes is implemented. The algorithm is then benchmarked on 19 well-known mathematical test functions as it can determine the exploitation and exploration capability of the algorithm, and the results are verified by a comparative study with Whale Optimization Algorithm (WOA), Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Deferential Evolution (DE), and Fast Evolutionary Programming (FEP). The results show that the ACCS algorithm can provide very competitive results compared to these well-known metaheuristics and other conventional methods.",12,1,2024
Critiques protocolaires d'Internet: comparaison des projets IPFS et SecureScuttleButt,"This paper explores two critical infrastructure proposals as alternatives to the current state of the Internet protocols: IPFS (Interplanetary File System) and Scuttlebutt, highlighting the political a priori and debates of these technical enterprises. To do so, I propose to analyze the discourses of the developers of these two systems in the mode of a critical discourse analysis.This article highlights a particular form of criticism of Internet regimes: infrastructural criticism, and highlights its variety through a comparative study. Through these two case studies, we will see how different alternatives to the current spatio-temporal implementations of the Internet allow us to identify the agency dimensions of these acts of hijacking and substitution, characterizing two quite different approaches to decentralized protocols, yet linked by a technical similarity.",29,1,2024
On the difference of the moduli of the two initial logarithmic coefficients,"In this paper, we give sharp bounds of the difference of the moduli of the second and the first logarithmic coefficient for the functions on the class $\mathcal U$, for the $\alpha$-convex functions, and for the class $\mathcal{G}(\alpha)$ introduced by Ozaki.",25,1,2024
Quantum computing for simulation of fluid dynamics,"We present a pedagogical introduction to a series of quantum computing algorithms for the simulation of classical fluids, with special emphasis on the Carleman-Lattice Boltzmann method.",13,1,2024
On weakly classical 1-absorbing prime submodules,"In this paper, we study weakly classical 1-absorbing prime submodules of a nonzero unital module $M$ over a commutative ring $R$ having a nonzero identity. A proper submodule $N$ of $M$ is said to be a weakly classical 1-absorbing prime submodule, if for each $m\in M$ and nonunits $a,b,c\in R,$ $0\neq abcm\in N$ implies that $abm\in N$ or $cm\in N$. We give various examples and properties of weakly classical 1-absorbing prime submodules. Also, we investiage the weakly classical 1-absorbing prime submodules of tensor product $F\otimes M$ of a (faithfully) flat $R$-module $F$ and any $R$-module $M.$ Also, we prove that if every proper submodule of an $R$-module $M$ is weakly classical 1-absorbing prime, then $Jac(R)^{3}M=0$. In terms of this result, we characterize modules over local rings in which every proper submodule is weakly classical 1-absorbing prime.",16,1,2024
Analysis of classical and quantum mechanical concepts of probability: A synopsis,"This paper addresses the central question of what a coherent concept of probability might look like that would do justice to both classical probability theory, axiomatized by Kolmogorov, and quantum theory. At a time when quanta are receiving increased and expanded attention -- think, for example, of the advances in quantum computers or the promises associated with this new technology (National Academies of Sciences: Engineering, and Medicine, 2019) -- an adequate interpretation of probability, which is no less important, should be given due attention, particularly with regard to quantum theory.",9,1,2024
Revisiting Taylor and the Trinity Test,"The atomic bomb uses fission of heavy elements to produce a large amount of energy. It was designed and deployed during World War II by the United States military. The first test of an atomic bomb occurred in July of 1945 in New Mexico and was given the name Trinity; this test was not declassified until 1949. In that year Geoffrey Ingram Taylor released two papers, detailing his process in calculating the energy yield of the atomic bomb from pictures of the Trinity explosion alone. Many scientists made similar calculations concurrently, though Taylor is often accredited with them. Since then many scientists have also attempted a to calculate a yield through various methods. This paper walks through these methods with a focus on Taylor's method -- based on first principles -- as well as redoing the calculations that he performed with modern tools. In this paper we make use of the state-of-the-art computer vision tools to find a more precise measurement of the blast radius, as well as using curve fitting and numerical integration methods. With more precise measurements we are able to follow in Taylor's footstep towards a more accurate approximation.",7,1,2024
Partial Dynamical Systems of $L^p$-Spaces and their Stability Spaces,"Using the convolution product and weak derivatives, we consider the partial dynamical systems of the locally convex $L^p(\Omega)$ spaces defined by the action of the smooth algebra $\mathscr{K}(\Omega)$ through its nets. Slice analysis is then employed to show that the Sobolev spaces $W^{k,p}(\Omega)$ are the stable states or space of these partial dynamical systems as limit spaces of the convolution actions of the smooth algebra $K(\Omega)$ on the Banach spaces $L^p(\Omega)$. Thus, the Sobolev spaces $W^{k,p}(\Omega)$ are closed subspaces of the $Lp(\Omega)$-spaces under convolution product and weak derivatives, with the weak derivative operators acting as equivariant maps of the slice spaces.",30,1,2024
Bridging Generative Networks with the Common Model of Cognition,"This article presents a theoretical framework for adapting the Common Model of Cognition to large generative network models within the field of artificial intelligence. This can be accomplished by restructuring modules within the Common Model into shadow production systems that are peripheral to a central production system, which handles higher-level reasoning based on the shadow productions' output. Implementing this novel structure within the Common Model allows for a seamless connection between cognitive architectures and generative neural networks.",25,1,2024
SAM-dPCR: Real-Time and High-throughput Absolute Quantification of Biological Samples Using Zero-Shot Segment Anything Model,"Digital PCR (dPCR) has revolutionized nucleic acid diagnostics by enabling absolute quantification of rare mutations and target sequences. However, current detection methodologies face challenges, as flow cytometers are costly and complex, while fluorescence imaging methods, relying on software or manual counting, are time-consuming and prone to errors. To address these limitations, we present SAM-dPCR, a novel self-supervised learning-based pipeline that enables real-time and high-throughput absolute quantification of biological samples. Leveraging the zero-shot SAM model, SAM-dPCR efficiently analyzes diverse microreactors with over 97.7% accuracy within a rapid processing time of 3.16 seconds. By utilizing commonly available lab fluorescence microscopes, SAM-dPCR facilitates the quantification of sample concentrations. The accuracy of SAM-dPCR is validated by the strong linear relationship observed between known and inferred sample concentrations. Additionally, SAM-dPCR demonstrates versatility through comprehensive verification using various samples and reactor morphologies. This accessible, cost-effective tool transcends the limitations of traditional detection methods or fully supervised AI models, marking the first application of SAM in nucleic acid detection or molecular diagnostics. By eliminating the need for annotated training data, SAM-dPCR holds great application potential for nucleic acid quantification in resource-limited settings.",22,1,2024
Numerical evaluation of code live-load models for estimating the forces caused by actual vehicles that act on bridge substructures,"The present paper assesses the efficacy of code live-load models in accurately estimating the vehicular loads transferred to bridge substructures, such as abutments, piers, and foundations. Realistic traffic vehicle data are represented using four Weigh-in-Motion databases, which provide an authentic representation of vehicle information, thus providing a realistic basis for the examination of the bridges studied. The evaluation includes various bridge models, such as single-span girder bridges and two-, three-, and four-span continuous pinned-support girder bridges. By analyzing exceedance rates, the study compares the extreme force values obtained for vehicles in the databases with those predicted by selected code live-load models. These exceedance rates are presented in spectra format, as a function of the span length. The significant variations observed in the exceedance rates highlight the need for improving existing code live-load models to achieve more accurate estimations of the forces transferred to bridge substructures. Such improvements would lead to more uniform reliability levels for any limit state, such as resistance, fatigue, serviceability, and cracking.",15,1,2024
RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees,"Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting watermark removal. Experiments on a diverse range of images generated by state-of-the-art diffusion models reveal substantial performance enhancements compared to existing approaches. For instance, our method demonstrates a notable increase in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in detecting watermarked images under adversarial attacks, while maintaining image quality, as indicated by closely aligned FID and CLIP scores.",23,1,2024
Entanglement of temporal sections as quantum histories and their quantum correlation bounds,"In this paper we focus on the underlying quantum structure of temporal correlations and show their peculiar nature which differentiate them from spatial quantum correlations. With a growing interest in representation of quantum states as topological objects, we consider quantum history bundles based on the temporal manifold and show the source of violation of monogamous temporal Bell-like inequalities. We introduce definitions for the mixture of quantum histories and consider their entanglement as sections over the Hilbert vector bundles. As a generalization of temporal Bell-like inequalities, we derive the quantum bound for multi-time Bell-like inequalities.",25,1,2024
Navigating the Docker Ecosystem: A Comprehensive Taxonomy and Survey,"The cloud computing landscape is rapidly expanding and growing in complexity. It has witnessed the emergence of Cloud Computing as a widely adopted model for efficiently processing large volumes of data by harnessing clusters of commodity computers. This evolution enables the handling of massive data through on-demand services, relying on numerous microservices with diverse dependencies. The technology of containers ensures secure storage, allowing for largescale data processing with high scalability and portability. Container technology, particularly exemplified by Docker in the last decade, plays a pivotal role in this scenario. It empowers microservices to process data swiftly, enabling developers to dynamically scale these services in real-time. This paper initiates by establishing a comprehensive taxonomy for delineating container architecture. Focusing specifically on Docker containers, we scrutinize various existing container related literature. Through this taxonomy and survey, we not only discern similarities and disparities in the architectural approaches of Docker container technology but also pinpoint areas necessitating further research.",3,1,2024
SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations,"A clinical artificial intelligence (AI) system is often validated on a held-out set of data which it has not been exposed to before (e.g., data from a different hospital with a distinct electronic health record system). This evaluation process is meant to mimic the deployment of an AI system on data in the wild; those which are currently unseen by the system yet are expected to be encountered in a clinical setting. However, when data in the wild differ from the held-out set of data, a phenomenon referred to as distribution shift, and lack ground-truth annotations, it becomes unclear the extent to which AI-based findings can be trusted on data in the wild. Here, we introduce SUDO, a framework for evaluating AI systems without ground-truth annotations. SUDO assigns temporary labels to data points in the wild and directly uses them to train distinct models, with the highest performing model indicative of the most likely label. Through experiments with AI systems developed for dermatology images, histopathology patches, and clinical reports, we show that SUDO can be a reliable proxy for model performance and thus identify unreliable predictions. We also demonstrate that SUDO informs the selection of models and allows for the previously out-of-reach assessment of algorithmic bias for data in the wild without ground-truth annotations. The ability to triage unreliable predictions for further inspection and assess the algorithmic bias of AI systems can improve the integrity of research findings and contribute to the deployment of ethical AI systems in medicine.",2,1,2024
Interaction of photons with silver and indium nuclei at energies up to 20 MeV,"The yields of photonuclear reactions in the 107Ag, 113In, and 115In nuclei have been measured. Monte Carlo simulations have been performed using the Geant4 code, and the results have been compared with the experimental ones. The isomeric ratios of the yields in the reactions 107Ag({\gamma},n)106m,gAg and 113In({\gamma},n)112m,gIn have been determined. The cross sections for the reactions 107Ag({\gamma},n)106gAg and 107Ag({\gamma},2n)105Ag at an energy of 20 MeV have been calculated on the basis of the experimental data.",9,1,2024
Dirac and Majorana Fermions in the Anti-de Sitter spacetime with Tachyonic approaches,"We analyze the Dirac equation in Anti-de Sitter (AdS) spacetime upon the comformally flat spacetime. We also apply the analysis to the Majorana condition. The analytical solutions are neat. We observe that Dirac fermions and Majorana fermions behave differently in AdS background. The wavefunction of tachyonic neutrinos leaves an track, clumping in a special flatland, embedded in the curved 3+1-D AdS spacetime. Besides, there emerges a boundary, and neutrinos also oscillate.",4,1,2024
C Analyzer : A Static Program Analysis Tool for C Programs,"In our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. Program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.This project work tries to leverage Abstract Interpretation techniques for static analysis of C programs. C Analyzer is a tool developed for static analysis of C programs. This implementation of C Analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. C Analyzer supports four abstract domains - Interval, Octagon, Polyhedra, and Bit Vector. We use these different domains for required precision in program verification. C Analyzer tool uses LLVM C/C++ compiler frontend Clang API to generate and traverse the Control Flow Graph (CFG) of a given C program. This tool generates invariants in different abstract domains for statements in basic blocks of CFG during CFG traversal. Using these invariants, some properties of a program, such as dividing by zero, modulus zero, arithmetic overflow, etc., can be analyzed. We also use a source-to-source transformation tool, CIL (Common Intermediate language), to transform some C constructs into simpler constructs, such as transforming logical operators, switch statements, and conditional operators into if-else ladders and transforming do-while and for loops into while loops.Using C Analyzer, C program constructs such as declarations, assignments, binary operations (arithmetic, relational, bitwise shift, etc.), conditions (if-else), loops (while, do while, for loop), nested conditions, and nested loops can be analyzed. Currently, this tool does not support arrays, structures, unions, pointers, or function calls.",28,1,2024
Electron Scattering at a Soft Temporal Potential Step,"We solve the problem of electron scattering at a soft temporal potential step. Given the relativistic nature of the problem, we use the Dirac equation, with its spinor wavefunction. We find solutions in terms of hypergeometric functions, which demonstrate that the observed phenomenon of later forward-wave and backward-wave electron scattering previously reported for a sharp (Heaviside) temporal potential step can be obtained experimentally and applied to new electronic devices.",26,1,2024
"Stars: Evolution, Stability and Statistical Mechanics","This paper reviews the physics of stars, the type, structure, evolution and stability. Simple thermodynamics and statistical mechanics are used to show the inner working of white dwarf and neutron stars. The major concentration of the paper will be on white dwarf stars although in some places references will also be made to neutron stars where the relations can be extended easily. It can be shown that a maximum mass limit is attached to each type of star which can be derived rigorously. Maximum entropy can be used to show that the gravitational contraction is balanced by the degeneracy pressure created by the electrons in the case of white dwarfs, and neutrons and protons which constitute the matter of the neutron star. Finally the kinetic equations which describe the luminosity of the star and the radiative transfer are introduced.",25,1,2024
Hybrid deep learning and physics-based neural network for programmable illumination computational microscopy,"Relying on either deep models or physical models are two mainstream approaches for solving inverse sample reconstruction problems in programmable illumination computational microscopy. Solutions based on physical models possess strong generalization capabilities while struggling with global optimization of inverse problems due to a lack of insufficient physical constraints. In contrast, deep learning methods have strong problem-solving abilities, but their generalization ability is often questioned because of the unclear physical principles. Besides, conventional deep models are difficult to apply to some specific scenes because of the difficulty in acquiring high-quality training data and their limited capacity to generalize across different scenarios. In this paper, to combine the advantages of deep models and physical models together, we propose a hybrid framework consisting of three sub-neural networks (two deep learning networks and one physics-based network). We first obtain a result with rich semantic information through a light deep learning neural network and then use it as the initial value of the physical network to make its output comply with physical process constraints. These two results are then used as the input of a fusion deep learning neural work which utilizes the paired features between the reconstruction results of two different models to further enhance imaging quality. The final result integrates the advantages of both deep models and physical models and can quickly solve the computational reconstruction inverse problem in programmable illumination computational microscopy and achieve better results. We verified the feasibility and effectiveness of the proposed hybrid framework with theoretical analysis and actual experiments on resolution targets and biological samples.",17,1,2024
Entangling Machine Learning with Quantum Tensor Networks,"This paper examines the use of tensor networks, which can efficiently represent high-dimensional quantum states, in language modeling. It is a distillation and continuation of the work done in (van der Poel, 2023). To do so, we will abstract the problem down to modeling Motzkin spin chains, which exhibit long-range correlations reminiscent of those found in language. The Matrix Product State (MPS), also known as the tensor train, has a bond dimension which scales as the length of the sequence it models. To combat this, we use the factored core MPS, whose bond dimension scales sub-linearly. We find that the tensor models reach near perfect classifying ability, and maintain a stable level of performance as the number of valid training examples is decreased.",9,1,2024
A Thorough Analysis of Radio Resource Assignment for UAV-Enhanced Vehicular Sidelink Communications,"The rapid expansion of connected and autonomous vehicles (CAVs) and the shift towards millimiter-wave (mmWave) frequencies offer unprecedented opportunities to enhance road safety and traffic efficiency. Sidelink communication, enabling direct Vehicle-to-Vehicle (V2V) communications, play a pivotal role in this transformation. As communication technologies transit to higher frequencies, the associated increase in bandwidth comes at the cost of a severe path and penetration loss. In response to these challenges, we investigate a network configuration that deploys beamforming-capable Unmanned Aerial Vehicles (UAVs) as relay nodes. In this work, we present a comprehensive analytical framework with a groundbreaking performance metric, i.e. average access probability, that quantifies user satisfaction, considering factors across different protocol stack layers. Additionally, we introduce two Radio Resources Assignment (RRA) methods tailored for UAVs. These methods consider parameters such as resource availability, vehicle distribution, and latency requirements. Through our analytical approach, we optimize the average access probability by controlling UAV altitude based on traffic density. Our numerical findings validate the proposed model and strategy, which ensures that Quality of Service (QoS) standards are met in the domain of Vehicle-to-Anything (V2X) sidelink communications.",19,1,2024
Water-Based Metaheuristics: How Water Dynamics Can Help Us to Solve NP-Hard Problems,"Many water-based optimization metaheuristics have been introduced during the last decade, both for combinatorial and for continuous optimization. Despite the strong similarities of these methods in terms of their underlying natural metaphors (most of them emulate, in some way or another, how drops collaboratively form paths down to the sea), in general the resulting algorithms are quite different in terms of their searching approach or their solution construction approach. For instance, each entity may represent a solution by itself or, alternatively, entities may construct solutions by modifying the landscape while moving. A researcher or practitioner could assume that the degree of similarity between two water-based metaheuristics heavily depends on the similarity of the natural water mechanics they emulate, but this is not the case. In order to bring some clarity to this mosaic of apparently related metaheuristics, in this paper we introduce them, explain their mechanics, and highlight their differences.",16,1,2024
Discriminative Consensus Mining with A Thousand Groups for More Accurate Co-Salient Object Detection,"Co-Salient Object Detection (CoSOD) is a rapidly growing task, extended from Salient Object Detection (SOD) and Common Object Segmentation (Co-Segmentation). It is aimed at detecting the co-occurring salient object in the given image group. Many effective approaches have been proposed on the basis of existing datasets. However, there is still no standard and efficient training set in CoSOD, which makes it chaotic to choose training sets in the recently proposed CoSOD methods. First, the drawbacks of existing training sets in CoSOD are analyzed in a comprehensive way, and potential improvements are provided to solve existing problems to some extent. In particular, in this thesis, a new CoSOD training set is introduced, named Co-Saliency of ImageNet (CoSINe) dataset. The proposed CoSINe is the largest number of groups among all existing CoSOD datasets. The images obtained here span a wide variety in terms of categories, object sizes, etc. In experiments, models trained on CoSINe can achieve significantly better performance with fewer images compared to all existing datasets. Second, to make the most of the proposed CoSINe, a novel CoSOD approach named Hierarchical Instance-aware COnsensus MinEr (HICOME) is proposed, which efficiently mines the consensus feature from different feature levels and discriminates objects of different classes in an object-aware contrastive way. As extensive experiments show, the proposed HICOME achieves SoTA performance on all the existing CoSOD test sets. Several useful training tricks suitable for training CoSOD models are also provided. Third, practical applications are given using the CoSOD technique to show the effectiveness. Finally, the remaining challenges and potential improvements of CoSOD are discussed to inspire related work in the future. The source code, the dataset, and the online demo will be publicly available atthis http URL.",15,1,2024
Enhancing Digital Hologram Reconstruction Using Reverse-Attention Loss for Untrained Physics-Driven Deep Learning Models with Uncertain Distance,"Untrained Physics-based Deep Learning (DL) methods for digital holography have gained significant attention due to their benefits, such as not requiring an annotated training dataset, and providing interpretability since utilizing the governing laws of hologram formation. However, they are sensitive to the hard-to-obtain precise object distance from the imaging plane, posing the $\textit{Autofocusing}$ challenge. Conventional solutions involve reconstructing image stacks for different potential distances and applying focus metrics to select the best results, which apparently is computationally inefficient. In contrast, recently developed DL-based methods treat it as a supervised task, which again needs annotated data and lacks generalizability. To address this issue, we propose $\textit{reverse-attention loss}$, a weighted sum of losses for all possible candidates with learnable weights. This is a pioneering approach to addressing the Autofocusing challenge in untrained deep-learning methods. Both theoretical analysis and experiments demonstrate its superiority in efficiency and accuracy. Interestingly, our method presents a significant reconstruction performance over rival methods (i.e. alternating descent-like optimization, non-weighted loss integration, and random distance assignment) and even is almost equal to that achieved with a precisely known object distance. For example, the difference is less than 1dB in PSNR and 0.002 in SSIM for the target sample in our experiment.",11,1,2024
Deep learning based detection of collateral circulation in coronary angiographies,"Coronary artery disease (CAD) is the dominant cause of death and hospitalization across the globe. Atherosclerosis, an inflammatory condition that gradually narrows arteries and has potentially fatal effects, is the most frequent cause of CAD. Nonetheless, the circulation regularly adapts in the presence of atherosclerosis, through the formation of collateral arteries, resulting in significant long-term health benefits. Therefore, timely detection of coronary collateral circulation (CCC) is crucial for CAD personalized medicine. We propose a novel deep learning based method to detect CCC in angiographic images. Our method relies on a convolutional backbone to extract spatial features from each frame of an angiography sequence. The features are then concatenated, and subsequently processed by another convolutional layer that processes embeddings temporally. Due to scarcity of data, we also experiment with pretraining the backbone on coronary artery segmentation, which improves the results consistently. Moreover, we experiment with few-shot learning to further improve performance, given our low data regime. We present our results together with subgroup analyses based on Rentrop grading, collateral flow, and collateral grading, which provide valuable insights into model performance. Overall, the proposed method shows promising results in detecting CCC, and can be further extended to perform landmark based CCC detection and CCC quantification.",8,1,2024
Haze Removal via Regional Saturation-Value Translation and Soft Segmentation,"This paper proposes a single image dehazing prior, called Regional Saturation-Value Translation (RSVT), to tackle the color distortion problems caused by conventional dehazing approaches in bright regions. The RSVT prior is developed based on two key observations regarding the relationship between hazy and haze-free points in the HSV color space. First, the hue component shows marginal variation between corresponding hazy and haze-free points, consolidating a hypothesis that the pixel value variability induced by haze primarily occurs in the saturation and value spaces. Second, in the 2D saturation-value coordinate system, most lines passing through hazy-clean point pairs are likely to intersect near the atmospheric light coordinates. Accordingly, haze removal for the bright regions can be performed by properly translating saturation-value coordinates. In addition, an effective soft segmentation method based on a morphological min-max channel is introduced. By combining the soft segmentation mask with the RSVT prior, a comprehensive single image dehazing framework is devised. Experimental results on various synthetic and realistic hazy image datasets demonstrate that the proposed scheme successfully addresses color distortion issues and restores visually appealing images. The code of this work is available atthis https URL.",7,1,2024
Non-existence of certain lightlike hypersurfaces of an indefinite Sasakian manifold,"Here, we consider a lightlike hypersurface, tangent to the structure vector field, of an indefinite Sasakian manifold. We prove that no such a hypersurface can either have parallel or recurrent second fundamental forms. In addition to the above, we also prove that no such a hypersurface may have parallel or recurrent induced structural tensors.",22,1,2024
Calculating the sequences behind a hexagonal lattice based equal circle packing in the Euclidian plane,"The article presents the mathematical sequences describing circle packing densities in four different geometric configurations involving a hexagonal lattice based equal circle packing in the Euclidian plane. The calculated sequences take form of either polynomials or rational functions. If the circle packing area is limited with a circle, the packing densities tend to decrease with increasing number of the packed circles and converge to values lower than {\pi}/(2\sqrt{3}). In cases with packing areas limited by equilateral triangles or equilateral hexagons the packing densities tend to increase with increasing number of the packed circles and converge to {\pi}/(2\sqrt{3}). The equilateral hexagons are shown to be the preferred equal circle packing surface areas with practical applications searching for high equal circle packing densities, since the packing densities with circle packing inside equilateral hexagons converge faster to {\pi}/(2\sqrt{3}) than in the case of equilateral triangle packing surface areas.",18,1,2024
Closed Form for Half-Area Overlap Offset of 2 Unit Disks,"The separation between the centers of two unit circles such that their overlapping area is exactly half of each's area is known to be around $0.8079455\dots$ (OEIS A133741). However, no closed form of this number is known. Here, we determine its closed form representation in terms of the inverse regularized beta function.",16,1,2024
"On the diophantine equation $X^6-Y^6 = W^n-Z^n, n=2,3,4$","In this paper, we proved that there are infinitely many integer solutions of $X^6 - Y^6 = W^n - Z^n,\ n=2,3,4$.",15,1,2024
Spectral Fredholm Theory in Von Neumann Algebras,"In this paper, we extend Fredholm theory in von Neumann algebras established by Breuer in [5] and [6] to spectral Fredholm theory. We consider 2 by 2 upper triangular operator matrices with coefficients in a von Neumann algebra and give the relationship between the generalized essential spectra in the sense of Breuer of such matrices and of their diagonal entries. Next, we prove that if a generalized Fredholm operator in the sense of Breuer has 0 as an isolated point of its spectrum, then the corresponding spectral projection is finite. Finally, we define the generalized B-Fredholm operator in a von Neumann algebra as a generalization in the sense of Breuer of the classical B-Fredholm operators on Hilbert and Banach spaces. We provide sufficient conditions under which a sum of a generalized B-Fredholm operator and a finite operator in a von Neumann algebra is again a generalized B-Fredholm operator.",14,1,2024
"Corrigendum to ""$m$-Periodic Gorenstein objects"" [J. Algebra 621 (2023)]","Let $(\mathcal{A,B})$ be a GP-admissible pair and $(\mathcal{Z,W})$ be a GI-admissible pair of classes of objects in an abelian category $\mathcal{C}$, and consider the class $\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}$ of $1$-periodic $(\omega,\mathcal{B})$-Gorenstein projective objects, where $\omega := \mathcal{A} \cap \mathcal{B}$ and $\nu := \mathcal{Z} \cap \mathcal{W}$. We claimed in \cite[Lem. 8.1]{HMP2023m} that the $(\mathcal{Z,W})$-Gorenstein injective dimension of $\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}$ is bounded by the $(\mathcal{Z,W})$-Gorenstein injective dimension of $\omega$, provided that: (1) $\omega$ is closed under direct summands, (2) $\mathrm{Ext}^1(\pi\mathcal{GP}_{(\omega,\mathcal{B},1)},\nu) = 0$, and (3) every object in $\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}$ admits a $\mathrm{Hom}(-,\nu)$-acyclic $\nu$-coresolution. These conditions are their duals are part of what we called ``Setup 1''. Moreover, if we replace $\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}$ by the class $\mathcal{GP}_{(\mathcal{A,B})}$ of $(\mathcal{A,B})$-Gorenstein projective objects, the resulting inequality is claimed to be true under a set of conditions named ``Setup 2''.The proof we gave for the claims $\mathrm{Gid}_{(\mathcal{Z,W})}(\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}) \leq \mathrm{Gid}_{(\mathcal{Z,W})}(\omega)$ and $\mathrm{Gid}_{(\mathcal{Z,W})}(\mathcal{GP}_{(\mathcal{A,B})}) \leq \mathrm{Gid}_{(\mathcal{Z,W})}(\omega)$ is incorrect, and the purpose of this note is to exhibit a corrected proof of the first inequality, under the additional assumption that every object in $\pi\mathcal{GP}_{(\omega,\mathcal{B},1)}$ has finite injective dimension relative to $\mathcal{Z}$. Setup 2 is no longer required, and as a result the second inequality was removed. We also fix those results in §\ 8 of \cite{HMP2023m} affected by Lemma 8.1, and comment some applications and examples.",13,1,2024
Fractal Nambu Mechanics: Extending Dynamics with Fractal Calculus,"In this paper, we extend the principles of Nambu mechanics by incorporating fractal calculus. This extension introduces Hamiltonian and Lagrangian mechanics that incorporate fractal derivatives. By doing so, we broaden the scope of our analysis to encompass the dynamics of fractal systems, enabling us to capture their intricate and self-similar properties. This novel approach opens up new avenues for understanding and modeling complex fractal structures, thereby advancing our comprehension of these intricate phenomena.",6,1,2024
Location of Zeros of Holomorphic Functions,"In this article, various results will be demonstrated that enable the delimitation of a zero-free region for holomorphic functions on a set $K$, studying the behavior of their imaginary or real part on the boundary of $K$. These findings contribute to a deeper understanding of the distribution of zeros, shedding light on the intricate nature of holomorphic functions within the specified set.",1,1,2024
Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations,"We present the Qualitative Explainable Graph (QXG): a unified symbolic and qualitative representation for scene understanding in urban mobility. QXG enables the interpretation of an automated vehicle's environment using sensor data and machine learning models. It leverages spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an intelligible scene model. Crucially, QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations and real-time decision-making across various sensor types. Our research showcases the transformative potential of QXG, particularly in the context of automated driving, where it elucidates decision rationales by linking the graph with vehicle actions. These explanations serve diverse purposes, from informing passengers and alerting vulnerable road users (VRUs) to enabling post-analysis of prior behaviours.",29,1,2024
Should the choice of BOIN design parameter p.tox only depend on the target DLT rate?,"When the early stopping parameter n.earlystop is relatively small or the cohortsize value is not optimized via simulation, it may be better to use p.tox < 1.4 * target.DLT.rate, or try out different cohort sizes, or increase n.earlystop, whichever is both feasible and provides better operating characteristics. This is because if the cohortsize was not optimized via simulation, even when n.earlystop = 12, the BOIN escalation/de-escalation rules generated using p.tox = 1.4 * target.DLT.rate could be exactly the same as those calculated using p.tox > 3 * target.DLT.rate, which might not be acceptable for some pediatric trials targeting 10% DLT rate. The traditional 3+3 design stops the dose finding process when 3 patients have been treated at the current dose level, 0 DLT has been observed, and the next higher dose has already been eliminated. If additional 3 patients were required to be treated at the current dose in the situation described above, the corresponding boundary table could be generated using BOIN design with target DLT rates ranging from 18% to 29%, p.saf ranging from 8% to 26%, and p.tox ranging from 39% to 99%. To generate the boundary table of this 3+3 design variant, BOIN parameters also need to satisfy a set of conditions.",26,1,2024
New characterizations of migrative 2-uninorms,"This article pays attention to the $\alpha$-migrativity of 2-uninorms with $\alpha\in [0,1]$ deeply. It describes the $\alpha$-migrativity of 2-uninorms completely, which generalizes and unifies some current existing characterizations for $\alpha$-migrativity of triangular norm, triangular conorm, uninorms, nullnorms, uni-nullnorms and null-uninorms, respectively.",11,1,2024
Characterizations of quasi-homogeneous aggregation functions,"In this article, we first modify the definition of quasi-homogeneous aggregation function, and then give the characterizations of quasi-homogeneous aggregation functions, which show us that quasi-homogeneous aggregation functions are classified into three classes. We finally introduce the concept of triple generator of quasi-homogeneous aggregation function, which is applied to construct a quasi-homogeneous aggregation function.",11,1,2024
Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks,"For classification, neural networks typically learn by minimizing cross-entropy, but are evaluated and compared using accuracy. This disparity suggests neural loss function search (NLFS), the search for a drop-in replacement loss function of cross-entropy for neural networks. We apply NLFS to image classifier convolutional neural networks. We propose a new search space for NLFS that encourages more diverse loss functions to be explored, and a surrogate function that accurately transfers to large-scale convolutional neural networks. We search the space using regularized evolution, a mutation-only aging genetic algorithm. After evolution and a proposed loss function elimination protocol, we transferred the final loss functions across multiple architectures, datasets, and image augmentation techniques to assess generalization. In the end, we discovered three new loss functions, called NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able to outperform cross-entropy in terms of a higher mean test accuracy as a simple drop-in replacement loss function across the majority of experiments.",30,1,2024
Realtime Facial Expression Recognition: Neuromorphic Hardware vs. Edge AI Accelerators,"The paper focuses on real-time facial expression recognition (FER) systems as an important component in various real-world applications such as social robotics. We investigate two hardware options for the deployment of FER machine learning (ML) models at the edge: neuromorphic hardware versus edge AI accelerators. Our study includes exhaustive experiments providing comparative analyses between the Intel Loihi neuromorphic processor and four distinct edge platforms: Raspberry Pi-4, Intel Neural Compute Stick (NSC), Jetson Nano, and Coral TPU. The results obtained show that Loihi can achieve approximately two orders of magnitude reduction in power dissipation and one order of magnitude energy savings compared to Coral TPU which happens to be the least power-intensive and energy-consuming edge AI accelerator. These reductions in power and energy are achieved while the neuromorphic solution maintains a comparable level of accuracy with the edge accelerators, all within the real-time latency requirements.",30,1,2024
Gated Chemical Units,"We introduce Gated Chemical Units (GCUs), a new type of gated recurrent cells which provide fresh insights into the commonly-used gated recurrent units, and bridge their gap to biologically-plausible neural models. We systematically derive GCUs from Electrical Equivalent Circuits (EECs), a widely adopted ordinary-differential-equations model in neuroscience for biological neurons with both electrical and chemical synapses. We focus on saturated EECs, as they are more stable, and chemical synapses, as they are more expressive. To define GCUs, we introduce a new kind of gate, we call a time gate (TG), in the associated difference-equations model of the EECs. The TG learns for each neuron the optimal time step to be used in a simple Euler integration scheme, and leads to a very efficient gated unit. By observing that the TG corresponds to the forget gate (FG) in traditional gated recurrent units, we provide a new formulation of these units as neural ODEs. We also show that in GCUs, the FG is in fact its liquid time constant. Finally, we demonstrate that GCUs not only explain the elusive nature of gates in traditional recurrent units, but also represent a very competitive alternative to these units.",30,1,2024
Using Sequential Runtime Distributions for the Parallel Speedup Prediction of SAT Local Search,"This paper presents a detailed analysis of the scalability and parallelization of local search algorithms for the Satisfiability problem. We propose a framework to estimate the parallel performance of a given algorithm by analyzing the runtime behavior of its sequential version. Indeed, by approximating the runtime distribution of the sequential process with statistical methods, the runtime behavior of the parallel process can be predicted by a model based on order statistics. We apply this approach to study the parallel performance of two SAT local search solvers, namely Sparrow and CCASAT, and compare the predicted performances to the results of an actual experimentation on parallel hardware up to 384 cores. We show that the model is accurate and predicts performance close to the empirical data. Moreover, as we study different types of instances (random and crafted), we observe that the local search solvers exhibit different behaviors and that their runtime distributions can be approximated by two types of distributions: exponential (shifted and non-shifted) and lognormal.",30,1,2024
Bridging Human Concepts and Computer Vision for Explainable Face Verification,"With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable insights into the decision-making processes.",30,1,2024
Verification for Object Detection -- IBP IoU,"We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.",30,1,2024
Multi-view Subspace Clustering via An Adaptive Consensus Graph Filter,"Multiview subspace clustering (MVSC) has attracted an increasing amount of attention in recent years. Most existing MVSC methods first collect complementary information from different views and consequently derive a consensus reconstruction coefficient matrix to indicate the subspace structure of a multi-view data set. In this paper, we initially assume the existence of a consensus reconstruction coefficient matrix and then use it to build a consensus graph filter. In each view, the filter is employed for smoothing the data and designing a regularizer for the reconstruction coefficient matrix. Finally, the obtained reconstruction coefficient matrices from different views are used to create constraints for the consensus reconstruction coefficient matrix. Therefore, in the proposed method, the consensus reconstruction coefficient matrix, the consensus graph filter, and the reconstruction coefficient matrices from different views are interdependent. We provide an optimization algorithm to obtain their optimal values. Extensive experiments on diverse multi-view data sets demonstrate that our approach outperforms some state-of-the-art methods.",30,1,2024
One-Spike SNN: Single-Spike Phase Coding with Base Manipulation for ANN-to-SNN Conversion Loss Minimization,"As spiking neural networks (SNNs) are event-driven, energy efficiency is higher than conventional artificial neural networks (ANNs). Since SNN delivers data through discrete spikes, it is difficult to use gradient methods for training, limiting its accuracy. To keep the accuracy of SNNs similar to ANN counterparts, pre-trained ANNs are converted to SNNs (ANN-to-SNN conversion). During the conversion, encoding activations of ANNs to a set of spikes in SNNs is crucial for minimizing the conversion loss. In this work, we propose a single-spike phase coding as an encoding scheme that minimizes the number of spikes to transfer data between SNN layers. To minimize the encoding error due to single-spike approximation in phase coding, threshold shift and base manipulation are proposed. Without any additional retraining or architectural constraints on ANNs, the proposed conversion method does not lose inference accuracy (0.58% on average) verified on three convolutional neural networks (CNNs) with CIFAR and ImageNetthis http URLaddition, graph convolutional networks (GCNs) are converted to SNNs successfully with an average accuracy loss of 0.90%.Most importantly, the energy efficiency of our SNN improves by 4.6~17.3 X compared to the ANN baseline.",30,1,2024
"On finite group scheme-theoretical categories, II","Let $\mathcal{C}:=\mathcal{C}(G,\omega,H,\psi)$ be a finite group scheme-theoretical category over an algebraically closed field of characteristic $p\ge 0$ as defined by the first author. For any indecomposable exact module category over $\mathcal{C}$, we classify its simple objects and provide an expression for their projective covers, in terms of double cosets and projective representations of certain closed subgroup schemes of $G$. This upgrades a result of Ostrik for group-theoretical fusion categories in characteristic $0$, and generalizes our previous work for the case $\omega=1$. As a byproduct, we describe the simples and indecomposable projectives of $\mathcal{C}$. Finally, we apply our results to describe the blocks of the center of ${\rm Coh}(G,\omega)$.",29,1,2024
Product Calculus and Stokes Theorem,In this paper the analogy between differential forms arising from integrals in additive calculus and forms arising from the integrals in product calculus is investigated. It is found that with an appropriate definition of scalar multiplication and vector addition a set of vector spaces can be constructed analogous to what is found in exterior calculus. A product differential is defined which allows for the product derivative version of closed and exact forms. The product differential also allows for a product integral version of Stokes theorem (for scalar functions).,29,1,2024
Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation,"Misinformation has become a major challenge in the era of increasing digital information, requiring the development of effective detection methods. We have investigated a novel approach to Out-Of-Context detection (OOCD) that uses synthetic data generation. We created a dataset specifically designed for OOCD and developed an efficient detector for accurate classification. Our experimental findings validate the use of synthetic data generation and demonstrate its efficacy in addressing the data limitations associated with OOCD. The dataset and detector should serve as valuable resources for future research and the development of robust misinformation detection systems.",29,1,2024
Procedural terrain generation with style transfer,"In this study we introduce a new technique for the generation of terrain maps, exploiting a combination of procedural generation and Neural Style Transfer. We consider our approach to be a viable alternative to competing generative models, with our technique achieving greater versatility, lower hardware requirements and greater integration in the creative process of designers and developers. Our method involves generating procedural noise maps using either multi-layered smoothed Gaussian noise or the Perlin algorithm. We then employ an enhanced Neural Style transfer technique, drawing style from real-world height maps. This fusion of algorithmic generation and neural processing holds the potential to produce terrains that are not only diverse but also closely aligned with the morphological characteristics of real-world landscapes, with our process yielding consistent terrain structures with low computational cost and offering the capability to create customized maps. Numerical evaluations further validate our model's enhanced ability to accurately replicate terrain morphology, surpassing traditional procedural methods.",28,1,2024
Time-Quantitatively Nonblocking Supervisory Control of Timed Discrete-Event Systems,"Recently we proposed an automaton property of quantitative nonblockingness in supervisory control of discrete-event systems, which quantifies the standard nonblocking property by capturing the practical requirement that all tasks be completed within a bounded number of steps. However, in practice tasks may be further required to be completed in specific time; this requirement cannot be fulfilled by the quantitatively nonblocking supervisor. To meet this new requirement, in this paper we introduce the concept of time-quantitative nonblockingness, which extends the concept of quantitative nonblockingness from untimed discrete-event systems (DES) to timed DES. This property requires that each task must be completed within a bounded time. Accordingly, we formulate a new time-quantitatively nonblocking supervisory control problem of TDES, and characterize its solvability in terms of a new concept of time-quantitative language completability. It is proved that there exists a unique supremal time-quantitatively completable sublanguage of a given language, and we develop an automaton-based algorithm to compute the supremal sublanguage. Finally, we present an approach to compute a maximally permissive supervisory control solution to the new time-quantitative nonblocking supervisory control problem.",28,1,2024
5 Year Update to the Next Steps in Quantum Computing,"It has been 5 years since the Computing Community Consortium (CCC) Workshop on Next Steps in Quantum Computing, and significant progress has been made in closing the gap between useful quantum algorithms and quantum hardware. Yet much remains to be done, in particular in terms of mitigating errors and moving towards error-corrected machines. As we begin to transition from the Noisy-Intermediate Scale Quantum (NISQ) era to a future of fault-tolerant machines, now is an opportune time to reflect on how to apply what we have learned thus far and what research needs to be done to realize computational advantage with quantum machines.",26,1,2024
Modules over linear spaces admitting a multiplicative basis,"We study the structure of certain modules $V$ over linear spaces $W$ with restrictions neither on the dimensions nor on the base field $\mathbb F$. A basis $\mathfrak B = \{v_i\}_{i\in I}$ of $V$ is called multiplicative respect to the basis $\mathfrak B' = \{w_j\}_{j \in J}$ of $W$ if for any $i \in I, j \in J$ we have either $v_iw_j = 0$ or $0 \neq v_iw_j \in \mathbb Fv_k$ for some $k \in I$. We show that if $V$ admits a multiplicative basis then it decomposes as the direct sum $V=\bigoplus_k V_k$ of well-described submodules admitting each one a multiplicative basis. Also the minimality of $V$ is characterized in terms of the multiplicative basis and it is shown that the above direct sum is by means of the family of its minimal submodules, admitting each one a multiplicative basis.",23,1,2024
Faster Projected GAN: Towards Faster Few-Shot Image Generation,"In order to solve the problems of long training time, large consumption of computing resources and huge parameter amount of GAN network in image generation, this paper proposes an improved GAN network model, which is named Faster Projected GAN, based on Projected GAN. The proposed network is mainly focuses on the improvement of generator of Projected GAN. By introducing depth separable convolution (DSC), the number of parameters of the Projected GAN is reduced, the training speed is accelerated, and memory is saved. Experimental results show that on ffhq-1k, art-painting, Landscape and other few-shot image datasets, a 20% speed increase and a 15% memory saving are achieved. At the same time, FID loss is less or no loss, and the amount of model parameters is better controlled. At the same time, significant training speed improvement has been achieved in the small sample image generation task of special scenes such as earthquake scenes with few public datasets.",23,1,2024
Alya towards Exascale: Optimal OpenACC Performance of the Navier-Stokes Finite Element Assembly on GPUs,"This paper addresses the challenge of providing portable and highly efficient code structures for CPU and GPU architectures. We choose the assembly of the right-hand term in the incompressible flow module of the High-Performance Computational Mechanics code Alya, which is one of the two CFD codes in the Unified European Benchmark Suite. Starting from an efficient CPU-code and a related OpenACC-port for GPUs we successively investigate performance potentials arising from code specialization, algorithmic restructuring and low-level optimizations.We demonstrate that only the combination of these different dimensions of runtime optimization unveils the full performance potential on the GPU and CPU. Roofline-based performance modelling is applied in this process and we demonstrate the need to investigate new optimization strategies if a classical roofline limit such as memory bandwidth utilization is achieved, rather than stopping the process. The final unified OpenACC-based implementation boosts performance by more than 50x on an NVIDIA A100 GPU (achieving approximately 2.5 TF/s FP64) and a further factor of 5x for an Intel Icelake based CPU-node (achieving approximately 1.0 TF/s FP64).The insights gained in our manual approach lays ground implementing unified but still highly efficient code structures for related kernels in Alya and other applications. These can be realized by manual coding or automatic code generation frameworks.",22,1,2024
Leveraging Chat-Based Large Vision Language Models for Multimodal Out-Of-Context Detection,"Out-of-context (OOC) detection is a challenging task involving identifying images and texts that are irrelevant to the context in which they are presented. Large vision-language models (LVLMs) are effective at various tasks, including image classification and text generation. However, the extent of their proficiency in multimodal OOC detection tasks is unclear. In this paper, we investigate the ability of LVLMs to detect multimodal OOC and show that these models cannot achieve high accuracy on OOC detection tasks without fine-tuning. However, we demonstrate that fine-tuning LVLMs on multimodal OOC datasets can further improve their OOC detection accuracy. To evaluate the performance of LVLMs on OOC detection tasks, we fine-tune MiniGPT-4 on the NewsCLIPpings dataset, a large dataset of multimodal OOC. Our results show that fine-tuning MiniGPT-4 on the NewsCLIPpings dataset significantly improves the OOC detection accuracy in this dataset. This suggests that fine-tuning can significantly improve the performance of LVLMs on OOC detection tasks.",22,1,2024
Constrained Reinforcement Learning for Adaptive Controller Synchronization in Distributed SDN,"In software-defined networking (SDN), the implementation of distributed SDN controllers, with each controller responsible for managing a specific sub-network or domain, plays a critical role in achieving a balance between centralized control, scalability, reliability, and network efficiency. These controllers must be synchronized to maintain a logically centralized view of the entire network. While there are various approaches for synchronizing distributed SDN controllers, most tend to prioritize goals such as optimization of communication latency or load balancing, often neglecting to address both the aspects simultaneously. This limitation becomes particularly significant when considering applications like Augmented and Virtual Reality (AR/VR), which demand constrained network latencies and substantial computational resources. Additionally, many existing studies in this field predominantly rely on value-based reinforcement learning (RL) methods, overlooking the potential advantages offered by state-of-the-art policy-based RL algorithms. To bridge this gap, our work focuses on examining deep reinforcement learning (DRL) techniques, encompassing both value-based and policy-based methods, to guarantee an upper latency threshold for AR/VR task offloading within SDN environments, while selecting the most cost-effective servers for AR/VR task offloading. Our evaluation results indicate that while value-based methods excel in optimizing individual network metrics such as latency or load balancing, policy-based approaches exhibit greater robustness in adapting to sudden network changes or reconfiguration.",21,1,2024
Veagle: Advancements in Multimodal Representation Learning,"Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by the successes and insights of previous works. Veagle leverages a dynamic mechanism to project encoded visual information directly into the language model. This dynamic approach allows for a more nuanced understanding of intricate details present in visual contexts. To validate the effectiveness of Veagle, we conduct comprehensive experiments on benchmark datasets, emphasizing tasks such as visual question answering and image understanding. Our results indicate a improvement of 5-6 \% in performance, with Veagle outperforming existing models by a notable margin. The outcomes underscore the model's versatility and applicability beyond traditional benchmarks.",18,1,2024
Delay-independent dual-rate PID controller for a packetbased Networked Control System,"In this paper, a novel delay-independent control structure for a networked control system (NCS) is proposed, where packet-based control strategies with predictor-based and dual-rate control techniques are integrated. The control solution is able to cope with some networked communication problems such as time-varying delays, packet dropouts and packet disorder. In addition, the proposed approach enables to reduce network load, and usage of connected devices, while maintaining a satisfactory control performance. As a delayindependent control solution, no network-induced delay measurement is needed for controller implementation. In addition, the control scheme is applicable to open-loop unstable plants. Control system stability is ensured in terms of linear matrix inequalities (LMIs). Simulation results show the main benefits of the control approach, which are experimentally validated by means of a Cartesian-robot-based test-bed platform.",17,1,2024
Seg-metrics: a Python package to compute segmentation metrics,"In response to a concerning trend of selectively emphasizing metrics in medical image segmentation (MIS) studies, we introduce \texttt{seg-metrics}, an open-source Python package for standardized MIS model evaluation. Unlike existing packages, \texttt{seg-metrics} offers user-friendly interfaces for various overlap-based and distance-based metrics, providing a comprehensive solution. \texttt{seg-metrics} supports multiple file formats and is easily installable through the Python Package Index (PyPI). With a focus on speed and convenience, \texttt{seg-metrics} stands as a valuable tool for efficient MIS model assessment.",12,1,2024
Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection,"Vision Transformers (ViTs) have become increasingly popular in large-scale Vision and Language Pre-training (VLP) models. Although previous VLP research has demonstrated the efficacy of ViTs, these efforts still struggle with computational inefficiencies caused by lengthy visual sequences. To address this challenge, we introduce an efficient VLP approach called TRIPS, which stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the visual sequence using a text-guided patch-selection layer in the visual backbone, thereby accelerating both training and inference processes. This patch-selection layer dynamically computes text-dependent visual attention, enabling it to identify attentive image tokens with text guidance and fuse inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any extra parameters and generalizes to most ViT-based VLP models. We incorporate TRIPS into three representative VLP models covering single-stream, dual-stream, and generative paradigms, and conduct extensive experiments on five widely-used multi-modal benchmark datasets. Our experimental results reveal that TRIPS delivers a 40% speedup, while maintaining competitive or superior performance on downstream tasks.",11,1,2024
GPU-accelerated Linear Algebra for Coupled Solvers in Industrial CFD Applications with OpenFOAM,"The present work describes the development of heterogeneous GPGPU implicit CFD coupled solvers, encompassing both density- and pressure- based approaches. In this setup, the assembled linear matrix is offloaded onto multiple GPUs using specialized external libraries to solve the linear problem efficiently. These coupled solvers are applied to two industrial test cases representing common scenarios: the NASA CRM in a transonic regime and the external aerodynamics study of the DriveAER car. Significant performance enhancements are evident when compared to their CPU counterparts. Specifically, the NASA CRM case achieves an overall speedup of more than 4x, while the DriveAER test case demonstrates improved stability and reduced computational time compared to segregated solvers. All calculations were carried out utilizing the GPU-based partition of the davinci-1 supercomputer at the Leonardo Labs, featuring 82 GPU-accelerated nodes.",10,1,2024
Epidemic modelling requires knowledge of the social network,"Compartmental models of epidemics are widely used to forecast the effects of communicable diseases such as COVID-19 and to guide policy. Although it has long been known that such processes take place on social networks, the assumption of random mixing is usually made, which ignores network structure. However, super-spreading events have been found to be power-law distributed, suggesting that the underlying networks may be scale free or at least highly heterogeneous. The random-mixing assumption would then produce an overestimation of the herd-immunity threshold for given $R_0$; and a (more significant) overestimation of $R_0$ itself. These two errors compound each other, and can lead to forecasts greatly overestimating the number of infections. Moreover, if networks are heterogeneous and change in time, multiple waves of infection can occur, which are not predicted by random mixing. A simple SIR model simulated on both Erdős-Rényi and scale-free networks shows that details of the network structure can be more important than the intrinsic transmissibility of a disease. It is therefore crucial to incorporate network information into standard models of epidemics.",9,1,2024
"Monte Carlo algorithm for calculating pre-neutron fragment mass and kinetic energy distributions from measurements using the 2E Technique for reaction 235U(nth, f)","An algorithm is proposed that enables the calculation of pre-neutron mass and energy distributions of fission fragments from uranium-235, induced by thermal neutrons, utilizing measurements obtained through the 2E technique. This algorithm facilitates the inference of the curve representing the average multiplicity of prompt neutrons relative to the pre-neutron mass of the fission fragments.",8,1,2024
AI incidents and 'networked trouble': The case for a research agenda,"Against a backdrop of widespread interest in how publics can participate in the design of AI, I argue for a research agenda focused on AI incidents - examples of AI going wrong and sparking controversy - and how they are constructed in online environments. I take up the example of an AI incident from September 2020, when a Twitter user created a 'horrible experiment' to demonstrate the racist bias of Twitter's algorithm for cropping images. This resulted in Twitter not only abandoning its use of that algorithm, but also disavowing its decision to use any algorithm for the task. I argue that AI incidents like this are a significant means for participating in AI systems that require further research. That research agenda, I argue, should focus on how incidents are constructed through networked online behaviours that I refer to as 'networked trouble', where formats for participation enable individuals and algorithms to interact in ways that others - including technology companies - come to know and come to care about. At stake, I argue, is an important mechanism for participating in the design and deployment of AI.",7,1,2024
Heating from Above in Non-scattering Suspensions: Phototactic Bioconvection under Collimated Irradiation,"Examining phototactic bioconvection in non-scattering suspensions with upper heating and collimated irradiation, this study delves into the intricate dynamics influenced by light and microorganisms. The study focuses on the linear stability of the basic state, examining neutral curves. The numerical analysis involves solving a system of equations using the MATLAB bvp4c solver. The investigation considers the impact of parameters, such as the thermal Rayleigh number, critical total intensity, and Lewis number, on the critical bioconvection Rayleigh number. As the critical total intensity varies, a transition from a stationary to an oscillatory solution (and vice versa) is observed. Phototactic microorganisms are incorporated into the model, and results show how varying parameters affect convection patterns and stability. The findings reveal interesting phenomena, including Hopf bifurcations and limit cycles.",30,1,2024
Asymptotic estimations of a perturbed symmetric eigenproblem,"We study ill-conditioned positive definite matrices that are disturbed by the sum of $m$ rank-one matrices of a specific form. We provide estimates for the eigenvalues and eigenvectors. When the condition number of the initial matrix tends to infinity, we bound the values of the coordinates of the eigenvectors of the perturbed matrix. Equivalently, in the coordinate system where the initial matrix is diagonal, we bound the rate of convergence of coordinates that tend to zero.",10,1,2024
Test for amenability for extensions of infinite residually finite groups,"Let $G$ be a countable group that admits a minimal equicontinuous action on the Cantor set (for example, a residually finite group). We show that the family of almost automorphic actions of $G$ on the Cantor set, is a test for amenability for $G$. More precisely, we show that if $G$ is non-amenable, then for every minimal equicontinuous system $(Y, G)$, such that $Y$ is a Cantor set, there exists a Toeplitz subshift with no invariant probability measures, whose maximal equicontinuous factor is $(Y,G)$. Consequently, we obtain that $G$ is amenable if and only if every Toeplitz $G$-subshift has at least one invariant probability measure.",10,1,2024
Local sensitivity analysis of heating degree day and cooling degree day temperature derivatives prices,We study the local sensitivity of heating degree day (HDD) and cooling degree day (CDD) temperature futures and option prices with respect to perturbations in the deseasonalized temperature or in one of its derivatives up to a certain order determined by the continuous-time autoregressive process modelling the deseasonalized temperature in the HDD and CDD indexes. We also consider an empirical case where a CAR process of autoregressive order 3 is fitted to New York temperatures and we perform a study of the local sensitivity of these financial contracts and a posterior analysis of the results.,24,1,2024
Organic electrochemical neurons and synapses with ion mediated spiking,"Future brain-machine interfaces, prosthetics, and intelligent soft robotics will require integrating artificial neuromorphic devices with biological systems. Due to their poor biocompatibility, circuit complexity, low energy efficiency, and operating principles fundamentally different from the ion signal modulation of biology, traditional Silicon-based neuromorphic implementations have limited bio-integration potential. Here, we report the first organic electrochemical neurons (OECNs) with ion-modulated spiking, based on allprinted complementary organic electrochemical transistors. We demonstrate facile biointegration of OECNs with Venus Flytrap (Dionaea muscipula) to induce lobe closure upon input stimuli. The OECNs can also be integrated with all-printed organic electrochemical synapses (OECSs), exhibiting short-term plasticity with paired-pulse facilitation and longterm plasticity with retention >1000 s, facilitating Hebbian learning. These soft and flexible OECNs operate below 0.6 V and respond to multiple stimuli, defining a new vista for localized artificial neuronal systems possible to integrate with bio-signaling systems of plants, invertebrates, and vertebrates.",18,1,2024
Evaporation of bacteria-laden surrogate respiratory fluid droplets: On a hydrophilic substrate versus contact-free environment confers differential bacterial infectivity,"The transmission of viruses/ bacteria cause infection predominantly via aerosols. The transmission mechanism of respiratory diseases is complex, including direct or indirect contact, large droplet, and airborne routes apart from close contact transmission. With this pretext, we have investigated two modes of droplet evaporation to understand its significance in airborne disease transmission; a droplet in a contact-free environment, which evaporates and forms droplet nuclei, and a droplet on a hydrophilic substrate (fomite). The study examines mass transport, the deposition pattern of bacteria in the precipitates, and their survival and virulence. The osmotic pressure increases with the salt concentration, inactivating the bacteria embedded in the precipitates with accelerated evaporation. Further, the bacteria's degree of survival and enhanced pathogenicity are compared for both evaporation modes. The striking differences in pathogenicity are attributed to the evaporation rate, oxygen availability, and reactive oxygen species (ROS) generation.",11,1,2024
Radical Pair Mechanism and the Role of Chirality-Induced Spin Selectivity during Planaria Regeneration: Effect of Weak Magnetic Field on ROS levels,"Planarian is an intriguing model system wherein the effect of electric and magnetic fields can be studied on various biochemical pathways during cell morphogenesis. Recent experimental observations have demonstrated the non-trivial modulation of reactive oxygen species (ROS) levels by a weak magnetic field during planaria regeneration. However, the underlying biophysical mechanism behind this remains elusive. In this paper, we study the radical pair mechanism to explain the effect of weak magnetic fields on ROS modulation during planaria regeneration to explain the experimental results. We also investigate the effect of chirality-induced spin selectivity (CISS) on ROS levels by including it in the framework of the radical pair mechanism. We conclude that the inclusion of CISS explains the experimental results better and allows the radical pair model to have more parametric space to satisfy the experimental constraints. This study explains the crucial process of ROS modulation by the weak magnetic field with and without CISS, thereby paving the way to unraveling the vast domain of ROS modulation for desired outcomes.",9,1,2024
K-Primitivity : A Literature Survey,"A nonnegative matrix A is said to be primitive if there exists a positive integer m such that entries in A^m are positive and smallest such m is called the exponent of A: Primitive matrices are useful in the study of finite Markov chains theory. In 1998, in the context of finite Markov chains, Ettore Fornasini and Maria Elena Valcher [6] extended the notion of primitivity for a nonnegative matrix pair (A;B) by considering a positive discrete homogeneous two-dimensional (2D) state model. Further generalization to this notion of primitivity for k-tuple (A1;A2;...;Ak) of nonnegative matrices A1;A2;...;Ak is quite natural and known as k-primitivity. In this paper we present various results on k-primitivity given by different researchers from time to time.",29,1,2024
Adjusting Dynamics of Hopfield Neural Network via Time-variant Stimulus,"As a paradigmatic model for nonlinear dynamics studies, the Hopfield Neural Network (HNN) demonstrates a high susceptibility to external disturbances owing to its intricate structure. This paper delves into the challenge of modulating HNN dynamics through time-variant stimuli. The effects of adjustments using two distinct types of time-variant stimuli, namely the Weight Matrix Stimulus (WMS) and the State Variable Stimulus (SVS), along with a Constant Stimulus (CS) are reported. The findings reveal that deploying four WMSs enables the HNN to generate either a four-scroll or a coexisting two-scroll attractor. When combined with one SVS, four WMSs can lead to the formation of an eight-scroll or four-scroll attractor, while the integration of four WMSs and multiple SVSs can induce grid-multi-scroll attractors. Moreover, the introduction of a CS and an SVS can significantly disrupt the dynamic behavior of the HNN. Consequently, suitable adjustment methods are crucial for enhancing the network's dynamics, whereas inappropriate applications can lead to the loss of its chaotic characteristics. To empirically validate these enhancement effects, the study employs an FPGA hardware platform. Subsequently, an image encryption scheme is designed to demonstrate the practical application benefits of the dynamically adjusted HNN in secure multimedia communication. This exploration into the dynamic modulation of HNN via time-variant stimuli offers insightful contributions to the advancement of secure communication technologies.",15,1,2024
Binding-Adaptive Diffusion Models for Structure-Based Drug Design,"Structure-based drug design (SBDD) aims to generate 3D ligand molecules that bind to specific protein targets. Existing 3D deep generative models including diffusion models have shown great promise for SBDD. However, it is complex to capture the essential protein-ligand interactions exactly in 3D space for molecular generation. To address this problem, we propose a novel framework, namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively extract subcomplex, the essential part of binding sites responsible for protein-ligand interactions. Then the selected protein-ligand subcomplex is processed with SE(3)-equivariant neural networks, and transmitted back to each atom of the complex for augmenting the target-aware 3D molecule diffusion generation with binding interaction information. We iterate this hierarchical complex-subcomplex process with cross-hierarchy interaction node for adequately fusing global binding context between the complex and its corresponding subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can generate molecules with more realistic 3D structures and higher binding affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while maintaining proper molecular properties. Our code is available atthis https URL",15,1,2024
Streamlining the Selection Phase of Systematic Literature Reviews (SLRs) Using AI-Enabled GPT-4 Assistant API,"The escalating volume of academic literature presents a formidable challenge in staying updated with the newest research developments. Addressing this, this study introduces a pioneering AI-based tool, configured specifically to streamline the efficiency of the article selection phase in Systematic Literature Reviews (SLRs). Utilizing the robust capabilities of OpenAI's GPT-4 Assistant API, the tool successfully homogenizes the article selection process across a broad array of academic disciplines. Implemented through a tripartite approach consisting of data preparation, AI-mediated article assessment, and structured result presentation, this tool significantly accelerates the time-consuming task of literature reviews. Importantly, this tool could be highly beneficial in fields such as management and economics, where the SLR process involves substantial human judgment. The adoption of a standard GPT model can substantially reduce potential biases and enhance the speed and precision of the SLR selection phase. This not only amplifies researcher productivity and accuracy but also denotes a considerable stride forward in the way academic research is conducted amidst the surging body of scholarly publications.",14,1,2024
Multi-objective Optimal Roadside Units Deployment in Urban Vehicular Networks,"The significance of transportation efficiency, safety, and related services is increasing in urban vehicular networks. Within such networks, roadside units (RSUs) serve as intermediates in facilitating communication. Therefore, the deployment of RSUs is of utmost importance in ensuring the quality of communication services. However, the optimization objectives, such as time delay and deployment cost, are commonly developed from diverse perspectives. As a result, it is possible that conflicts may arise among the objectives. Furthermore, in urban environments, the presence of various obstacles, such as buildings, gardens, lakes, and other infrastructure, poses challenges for the deployment of RSUs. Hence, the deployment encounters significant difficulties due to the existence of multiple objectives, constraints imposed by obstacles, and the necessity to explore a large-scale optimization space. To address this issue, two versions of multi-objective optimization algorithms are proposed in this paper. By utilizing a multi-population strategy and an adaptive exploration technique, the methods efficiently explore a large-scale decision-variable space. In order to mitigate the issue of an overcrowded deployment of RSUs, a calibrating mechanism is adopted to adjust RSU density during the optimization procedures. The proposed methods also take care of data offloading between vehicles and RSUs by setting up an iterative best response sequence game (IBRSG). By comparing the proposed algorithms with several state-of-the-art algorithms, the results demonstrate that our strategies perform better in both high-density and low-density urban scenarios. The results also indicate that the proposed solutions substantially improve the efficiency of vehicular networks.",14,1,2024
A short note on deformations of (strongly) Gorenstein-projective modules over the dual numbers,"Let $\mathbf{k}$ be a field of arbitrary characteristic, and let $\Lambda$ be a finite dimensional $\mathbf{k}$-algebra. In this short note we prove that if $V$ is a finitely generated strongly Gorenstein-projective left $\Lambda$-module whose stable endomorphism ring $\underline{\mathrm{End}}_{\Lambda}(V)$ is isomorphic to $\mathbf{k}$, then $V$ has an universal deformation ring $R(\Lambda,V)$ isomorphic to the ring of dual numbers $\mathbf{k}[\epsilon]$ with $\epsilon^2=0$. As a consequence, we obtain the following result. Assume that $Q$ is a finite connected acyclic quiver, let $\mathbf{k} Q$ be the corresponding path algebra and let $\Lambda = \mathbf{k} Q[\epsilon] = \mathbf{k} Q\otimes_{\mathbf{k}} \mathbf{k}[\epsilon]$. If $V$ is a finitely generated Gorenstein-projective left $\Lambda$-module with $\underline{\mathrm{End}}_{\Lambda}(V)=\mathbf{k}$, then $V$ has an universal deformation ring $R(\Lambda,V)$ isomorphic to $\mathbf{k}[\epsilon]",13,1,2024
Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image,"The parametric constant false alarm rate (CFAR) detection algorithms which are based on various statistical distributions, such as Gaussian, Gamma, Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most widely used to detect the ship targets in SAR image at present. However, the clutter background in SAR images is complicated and variable. When the actual clutter background deviates from the assumed statistical distribution, the performance of the parametric CFAR detector will deteriorate. In addition to the parametric CFAR schemes, there is another class of nonparametric CFAR detectors which can maintain a constant false alarm rate for the target detection without the assumption of a known clutter distribution. In this work, the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is proposed and analyzed, and a closed form of the false alarm rate for the Wilcoxon nonparametric detector to determine the decision threshold is presented. By comparison with several typical parametric CFAR schemes on Radarsat-2, ICEYE-X6 and Gaofen-3 SAR images, the robustness of the Wilcoxon nonparametric detector to maintain a good false alarm performance in different detection backgrounds is revealed, and its detection performance for the weak ship in rough sea surface is improved to some extent. Moreover, the Wilcoxon nonparametric detector can suppress the false alarms resulting from the sidelobes at some degree and its detection speed is fast.",11,1,2024
"Reply to Comment on ""Doppler signature in electrodynamic retarded potentials""","Calin Galeriu(CG) has commented [1] about our manuscript, ""Doppler signature in electrodynamic retarded potentials"" [2], that must be properly addressed, amending at the same time of notation errors [3] to clarify some formal aspects which were discussed and criticized in C. Galeriu's comments. The same author (CG) has published three papers concerning the origin of the Doppler term, v/c, in the Liénard-Wiechert (L-W) potentials [4-6].",10,1,2024
Motion Guided Token Compression for Efficient Masked Video Modeling,"Recent developments in Transformers have achieved notable strides in enhancing video comprehension. Nonetheless, the O($N^2$) computation complexity associated with attention mechanisms presents substantial computational hurdles when dealing with the high dimensionality of videos. This challenge becomes particularly pronounced when striving to increase the frames per second (FPS) to enhance the motion capturing capabilities. Such a pursuit is likely to introduce redundancy and exacerbate the existing computational limitations. In this paper, we initiate by showcasing the enhanced performance achieved through an escalation in the FPS rate. Additionally, we present a novel approach, Motion Guided Token Compression (MGTC), to empower Transformer models to utilize a smaller yet more representative set of tokens for comprehensive video representation. Consequently, this yields substantial reductions in computational burden and remains seamlessly adaptable to increased FPS rates. Specifically, we draw inspiration from video compression algorithms and scrutinize the variance between patches in consecutive video frames across the temporal dimension. The tokens exhibiting a disparity below a predetermined threshold are then masked. Notably, this masking strategy effectively addresses video redundancy while conserving essential information. Our experiments, conducted on widely examined video recognition datasets, Kinetics-400, UCF101 and HMDB51, demonstrate that elevating the FPS rate results in a significant top-1 accuracy score improvement of over 1.6, 1.6 and 4.0. By implementing MGTC with the masking ratio of 25\%, we further augment accuracy by 0.1 and simultaneously reduce computational costs by over 31\% on Kinetics-400. Even within a fixed computational budget, higher FPS rates paired with MGTC sustain performance gains when compared to lower FPS settings.",10,1,2024
Improved Forecasting Using a PSO-RDV Framework to Enhance Artificial Neural Network,"Decision making and planning have long relied heavily on AI-driven forecasts. The government and the general public are working to minimize the risks while maximizing benefits in the face of potential future public health uncertainties. This study used an improved method of forecasting utilizing the Random Descending Velocity Inertia Weight (RDV IW) technique to improve the convergence of Particle Swarm Optimization (PSO) and the accuracy of Artificial Neural Network (ANN). The IW technique, inspired by the motions of a golf ball, modified the particles' velocities as they approached the solution point to a parabolically descending structure. Simulation results revealed that the proposed forecasting model with [0.4, 0.9] combination of alpha and alpha_dump exhibits a 6.36% improvement in position error and 11.75% improvement in computational time compared to the old model, thus, improving its convergence. It reached the optimum level at minimal steps with 12.50% improvement as against the old model since it provides better velocity averages when speed stabilization occurs at the 24th iteration. Meanwhile, the computed p-values for NRMSE (0.04889174), MAE (0.02829063), MAPE (0.02226053), WAPE (0.01701545), and R2 (0.00000021) of the proposed algorithm are less than the set 0.05 level of significance, thus the values indicated a significant result in terms of accuracy performance. Applying the modified ANN-PSO using RDV IW technique greatly improved the new HIV/AIDS forecasting model compared with the two models.",10,1,2024
Handover Management through Reconfigurable Intelligent Surfaces for VLC under Blockage Conditions,"In this paper, we consider an indoor visible light communication (VLC) system with multiple ""white"" light emitting diodes serving to form overlapping wireless communication cells. In order to maintain seamless connectivity to mobile users, a handover procedure should be implemented. In particular, practical conditions such as blockages due to obstacles inside the room environment and the mobility of users can affect direct VLC connectivity. The use of reconfigurable intelligent surfaces (RISs) in optical wireless systems allows to exploit non-direct connectivity links, thus providing efficient communication links. In this paper, we present a proactive handover mechanism that exploits the presence of a RIS, in order to redirect the communication links in case of blockages. The proposed approach has been implemented both in hard and soft modes and assessed in terms of achievable data rate and handover latency for a user walking in a given reference room at different user speeds and blockage conditions. Our presented results and comparisons with conventional handover methods (i.e., without RIS) are helpful in showing the superiority of the presented algorithm.",29,1,2024
NFT1000: A Visual Text Dataset For Non-Fungible Token Retrieval,"With the rise of 'Metaverse' and 'Web3.0', NFT ( Non-Fungible Token ) has emerged as a kind of pivotal digital asset, garnering significant attention. By the end of November 2023, more than 1.4 billion NFT tokens have been minted across various blockchain platforms. To effectively locate a satisfactory NFT token, conducting searches within the extensive array of NFT data is essential. The challenge in NFT retrieval is heightened due to the high degree of similarity among different NFT tokens, in terms of regional and semantic aspects. Achieving accurate and efficient retrieval within the large-scale, highly similar NFT data presents a formidable challenge for both the academic and industrial communities. In this paper, we will introduce a dataset named 'NFT Top1000 Visual Text Dataset'(henceforth, NFT1000), containing 7.56 million image-text pairs, and being collected from 1000 most famous PFP NFT collections by sales volume on the Ethereum blockchain. Based on the dataset, we test the CLIP (Contrastive Language-Image Pretraining) models as a baseline. Additionally, we also propose a concept of Comprehensive Variance Index (CVI in short), which is a robust metric designed to assess the similarity and retrieval difficulty of visual-text pairs data.",29,1,2024
Bike3S: A Tool for Bike Sharing Systems Simulation,"Vehicle sharing systems are becoming increasingly popular. The effectiveness of such systems depends, among other factors, on different strategic and operational management decisions and policies, like the dimension of the fleet or the distribution of vehicles. It is of foremost importance to be able to anticipate and evaluate the potential effects of such strategies before they can be successfully deployed. In this paper we present Bike3S, a simulator for a station-based bike sharing system. The simulator performs semi-realistic simulations of the operation of a bike sharing system and allows for evaluating and testing different management decisions and strategies. In particular, the simulator has been designed to test different station capacities, station distributions, and balancing strategies. The simulator carries out microscopic agent-based simulations, where users of different types can be defined that act according to their individual goals and objectives which influences the overall dynamics of the whole system.",24,1,2024
Pioneering Deterministic Scheduling and Network Structure Optimization for Time-Critical Computing Tasks in Industrial IoT,"The Industrial Internet of Things (IIoT) has become a critical technology to accelerate the process of digital and intelligent transformation of industries. As the cooperative relationship between smart devices in IIoT becomes more complex, getting deterministic responses of IIoT periodic time-critical computing tasks becomes a crucial and nontrivial problem. However, few current works in cloud/edge/fog computing focus on this problem. This paper is a pioneer to explore the deterministic scheduling and network structural optimization problems for IIoT periodic time-critical computing tasks. We first formulate the two problems and derive theorems to help quickly identify computation and network resource sharing conflicts. Based on this, we propose a deterministic scheduling algorithm, \textit{IIoTBroker}, which realizes deterministic response for each IIoT task by optimizing the fine-grained computation and network resources allocations, and a network optimization algorithm, \textit{IIoTDeployer}, providing a cost-effective structural upgrade solution for existing IIoT networks. Our methods are illustrated to be cost-friendly, scalable, and deterministic response guaranteed with low computation cost from our simulation results.",24,1,2024
Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?,"In the European context, both the EU AI Act proposal and the draft Standardisation Request on safe and trustworthy AI link standardisation to fundamental rights. However, these texts do not provide any guidelines that specify and detail the relationship between AI standards and fundamental rights, its meaning or implication. This chapter aims to clarify this critical regulatory blind spot. The main issue tackled is whether the adoption of AI harmonised standards, based on the future AI Act, should take into account fundamental rights. In our view, the response is yes. The high risks posed by certain AI systems relate in particular to infringements of fundamental rights. Therefore, mitigating such risks involves fundamental rights considerations and this is what future harmonised standards should reflect. At the same time, valid criticisms of the European standardisation process have to be addressed. Finally, the practical incorporation of fundamental rights considerations in the ongoing European standardisation of AI systems is discussed.",23,1,2024
Microscopic Origin of Criticality at Macroscale in QCD Chiral Phase Transition,"We reveal that the criticality of the chiral phase transition in QCD at the macroscale arises from the microscopic energy levels of its fundamental constituents, the quarks. We establish a novel relation between cumulants of the chiral order parameter (i.e., chiral condensate) and correlations among the energy levels of quarks (i.e., eigenspectra of the massless Dirac operator), which naturally leads to a generalization of the Banks-Casher relation. Based on this novel relation and through (2+1)-flavor lattice QCD calculations using the HISQ action with varying light quark masses in the vicinity of the chiral phase transition, we demonstrate that the correlations among the infrared part of the Dirac eigenspectra exhibit same universal scaling behaviors as expected of the cumulants of the chiral condensate. We find that these universal scaling behaviors extend up to the physical values of the up and down quark masses.",22,1,2024
Computation Rate Maximization for Wireless Powered Edge Computing With Multi-User Cooperation,"The combination of mobile edge computing (MEC) and radio frequency-based wireless power transfer (WPT) presents a promising technique for providing sustainable energy supply and computing services at the network edge. This study considers a wireless-powered mobile edge computing system that includes a hybrid access point (HAP) equipped with a computing unit and multiple Internet of Things (IoT) devices. In particular, we propose a novel muti-user cooperation scheme to improve computation performance, where collaborative clusters are dynamically formed. Each collaborative cluster comprises a source device (SD) and an auxiliary device (AD), where the SD can partition the computation task into various segments for local processing, offloading to the HAP, and remote execution by the AD with the assistance of the HAP. Specifically, we aims to maximize the weighted sum computation rate (WSCR) of all the IoT devices in the network. This involves jointly optimizing collaboration, time and data allocation among multiple IoT devices and the HAP, while considering the energy causality property and the minimum data processing requirement of each device. Initially, an optimization algorithm based on the interior-point method is designed for time and data allocation. Subsequently, a priority-based iterative algorithm is developed to search for a near-optimal solution to the multi-user collaboration scheme. Finally, a deep learning-based approach is devised to further accelerate the algorithm's operation, building upon the initial two algorithms. Simulation results show that the performance of the proposed algorithms is comparable to that of the exhaustive search method, and the deep learning-based algorithm significantly reduces the execution time of the algorithm.",22,1,2024
Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models,"Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment andthis http URLrecent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. This study employs a unique set of dropout masks - none, random, bottomup, and topdown - to enhance model performance in analyzing ocular images. Our results demonstrate that the bottomup GFlowOut mask significantly improves accuracy, outperforming the traditional dropout approach.",21,1,2024
Joint Resource Allocation and Trajectory Design for Resilient Multi-UAV Communication Networks,"In contrast to terrestrial wireless networks, dynamic Unmanned Aerial Vehicle (UAV) networks are susceptible to unexpected link failures arising from UAV breakdowns or the depletion of its batteries. Drastic user rate fluctuations and sum rate drops can occur due to the unexpected UAV link failures. Previous research has focused primarily on re-establishing these links to maintain service continuity, while neglecting overall system performance, including sum rate and user rate fluctuations. This letter proposes a resilient UAV network design utilizing the modern portfolio theory (MPT), which jointly optimizes the bandwidth allocation, UAV-user association, and UAV trajectories to enhance the overall service stability. Specifically, the design incorporates a novel utility function based on MPT to achieve a better balance between the sum rate and user rate fluctuations. To solve the joint optimization problem, we propose an iterative algorithm based on alternating optimization (AO) and successive convex approximation (SCA). Simulation results show that our scheme outperforms the other two baselines in terms of sum rate and user rate fluctuations. Furthermore, the resilience requirement in terms of sum rate, user rate fluctuations and user fairness can be achieved by flexibly tuning weight factor in our proposed algorithm.",21,1,2024
Quantum Inspired Chaotic Salp Swarm Optimization for Dynamic Optimization,"Many real-world problems are dynamic optimization problems that are unknown beforehand. In practice, unpredictable events such as the arrival of new jobs, due date changes, and reservation cancellations, changes in parameters or constraints make the search environment dynamic. Many algorithms are designed to deal with stationary optimization problems, but these algorithms do not face dynamic optimization problems or manage them correctly. Although some optimization algorithms are proposed to deal with the changes in dynamic environments differently, there are still areas of improvement in existing algorithms due to limitations or drawbacks, especially in terms of locating and following the previously identified optima. With this in mind, we studied a variant of SSA known as QSSO, which integrates the principles of quantum computing. An attempt is made to improve the overall performance of standard SSA to deal with the dynamic environment effectively by locating and tracking the global optima for DOPs. This work is an extension of the proposed new algorithm QSSO, known as the Quantum-inspired Chaotic Salp Swarm Optimization (QCSSO) Algorithm, which details the various approaches considered while solving DOPs. A chaotic operator is employed with quantum computing to respond to change and guarantee to increase individual searchability by improving population diversity and the speed at which the algorithm converges. We experimented by evaluating QCSSO on a well-known generalized dynamic benchmark problem (GDBG) provided for CEC 2009, followed by a comparative numerical study with well-regarded algorithms. As promised, the introduced QCSSO is discovered as the rival algorithm for DOPs.",21,1,2024
"Revisiting Common Randomness, No-signaling and Information Structure in Decentralized Control",This work revisits the no-signaling condition for decentralized information structures. We produce examples to show that within the no-signaling polytope exist strategies that cannot be achieved by passive common randomness but instead require agents to either share their observations with a mediator or communicate directly with each other. This poses a question mark on whether the no-signaling condition truly captures the decentralized information structure in the strictest sense.,20,1,2024
Self-Tuning Network Control Architectures with Joint Sensor and Actuator Selection,"We formulate a mathematical framework for designing a self-tuning network control architecture, and propose a computationally-feasible greedy algorithm for online architecture optimization. In this setting, the locations of active sensors and actuators in the network, as well as the feedback control policy are jointly adapted using all available information about the network states and dynamics to optimize a performance criterion. We show that the case with full-state feedback can be solved with dynamic programming, and in the linear-quadratic setting, the optimal cost functions and policies are piecewise quadratic and piecewise linear, respectively. Our framework is extended for joint sensor and actuator selection for dynamic output feedback control with both control performance and architecture costs. For large networks where exhaustive architecture search is prohibitive, we describe a greedy heuristic for actuator selection and propose a greedy swapping algorithm for joint sensor and actuator selection. Via numerical experiments, we demonstrate a dramatic performance improvement of greedy self-tuning architectures over fixed architectures. Our general formulation provides an extremely rich and challenging problem space with opportunities to apply a wide variety of approximation methods from stochastic control, system identification, reinforcement learning, and static architecture design for practical model-based control.",19,1,2024
Interactive Mars Image Content-Based Search with Interpretable Machine Learning,"The NASA Planetary Data System (PDS) hosts millions of images of planets, moons, and other bodies collected throughout many missions. The ever-expanding nature of data and user engagement demands an interpretable content classification system to support scientific discovery and individual curiosity. In this paper, we leverage a prototype-based architecture to enable users to understand and validate the evidence used by a classifier trained on images from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to providing explanations, we investigate the diversity and correctness of evidence used by the content-based classifier. The work presented in this paper will be deployed on the PDS Image Atlas, replacing its non-interpretable counterpart.",19,1,2024
Negative magnetoresistance induced by longitudinal photons in Dirac/Weyl semimetals,"A low-energy model is built to study systems such as Dirac/Weyl semimetals, according to statistical quantum electrodynamics formalism. We report that the introduction of a pseudoscalar, associated to longitudinal photons propagating along a magnetic field B, could transforms a Dirac semimetal into a Weyl semimetal with a pair of Weyl nodes for each point of Dirac. The nodes are separated by a pseudovector electric field induced dynamically along B associated to a chiral effect on the Fermi surface. A topological quantum transition is produced between a chiral-and non chiral symmetry phase. A general expression to the longitudinal magnetoconductivity is found. It provides the possibility of generalizing the usual expressions of the magnetoconductivity reported in the literature. This has a quadratic dependence on B, which is associated with a positive contribution to the magnetoconductivity. This is a prominent signature of the chiral magnetic effect in Dirac/Weyl systems in parallel electric and magnetic fields. We report a chiral effect induced by longitudinal photons associated to a negative longitudinal magnetoresistance in Dirac systems via an axial anomaly relation. We show some numerical results, and reproduced with a high level of accuracy some of the experimental results, in the low temperature region, obtained to the magnetoresistance of ZrTe5 and Na3Bi. We believe that a wide variety of these semimetals can be studied by using our general expression to the negative longitudinal magnetoresistance.",19,1,2024
Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors,"In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and goal-oriented communication strategies becomes imperative. This integration will enable sensing, transmission, and processing of exclusively pertinent task data, ensuring conveyed information possesses understandable, pragmatic semantic significance, aligning with destination needs and goals. Without doubt, no communication is error free. Within this context, besides errors stemming from typical wireless communication dynamics, potential distortions between transmitter-intended and receiver-interpreted meanings can emerge due to limitations in semantic processing capabilities, as well as language and knowledge representation disparities between transmitters and receivers. The main contribution of this paper is two-fold. First, it proposes and details a novel mathematical modeling of errors stemming from language mismatches at both semantic and effectiveness levels. Second, it provides a novel algorithmic solution to counteract these types of errors which leverages optimal transport theory. Our numerical results show the potential of the proposed mechanism to compensate for language mismatches, thereby enhancing the attainability of reliable communication under noisy communication environments.",19,1,2024
A novel method to compute the contact surface area between an organ and cancer tissue,"With ""contact surface area"" (CSA) we refers to the area of contact between a tumor and an organ. This indicator has been identified as a predictive factor for surgical peri-operative parameters, particularly in the context of kidney cancer. However, state-of-the-art algorithms for computing the CSA rely on assumptions about the tumor shape and require manual human annotation. In this study, we introduce an innovative method that relies on 3D reconstructions of tumors and organs to provide an accurate and objective estimate of the CSA. Our approach consists of a segmentation protocol for reconstructing organs and tumors from Computed Tomography (CT) images and an algorithm leveraging the reconstructed meshes to compute the CSA. With the aim to contributing to the literature with replicable results, we provide an open-source implementation of our algorithm, along with an easy-to-use graphical user interface to support its adoption and widespread use. We evaluated the accuracy of our method using both a synthetic dataset and reconstructions of 87 real tumor-organ pairs.",19,1,2024
Fabrication of $^{108}$Cd target for the astrophysical p-process studies,"The detailed process of preparing enriched $^{108}$Cd targets on mylar and copper backing using the vacuum evaporation technique is described. These targets were employed in an experiment to measure the proton capture cross-section at energies significantly below the Coulomb barrier, for the astrophysical p-process studies. Due to the low melting point and high vapor pressure of cadmium, some adjustments were implemented in the Telemark multipocket e-beam setup. The target thickness was determined through the measurement of alpha particle energy loss from a triple alpha source and also by RBS measurements. The thickness of the $^{108}$Cd films varies between 290 to 660 $\mu$g/cm$^2$, with a non-uniformity of approximately 10$\%$. X-ray Photoelectron Spectroscopy (XPS) and X-ray Fluorescence (XRF) analyses were conducted to examine the presence of impurities and to assess surface morphology, phase, and chemical composition.",19,1,2024
MB-RACS: Measurement-Bounds-based Rate-Adaptive Image Compressed Sensing Network,"Conventional compressed sensing (CS) algorithms typically apply a uniform sampling rate to different image blocks. A more strategic approach could be to allocate the number of measurements adaptively, based on each image block's complexity. In this paper, we propose a Measurement-Bounds-based Rate-Adaptive Image Compressed Sensing Network (MB-RACS) framework, which aims to adaptively determine the sampling rate for each image block in accordance with traditional measurement bounds theory. Moreover, since in real-world scenarios statistical information about the original image cannot be directly obtained, we suggest a multi-stage rate-adaptive sampling strategy. This strategy sequentially adjusts the sampling ratio allocation based on the information gathered from previous samplings. We formulate the multi-stage rate-adaptive sampling as a convex optimization problem and address it using a combination of Newton's method and binary search techniques. Additionally, we enhance our decoding process by incorporating skip connections between successive iterations to facilitate a richer transmission of feature information across iterations. Our experiments demonstrate that the proposed MB-RACS method surpasses current leading methods, with experimental evidence also underscoring the effectiveness of each module within our proposed framework.",19,1,2024
Attention Based Molecule Generation via Hierarchical Variational Autoencoder,"Molecule generation is a task made very difficult by the complex ways in which we represent molecules computationally. A common technique used in molecular generative modeling is to use SMILES strings with recurrent neural networks built into variational autoencoders - but these suffer from a myriad of issues: vanishing gradients, long-range forgetting, and invalid molecules. In this work, we show that by combining recurrent neural networks with convolutional networks in a hierarchical manner, we are able to both extract autoregressive information from SMILES strings while maintaining signal and long-range dependencies. This allows for generations with very high validity rates on the order of 95% when reconstructing known molecules. We also observe an average Tanimoto similarity of .6 between test set and reconstructed molecules, which suggests our method is able to map between SMILES strings and their learned representations in a more effective way than prior works using similar methods.",18,1,2024
PyRQA -- Conducting Recurrence Quantification Analysis on Very Long Time Series Efficiently,"PyRQA is a software package that efficiently conducts recurrence quantification analysis (RQA) on time series consisting of more than one million data points. RQA is a method from non-linear time series analysis that quantifies the recurrent behaviour of systems. Existing implementations to RQA are not capable of analysing such very long time series at all or require large amounts of time to calculate the quantitative measures. PyRQA overcomes their limitations by conducting the RQA computations in a highly parallel manner. Building on the OpenCL framework, PyRQA leverages the computing capabilities of a variety of parallel hardware architectures, such as GPUs. The underlying computing approach partitions the RQA computations and enables to employ multiple compute devices at the same time. The goal of this publication is to demonstrate the features and the runtime efficiency of PyRQA. For this purpose we employ a real-world example, comparing the dynamics of two climatological time series, and a synthetic example, reducing the runtime regarding the analysis of a series consisting of over one million data points from almost eight hours using state-of-the-art RQA software to roughly 69 seconds using PyRQA.",18,1,2024
Characterization of principal bundles: the noncommutative algebraic case,"We review Hopf-Galois extensions, in particular faithfully flat ones, accepted to be the noncommutative algebraic dual of a principal bundle. We also make a short digression into how quantum groups relate to Hopf-Galois extensions. Several examples are given, in order to provide a satisfactory understanding of each topic.",18,1,2024
Investigating the Generalizability of Physiological Characteristics of Anxiety,"Recent works have demonstrated the effectiveness of machine learning (ML) techniques in detecting anxiety and stress using physiological signals, but it is unclear whether ML models are learning physiological features specific to stress. To address this ambiguity, we evaluated the generalizability of physiological features that have been shown to be correlated with anxiety and stress to high-arousal emotions. Specifically, we examine features extracted from electrocardiogram (ECG) and electrodermal (EDA) signals from the following three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE) dataset. We aim to understand whether these features are specific to anxiety or general to other high-arousal emotions through a statistical regression analysis, in addition to a within-corpus, cross-corpus, and leave-one-corpus-out cross-validation across instances of stress and arousal. We used the following classifiers: Support Vector Machines, LightGBM, Random Forest, XGBoost, and an ensemble of the aforementioned models. We found that models trained on an arousal dataset perform relatively well on a previously unseen stress dataset, and vice versa. Our experimental results suggest that the evaluated models may be identifying emotional arousal instead of stress. This work is the first cross-corpus evaluation across stress and arousal from ECG and EDA signals, contributing new findings about the generalizability of stress detection.",23,1,2024
"Formulation, Colloidal Characterization, and In Vitro Biological Effect of BMP-2 Loaded PLGA Nanoparticles for Bone Regeneration","Nanoparticles (NPs) based on the polymer poly (lactide-co-glycolide) acid (PLGA) have been widely studied in developing delivery systems for drugs and therapeutic biomolecules, due to the biocompatible and biodegradable properties of the PLGA. In this work, a synthesis method for bone morphogenetic protein (BMP-2)-loaded PLGA NPs was developed and optimized, in order to carry out and control the release of BMP-2, based on the double-emulsion (water/oil/water,W/O/W) solvent evaporation technique. The polymeric surfactant Pluronic F68 was used in the synthesis procedure, as it is known to have an effect on the reduction of the size of the NPs, the enhancement of their stability, and the protection of the encapsulated biomolecule. Spherical solid polymeric NPs were synthesized, showing a reproducible multimodal size distribution, with diameters between 100 and 500 nm. This size range appears to allow the protein to act on the cell surface and at the cytoplasm level. The effect of carrying BMP-2 co-adsorbed with bovine serum albumin on the NP surface was analyzed. The colloidal properties of these systems (morphology by SEM, hydrodynamic size, electrophoretic mobility, temporal stability, protein encapsulation, and short-term release profile) were studied. The effect of both BMP2-loaded NPs on the proliferation, migration, and osteogenic differentiation of mesenchymal stromal cells from human alveolar bone (ABSC) was also analyzed in vitro.",19,1,2024
Real-Time Seedless Post-Processing for Quantum Random Number Generators,"Quantum-proof randomness extraction is essential for handling quantum side information possessed by a quantum adversary, which is widely applied in various quantum cryptography tasks. In this study, we introduce a real-time two-source quantum randomness extractor against quantum side information. Our extractor is tailored for forward block sources, a novel category of min-entropy sources introduced in this work. These sources retain the flexibility to accommodate a broad range of quantum random number generators. Our online algorithms demonstrate the extraction of a constant fraction of min-entropy from two infinitely long independent forward block sources. Moreover, our extractor is inherently block-wise parallelizable, presenting a practical and efficient solution for the timely extraction of high-quality randomness. Applying our extractors to the raw data of one of the most commonly used quantum random number generators, we achieve a simulated extraction speed as high as 64 Gbps.",29,1,2024
Betting on what is neither verifiable nor falsifiable,"Prediction markets are useful for estimating probabilities of claims whose truth will be revealed at some fixed time -- this includes questions about the values of real-world events (i.e. statistical uncertainty), and questions about the values of primitive recursive functions (i.e. logical or algorithmic uncertainty). However, they cannot be directly applied to questions without a fixed resolution criterion, and real-world applications of prediction markets to such questions often amount to predicting not whether a sentence is true, but whether it will be proven. Such questions could be represented by countable unions or intersections of more basic events, or as First-Order-Logic sentences on the Arithmetical Hierarchy (or even beyond FOL, as hyperarithmetical sentences). In this paper, we propose an approach to betting on such events via options, or equivalently as bets on the outcome of a ""verification-falsification game"". Our work thus acts as an alternative to the existing framework of Garrabrant induction for logical uncertainty, and relates to the stance known as constructivism in the philosophy of mathematics; furthermore it has broader implications for philosophy and mathematical logic.",29,1,2024
Visual acuity and contrast sensitivity under monochromatic yellow light,"This study investigates the impact of monochromatic lighting on visual acuity (VA) and contrast sensitivity (CS). Traditional assessments of VA and CS are typically conducted under the illumination of ``white'' light, but variations in color temperature can influence outcomes. Utilizing data from an exhibition by Olafur Eliasson, where a room was illuminated with low-pressure sodium lamps, creating an almost monochromatic yellow light, we compared visual assessments in the yellow room with conventional lighting in a white room.For VA, the results show no significant differences between the two lighting conditions, while for CS, a more nuanced situation is observed. The bias in CS measurements is clinically relevant, and the p-value suggests that further investigation with a larger, more diverse sample may be worthwhile.Despite limitations, such as higher illumination conditions than standard protocols, the unique ``laboratory'' offered by the exhibition facilitated measurements not easily achievable in a traditional setting.",22,1,2024
Collision of two spinning billiard balls and the role of table,"We study the collision dynamics of a spinning cue ball approaching a static object ball with equal mass on a plane, common in billiards. While typical collisions in billiards are nearly perfectly elastic, with a restitution coefficient close to 1 and low friction, we explore three deviations from ideal elastic collisions: The non-elastic nature, the friction effects between the balls during collision, the friction between the ball and the table. We describe the detailed collision outcomes, emphasizing the importance of considering frictions. We reveal that friction, both between the balls and with the table, significantly influences the post-collision motions, deviating from the expectations of a purely elastic collision. The insights gained contribute to a better understanding of ball dynamics, impacting strategies and gameplay in billiards.",19,1,2024
Mechanical detection of nuclear decays,"More than a century of development has led to detectors that can precisely measure energy deposits from particles produced by radioactive decays. However, neutral particles emitted in such decays may escape detection if they do not subsequently interact within the detector. Here we report the detection of individual nuclear $\alpha$ decays through the mechanical recoil of the entire object in which the decaying nuclei are embedded. Momentum conservation ensures that such measurements are sensitive to any particles emitted in the decay, including neutral particles. Detection of the minuscule recoil of an object more than $10^{12}$ times more massive than the emitted particles is made possible by recently developed techniques in levitated optomechanics, which enable high-precision optical control and measurement of the mechanical motion of a micron-sized particle. The techniques developed here may find use in fields ranging from nuclear forensics to dark matter and neutrino physics.",18,1,2024
Deep Learning Based Event Reconstruction for Cyclotron Radiation Emission Spectroscopy,"The objective of the Cyclotron Radiation Emission Spectroscopy (CRES) technology is to build precise particle energy spectra. This is achieved by identifying the start frequencies of charged particle trajectories which, when exposed to an external magnetic field, leave semi-linear profiles (called tracks) in the time-frequency plane. Due to the need for excellent instrumental energy resolution in application, highly efficient and accurate track reconstruction methods are desired. Deep learning convolutional neural networks (CNNs) - particularly suited to deal with information-sparse data and which offer precise foreground localization - may be utilized to extract track properties from measured CRES signals (called events) with relative computational ease. In this work, we develop a novel machine learning based model which operates a CNN and a support vector machine in tandem to perform this reconstruction. A primary application of our method is shown on simulated CRES signals which mimic those of the Project 8 experiment - a novel effort to extract the unknown absolute neutrino mass value from a precise measurement of tritium $\beta^-$-decay energy spectrum. When compared to a point-clustering based technique used as a baseline, we show a relative gain of 24.1% in event reconstruction efficiency and comparable performance in accuracy of track parameter reconstruction.",5,1,2024
Comparing MCMC algorithms in Stochastic Volatility Models using Simulation Based Calibration,"Simulation Based Calibration (SBC) is applied to analyse two commonly used, competing Markov chain Monte Carlo algorithms for estimating the posterior distribution of a stochastic volatility model. In particular, the bespoke 'off-set mixture approximation' algorithm proposed by Kim, Shephard, and Chib (1998) is explored together with a Hamiltonian Monte Carlo algorithm implemented through Stan. The SBC analysis involves a simulation study to assess whether each sampling algorithm has the capacity to produce valid inference for the correctly specified model, while also characterising statistical efficiency through the effective sample size. Results show that Stan's No-U-Turn sampler, an implementation of Hamiltonian Monte Carlo, produces a well-calibrated posterior estimate while the celebrated off-set mixture approach is less efficient and poorly calibrated, though model parameterisation also plays a role. Limitations and restrictions of generality are discussed.",28,1,2024
Binding SNOMED-CT Terms to Archetype Elements: Establishing a Baseline of Results,"Introduction: This article is part of the Focus Theme of METHODS of Information in Medicine on ""Managing Interoperability and Complexity in Health Systems"". Background: The proliferation of archetypes as a means to represent information of Electronic Health Records has raised the need of binding terminological codes - such as SNOMED CT codes - to their elements, in order to identify them univocally. However, the large size of the terminologies makes it difficult to perform this task manually. Objectives: To establish a baseline of results for the aforementioned problem by using off-the-shelf string comparison-based techniques against which results from more complex techniques could be evaluated. Methods: Nine Typed Comparison METHODS were evaluated for binding using a set of 487 archetype elements. Their recall was calculated and Friedman and Nemenyi tests were applied in order to assess whether any of the methods outperformed the others. Results: Using the qGrams method along with the 'Text' information piece of archetype elements outperforms the other methods if a level of confidence of 90% is considered. A recall of 25.26% is obtained if just one SNOMED CT term is retrieved for each archetype element. This recall rises to 50.51% and 75.56% if 10 and 100 elements are retrieved respectively, that being a reduction of more than 99.99% on the SNOMED CT code set. Conclusions: The baseline has been established following the above-mentioned results. Moreover, it has been observed that although string comparison-based methods do not outperform more sophisticated techniques, they still can be an alternative for providing a reduced set of candidate terms for each archetype element from which the ultimate term can be chosen later in the more-than-likely manual supervision task.",26,1,2024
Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection,"Solving constrained multi-objective optimization problems with evolutionary algorithms has attracted considerable attention. Various constrained multi-objective optimization evolutionary algorithms (CMOEAs) have been developed with the use of different algorithmic strategies, evolutionary operators, and constraint-handling techniques. The performance of CMOEAs may be heavily dependent on the operators used, however, it is usually difficult to select suitable operators for the problem at hand. Hence, improving operator selection is promising and necessary for CMOEAs. This work proposes an online operator selection framework assisted by Deep Reinforcement Learning. The dynamics of the population, including convergence, diversity, and feasibility, are regarded as the state; the candidate operators are considered as actions; and the improvement of the population state is treated as the reward. By using a Q-Network to learn a policy to estimate the Q-values of all actions, the proposed approach can adaptively select an operator that maximizes the improvement of the population according to the current state and thereby improve the algorithmic performance. The framework is embedded into four popular CMOEAs and assessed on 42 benchmark problems. The experimental results reveal that the proposed Deep Reinforcement Learning-assisted operator selection significantly improves the performance of these CMOEAs and the resulting algorithm obtains better versatility compared to nine state-of-the-art CMOEAs.",15,1,2024
High return loss at the end face of fiber,A micro-optics device has been developed to limit backreflection at the end face of single-mode fibers. The measured return loss at a wavelength of 1.55 m is as high as 28 dB. This device allows for preservation of the geometric aspect of a normally cut end-face fiber.,11,1,2024
Symmetry and pseudosymmetry properties of Vaidya-Bonner-de Sitter spacetime,"The primary focus of the current study is to explore the geometrical properties of the Vaidya-Bonner-de Sitter (briefly, VBdS) spacetime, which is a generalization of Vaidya-Bonner spacetime, Vaidya spacetime and Schwarzschild spacetime. In this study we have shown that the VBdS spacetime describes various types of pseudosymmetric structures, including pseudosymmetry due to conformal curvature, conharmonic curvature and other curvatures. Additionally, it is shown that such a spacetime is 2-quasi-Einstein, Einstein manifold of level 3, generalized Roter type, and that conformal 2-forms are recurrent. The geometric features of the Vaidya-Bonner spacetime, Vaidya spacetime, and Schwarzschild spacetime are obtained as a particular instance of the main determination. It is further established that the VBdS spacetime admits almost Ricci soliton and almost \eta-Yamabe soliton with respect to non-Killing vector fields. Also, it is proved that such a spacetime possesses generalized conharmonic curvature inheritance. It is interesting to note that in the VBdS spacetime the tensors Q(T,R), Q(S,R) and Q(g,R) are linearly dependent. Finally, this spacetime is compared with the Vaidya-Bonner spacetime with respect to their admitting geometric structures, viz., various kinds of symmetry and pseudosymmetry properties.",19,1,2024
Passive Aperiodic Optical Phased Array based on Uniform Random Shuffle,"Grating lobes arise from the periodic nature of element spacing in the optical phased array. Essentially, the phased array performs the Spatial Fourier Transform on light; the steering capability of the main lobe is governed by phase shift variations among waveguides, and the Sidelobe Suppression Ratio (SLSR) correlates with the uniformity of emitter positions. Leveraging this understanding, we have optimized a 1x64 channel passive aperiodic OPAs with the uniform random shuffle in the emitter's position. Our conceptual simulations highlight a robust steering capability (18.60° / 10nm) and SLSR (-13.46 dB @ 0° / -8.27 dB @ +/-45°), and initial measurements demonstrate the steering capability (9.8 ° / 10nm, with smaller phase shifts design) and SLSR (-6.1dB @ -33.4°) from the preliminary fabrication.",31,1,2024
The Commuting Graph of a Solvable A-Group,"Let $G$ be a finite group. Recall that an $A$-group is a group whose Sylow subgroups are all abelian. In this paper, we investigate the upper bound on the diameter of the commuting graph of a solvable $A$-group. Assuming that the commuting graph is connected, we show when the derived length of $G$ is 2, the diameter of the commuting graph will be at most 4. In the general case, we show that the diameter of the commuting graph will be at most 6. In both cases, examples are provided to show that the upper bound of the commuting graph cannot be improved.",30,1,2024
Numerical analysis of physics-informed neural networks and related models in physics-informed machine learning,"Physics-informed neural networks (PINNs) and their variants have been very popular in recent years as algorithms for the numerical simulation of both forward and inverse problems for partial differential equations. This article aims to provide a comprehensive review of currently available results on the numerical analysis of PINNs and related models that constitute the backbone of physics-informed machine learning. We provide a unified framework in which analysis of the various components of the error incurred by PINNs in approximating PDEs can be effectively carried out. A detailed review of available results on approximation, generalization and training errors and their behavior with respect to the type of the PDE and the dimension of the underlying domain is presented. In particular, the role of the regularity of the solutions and their stability to perturbations in the error analysis is elucidated. Numerical results are also presented to illustrate the theory. We identify training errors as a key bottleneck which can adversely affect the overall performance of various models in physics-informed machine learning.",30,1,2024
Anomalous thermal oxidation of gadolinium thin films deposited on silicon by high pressure sputtering,"Thin gadolinium metallic layers were deposited by high-pressure sputtering in pure Ar atmosphere. Subsequently, in situ thermal oxidation was performed at temperatures ranging from 150 to 750 $^\circ$C. At an oxidation temperature of 500 $^\circ$C the films show a transition from monoclinic structure to a mixture of monoclinic and cubic. Regrowth of interfacial SiO$_x$ is observed as temperature is increased, up to 1.6 nm for 750 $^\circ$C. This temperature yields the lowest interface trap density, 4e10 eV$^{-1}$ cm$^{-2}$, but the effective permittivity of the resulting dielectric is only 7.4. The reason of this low value is found on the oxidation mechanism, which yields a surface with located bumps. These bumps increase the average thickness, thus reducing the capacitance and therefore the calculated permittivity.",29,1,2024
Compact Ka-Band Metalens Antenna Enabled by Physics Assisted Particle Swarm Optimization (PA-PSO) Algorithm,"The design of multiple-feed lens antennas requires multivariate and multi-objective optimization processes, which can be accelerated by PSO algorithms. However, the PSO algorithm often fails to achieve optimal results with limited computation resources since the spaces of candidate solutions are quite large for lens antenna designs. This paper presents a design paradigm for multiple-feed lens antennas based on a physics-assisted particle swarm optimization (PA-PSO) algorithm, which guides the swarm of particles based on the laws of physics. As a proof of concept, a design of compact metalens antenna is proposed, which measures a +-55° field of view, a 21 dBi gain with a flatness within 4 dB, a 3-dB bandwidth > 12°, and a compact design with a F-number of 0.2. The proposed PA-PSO algorithm reaches the optimal results 6 times faster than the ordinary PSO algorithm, which shows promising applications on metasurface antenna designs.",29,1,2024
A Hessian-based energy-optimization study of morphoelasticity,"Morphoelasticity describes a family of nonlinear PDE system and has been widely applied to understand growth and morphogenesis in living and nonliving structures. Most previous studies depend on locally linearizing either the PDE system or its discretized numerical counterpart. This involves conducting a linear stability analysis for a trivial solution or utilizing Newton-Raphson iteration on the force residual equations derived from finite element approximation, respectively. The paper presents a numerical approach that frames the search for solutions as the optimization of a discretized energy functional, similar to Tallinen et al., PRL, 2013. In contrast to Tallinen et al., we utilize the Hessian matrix of the energy to develop a gradient descent algorithm that ensures energy stability, which allows us to search solutions robustly in different types of bifurcations. The solutions are further refined locally by Newton-Raphson iterations. We showcase the effectiveness of our approach by studying shape transitions induced by the growth in the interior layer of a bilayer annulus, with the outer layer constrained at its outermost boundary. By investigating three cases with varying elastic-moduli ratios between the two layers, we illustrate that our new method can not only identify and characterize supercritical bifurcations leading to smooth wrinkles but also capture subcritical transitions from smooth solutions to non-smooth crease-forming solutions. Moreover, it reveals aperiodic solutions, both smooth and non-smooth, that have not been previously reported.",29,1,2024
Yamdb: easily accessible thermophysical properties of liquid metals and molten salts,"Yamdb (Yet another materials data base) addresses the need to provide thermophysical properties of liquid metals and molten salts in an easily accessible manner. Mathematical relations describing material properties - usually determined by experiment - are taken from the literature. Equations and their coefficients are stored separately. The former can be implemented in any programming language (Python and Go in this case) and the latter are kept in YAML files together with additional information (source, temperature range, composition, accuracy if available, etc).",28,1,2024
AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning,"Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, the model learns to emphasize on the correlated patterns across modalities, which may help align the cross-attended mode-specific descriptors pairwise within a joint-embedding space and thereby compensate for missing modalities during inference. By leveraging the spatio-temporal details at the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior performance compared to the best-performing state-of-the-art multimodal methods, by effectively leveraging body language in place of face expression, it also exhibits an enhanced privacy feature. By reporting around 2-5% improvement in the weighted-F1 score, the proposed multimodal joint embedding module facilitates an impressive performance gain in a variety of missing-modality query scenarios during test time.",26,1,2024
Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array,"Large language models (LLMs) have made headlines for synthesizing correct-sounding responses to a variety of prompts, including code generation. In this paper, we present the prompts used to guide ChatGPT4 to produce a synthesizable and functional verilog description for the entirety of a programmable Spiking Neuron Array ASIC. This design flow showcases the current state of using ChatGPT4 for natural language driven hardware design. The AI-generated design was verified in simulation using handcrafted testbenches and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5 using an open-source EDA flow.",25,1,2024
Intersection of Parabolic Subgroups in Euclidean Braid Groups: a short proof,"We give a short proof for the fact, already proven by Thomas Haettel, that the arbitrary intersection of parabolic subgroups in Euclidean Braid groups $A[\tilde{A}_n]$ is again a parabolic subgroup. To that end, we use that the spherical-type Artin group $A[B_{n+1}]$ is isomorphic to $A[\tilde{A}_n] \rtimes \mathbb{Z}$.",24,1,2024
Recovering the Fragmentation Rate in the Growth-Fragmentation Equation,"We consider the inverse problem of determining the fragmentation rate from noisy measurements in the growth-fragmentation equation. We use Fourier transform theory on locally compact groups to treat this problem for general fragmentation probabilities. We develop a regularization method based on spectral filtering, which allows us to deal with the inverse problem in weighted ${L}^2$ spaces. %Our approach regularizes the signal generated by differential operators in the frequency domain. As a result, we obtain a regularization method with error of order $O(\varepsilon^{\frac{2m}{2m+1}})$, where $\varepsilon$ is the noise level and $m>0$ is the {\em a priori} regularity order of the fragmentation rate.",20,1,2024
SRAM Alpha-SER Estimation From Word-Line Voltage Margin Measurements: Design Architecture and Experimental Results,"Experimental results from a 65 nm CMOS commercial technology SRAM test chip reveal a linear correlation between a new electrical parameter -- the word-line voltage margin (VWLVM) -- and the measured circuit alpha-SER. Additional experiments show that no other memory cell electrical robustness-related parameters exhibit such correlation. The technique proposed is based on correlating the VWLVM to the SER measured on a small number of circuit samples to determine the correlation parameters. Then, the remaining non-irradiated circuits SER is determined from electrical measurements (VWLVM) without the need of additional radiation experiments. This method represents a significant improvement in time and cost, while simplifying the SER-determination methods since most of the circuits do not require irradiation. The technique involves a minor memory design modification that does not degrade circuit performance, while circuit area increase is negligible.",20,1,2024
PDM relativistic quantum oscillator in Einstein-Maxwell-Lambda space-time,"In this analysis, we study the dynamics of quantum oscillator fields within the context of a position-dependent mass (PDM) system situated in an Einstein-Maxwell space-time, incorporating a non-zero cosmological constant. The magnetic field is aligned along the symmetry axis direction. To analyze PDM quantum oscillator fields, we introduce a modification to the Klein-Gordon equation by substituting the four-momentum vector $p_{\mu} \to \Big(p_{\mu}+i\,\eta\,X_{\mu}+i\,\mathcal{F}_{\mu}\Big)$ into the Klein-Gordon equation, where the four-vector is defibed by $X_{\mu}=(0, r, 0, 0)$, $\mathcal{F}_{\mu}=(0, \mathcal{F}_r, 0, 0)$ with $\mathcal{F}_r=\frac{f'(r)}{4\,f(r)}$, and $\eta$ is the mass oscillator frequency. The radial wave equation for the relativistic modified Klein-Gordon equation is derived and subsequently solved for two distinct cases: (i) $f(r)=e^{\frac{1}{2}\,\alpha\,r^2}$, and (ii) $f(r)=r^{\beta}$, where $\alpha \geq 0, \beta \geq 0$. The resultant energy levels and wave functions for quantum oscillator fields are demonstrated to be influenced by both the cosmological constant and the geometrical topology parameter which breaks the degeneracy of the energy spectrum. Furthermore, we observed noteworthy modifications in the energy levels and wave functions when compared to the results derived in the flat space background.",20,1,2024
Using Bright-Point Shapes to Constrain Wave-Heating of the Solar Corona: Predictions for DKIST,"Magnetic bright points on the solar photosphere mark the footpoints of kilogauss magnetic flux tubes extending toward the corona. Convective buffeting of these tubes is believed to excite magnetohydrodynamic waves, which can propagate to the corona and there deposit heat. Measuring wave excitation via bright-point motion can thus constrain coronal and heliospheric models, and this has been done extensively with centroid tracking, which can estimate kink-mode wave excitation. DKIST is the first telescope to provide well-resolved observations of bright points, allowing shape and size measurements to probe the excitation of other wave modes that have been difficult, if not impossible, to study to date. In this work, we demonstrate a method of automatic bright-point tracking that robustly identifies the shapes of bright points, and we develop a technique for interpreting measured bright-point shape changes as the driving of a range of thin-tube wave modes. We demonstrate these techniques on a MURaM simulation of DKIST-like resolution. These initial results suggest that modes other than the long-studied kink mode could increase the total available energy budget for wave-heating by 50%. Pending observational verification as well as modeling of the propagation and dissipation of these additional wave modes, this could represent a significant increase in the potency of wave-turbulence heating models.",19,1,2024
A robustness-enhanced reconstruction based on discontinuity feedback factor for high-order finite volume scheme,"In this paper, a robustness-enhanced reconstruction for the high-order finite volume scheme is constructed on the 2-D structured mesh, and both the high-order gas-kinetic scheme(GKS) and the Lax-Friedrichs(L-F) flux solver are considered to verify the validity of this algorithm. The strategy of the successful WENO reconstruction is adopted to select the smooth sub-stencils. However, there are cases where strong discontinuities exist in all sub-stencils of the WENO reconstruction, which leads to a decrease in the robustness. To improve the robustness of the algorithm in discontinuous regions in two-dimensional space, the hybrid reconstruction based on a combination of discontinuity feedback factor(DF) \cite{ji2021gradient} and WENO reconstruction is developed to deal with the possible discontinuities. Numerical results from smooth to extreme cases have been presented and validate that the new finite volume scheme is effective for robustness enhancement and maintains high resolution compared to the WENO scheme.",19,1,2024
A comparative study of explicit and implicit Large Eddy Simulations using a high-order discontinuous Galerkin solver: application to a Formula 1 front wing,"This paper explores two Large Eddy Simulation (LES) approaches within the framework of the high-order discontinuous Galerkin solver, Horses3D. The investigation focuses on an Inverted Multi-element Wing in Ground Effect (i.e. 2.5D Imperial Front Wing section) representing a Formula 1 front wing, and compares the strengths and limitations of the two LES methods. The explicit LES formulation relies on the Vreman model, that adapts to laminar, transitional and turbulent regimes. The numerical formulation uses nodal basis functions and Gauss points. The implicit LES formulation, does not require explicit turbulence modeling but relies in the discretization scheme. We use the Kennedy-Gruber entropy stable formulation to enhance stability in under resolved simulations, since we recover the continuous properties such as entropy conservation at a discrete level. This formulation employs Gauss-Lobatto points, which downgrades the accuracy of integration but allows for larger time steps in explicit time integration. We compare our results to Nektar++ [1] showing that both LES techniques provide results that agree well with the reference values. The implicit LES shows to better capture transition and allows for larger time steps at a similar cost per iteration. We conclude that this implicit LES formulation is very attractive for complex simulations.",15,1,2024
Particle acceleration in pulsars and pulsar wind nebulae,"These notes summarise the contents of the lectures I delivered at the International School of Physics ""Enrico Fermi"" on ""Foundations of Cosmic Ray Astrophysics"". The lectures were dealing with the physics of Pulsars and Pulsar Wind Nebulae (PWNe) in the Cosmic Ray (CR) perspective. It has become now clear that the processes taking place in the environment of fast rotating, highly magnetized neutron stars, often detected as pulsars, play a crucial role in the formation of the CR spectrum detected at the Earth. These lectures discuss the main aspects of this connection. Pulsars are likely contributors of the CR lepton flux at the Earth thanks to their nature of electron-positron factories. Pulsars and their nebulae are the best potential leptonic PeVatron in the Galaxy, and the Crab Nebula, the prototype of the Pulsar Wind Nebula class is the only established PeVatron in the Galaxy. Pulsars are however also potential sources of high energy hadrons, up to the energies relevant for UHECRs. Pulsars and their nebulae are the best potential leptonic PeVatrons in the Galaxy, and the Crab Nebula, the prototype of the Pulsar Wind Nebula class, is the only established PeVatron in the Galaxy. Finally, regions of suppressed particle diffusion have been observed around evolved pulsars, the so-called TeV halos, which could have an impact on galactic CR transport. These lectures discuss the physics of pulsars and PWNe, summarising what we know about these systems and what pieces of information are still missing to fully assess their role in all the above mentioned Cosmic Ray connected aspects.",15,1,2024
Modelling the rotation dependence of cycle variability in sun-like stars: Answering why only slowly rotating stars produce grand minima,"The Sun and solar-type stars exhibit irregular cyclic variations in their magnetic activity over long time scales. To understand this irregularity, we employed the flux transport dynamo models to investigate the behavior of one solar mass star at various rotation rates. To achieve this, we have utilized a mean-field hydrodynamic model to specify differential rotation and meridional circulation, and we have incorporated stochastic fluctuations in the Babcock-Leighton source of the poloidal field to capture inherent fluctuations in the stellar convection. Our simulations successfully demonstrated consistency with the observational data, revealing that rapidly rotating stars exhibit highly irregular cycles with strong magnetic fields and no Maunder-like grand minima. On the other hand, slow rotators produce smoother cycles with weaker magnetic fields, long-term amplitude modulation, and occasional extended grand minima. We observed that the frequency and duration of grand minima increase with the decreasing rotation rate. These results can be understood as the tendency of a less supercritical dynamo in slow rotators to be more prone to produce extended grand minima. We further explore the possible existence of the dynamo in the subcritical regime in a Babcock-Leighton-type framework and in the presence of a small-scale dynamo.",15,1,2024
Energy Momentum Localization in Quantum Gravity,"We introduce quantum spatio-temporal dynamics (QSD) as modeled by the Nexus Paradigm (NP) of quantum gravity to resolve the problem of energy-momentum localization in a gravitational field. Currently, the gravitational field as described using the language of geometry modeled under General Relativity (GR) fails to provide a generally accepted definition of energy-momentum. Attempts at resolving this problem using geometric methods have resulted in various energy-momentum complexes whose physical meaning remain dubious since the resulting complexes are non-tensorial under a general coordinate transformation. In QSD, the tangential manifold is the affine connection field in which energy-momentum localization is readily defined. We also discover that the positive mass condition is a natural consequence of quantization and that dark energy is a Higgs like field with negative energy density everywhere. Finally, energy-momentum localization in quantum gravity shows that a free falling object will experience larger vacuum fluctuations (uncertainties in location) in strong gravity than in weak gravity and that the amplitudes of these oscillations define the energy of the free falling object.",13,1,2024
"Effective conductivities of some multi-color, isotropic, regular tessellations","Algebraic expressions are found for the effective conductivities of some infinite tessellations composed of conducting square, triangular, or hexagonal tiles. A tessellation is further characterized by the number N of different colors (different tile conductivities) represented. Tiles of a color are distributed randomly, and constitute an areal fraction 1/N of the tessellation. The expressions take account of the percolation threshold associated with the tile type. This generates a series of expressions for the three-color case, that suggests this approach gives lower bounds for the true effective conductivities.",12,1,2024
LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration,"Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelmed, by assisting the users with simple instructions and informing authorities with their location and emergency information.",12,1,2024
Optically Levitated Nanoparticles as Receiving Antennas for Low Frequency Wireless Communication,"Low-frequency (LF) wireless communications play a crucial role in ensuring anti-interference, long-range, and efficient communication across various environments. However, in conventional LF communication systems, their antenna size is required to be inversely proportional to the wavelength, so that their mobility and flexibility are greatly limited. Here we introduce a novel prototype of LF receiving antennas based on optically levitated nanoparticles, which overcomes the size-frequency limitation to reduce the antenna size to the hundred-nanometer scale. These charged particles are extremely sensitive to external electric field as mechanical resonators, and their resonant frequencies are adjustable. The effectiveness of these antennas was experimentally demonstrated by using the frequency shift keying (2FSK) modulation scheme. The experimental results indicate a correlation between error rate and factors such as transmission rate, signal strength, and vacuum degree with a signal strength of approximately 0.1V/m and a bit error rate below 0.1%. This advancement in leveraging levitated particle mechanical resonators (LPMRs) as LF antennas marks a significant stride in long-distance communication technology.",11,1,2024
Case report: Abnormal radiation dose rate measurement near Fukushima Daiichi nuclear power plant,"This paper presents the results of radiation dose rate measurements taken by the author in various locations in South Korea and Japan, including near the Fukushima Daiichi nuclear power plant site. Air dose rates were measured in Seoul, during a flight over the East Sea, in Fukushima Prefecture, and in Tokyo using a calibrated survey instrument. The measurements were compared with reference values from official sources or previous studies. The author discovered that the majority of the measurements fell within the normal range of environmental radiation, except for an unusually high dose rate at Tomioka Station, which is approximately 10 km away from the Fukushima Daiichi nuclear plant. The author recommends that Japanese institutions conduct more precise measurements in the vicinity of the nuclear accident sites.",8,1,2024
Cooperative near- and far-field thermal management via diffusive superimposed dipoles,"Active metadevices with external excitations exhibit significant potential for advanced heat regulation. Nonetheless, conventional inputs, like heating/cooling and introducing convection by rotating plate, display inherent limitations. One is the only focus on far-field control to eliminate temperature distortion in the background while neglecting near-field regulation in the functional region. Another is lacking adaptability due to complex devices like thermoelectric modules and stepping motors. To tackle these challenges, the concept of diffusive superimposed dipoles characterized by orthogonal thermal dipole moments is proposed. Cooperative near- and far-field regulation of temperature fields is achieved by designing superimposed dipole moments, enabling transparency and cloaking functionalities. Simulation and experiment outcomes affirm the efficacy of this adaptive thermal field control technique, even when interface thermal resistance is taken into account. Adaptivity stems from dipole moment decomposability, allowing metadevices to operate in various heat flux directions and background thermal conductivity. These findings could pave the way for cooperative and adaptive thermal management and hold potential applications in other Laplace fields, including direct current and hydrodynamics.",6,1,2024
Human-Centric Goal Reasoning with Ripple-Down Rules,"ActorSim is a goal reasoning framework developed at the Naval Research Laboratory. Originally, all goal reasoning rules were hand-crafted. This work extends ActorSim with the capability of learning by demonstration, that is, when a human trainer disagrees with a decision made by the system, the trainer can take over and show the system the correct decision. The learning component uses Ripple-Down Rules (RDR) to build new decision rules to correctly handle similar cases in the future. The system is demonstrated using the RoboCup Rescue Agent Simulation, which simulates a city-wide disaster, requiring emergency services, including fire, ambulance and police, to be dispatched to different sites to evacuate civilians from dangerous situations. The RDRs are implemented in a scripting language, FrameScript, which is used to mediate between ActorSim and the agent simulator. Using Ripple-Down Rules, ActorSim can scale to an order of magnitude more goals than the previous version.",30,1,2024
Integer Optimization of CT Trajectories using a Discrete Data Completeness Formulation,"X-ray computed tomography (CT) plays a key role in digitizing three-dimensional structures for a wide range of medical and industrial applications. Traditional CT systems often rely on standard circular and helical scan trajectories, which may not be optimal for challenging scenarios involving large objects, complex structures, or resource constraints. In response to these challenges, we are exploring the potential of twin robotic CT systems, which offer the flexibility to acquire projections from arbitrary views around the object of interest. Ensuring complete and mathematically sound reconstructions becomes critical in such systems. In this work, we present an integer programming-based CT trajectory optimization method. Utilizing discrete data completeness conditions, we formulate an optimization problem to select an optimized set of projections. This approach enforces data completeness and considers absorption-based metrics for reliability evaluation. We compare our method with an equidistant circular CT trajectory and a greedy approach. While greedy already performs well in some cases, we provide a way to improve greedy-based projection selection using an integer optimization approach. Our approach improves CT trajectories and quantifies the optimality of the solution in terms of an optimality gap.",29,1,2024
Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate,"Autonomous vehicles are suited for continuous area patrolling problems. Finding an optimal patrolling strategy can be challenging due to unknown environmental factors, such as wind or landscape; or autonomous vehicles' constraints, such as limited battery life or hardware failures. Importantly, patrolling large areas often requires multiple agents to collectively coordinate their actions. However, an optimal coordination strategy is often non-trivial to be manually defined due to the complex nature of patrolling environments. In this paper, we consider a patrolling problem with environmental factors, agent limitations, and three typical cooperation problems -- collision avoidance, congestion avoidance, and patrolling target negotiation. We propose a multi-agent reinforcement learning solution based on a reinforced inter-agent learning (RIAL) method. With this approach, agents are trained to develop their own communication protocol to cooperate during patrolling where faults can and do occur. The solution is validated through simulation experiments and is compared with several state-of-the-art patrolling solutions from different perspectives, including the overall patrol performance, the collision avoidance performance, the efficiency of battery recharging strategies, and the overall fault tolerance.",28,1,2024
Convergence Rate of Projected Subgradient Method with Time-varying Step-sizes,"We establish the optimal ergodic convergence rate for the classical projected subgradient method with a time-varying step-size. This convergence rate remains the same even if we slightly increase the weight of the most recent points, thereby relaxing the ergodic sense.",23,1,2024
Towards Semi-Autonomous Robotic Arm Manipulation Operator Intention Detection from Forces Feedback,"In harsh environments such as those found in nuclear facilities, the use of robotic systems is crucial for performing tasks that would otherwise require human intervention. This is done to minimize the risk of human exposure to dangerous levels of radiation, which can have severe consequences for health and even be fatal. However, the telemanipulation systems employed in these environments are becoming increasingly intricate, relying heavily on sophisticated control methods and local master devices. Consequently, the cognitive burden on operators during labor-intensive tasks is growing. To tackle this challenge, operator intention detection based on task learning can greatly enhance the performance of robotic tasks while reducing the reliance on human effort in teleoperation, particularly in a glovebox environment. By accurately predicting the operator's intentions, the robot can carry out tasks more efficiently and effectively, with minimal input from the operator. In this regard, we propose the utilization of Convolutional Neural Networks, a machine learning approach, to learn and forecast the operator's intentions using raw force feedback spatiotemporal data. Through our experimental study on glovebox tasks for nuclear applications, such as radiation survey and object grasping, we have achieved promising outcomes. Our approach holds the potential to enhance the safety and efficiency of robotic systems in harsh environments, thus diminishing the risk of human exposure to radiation while simultaneously improving the precision and speed of robotic operations.",19,1,2024
AntiDeepFake: AI for Deep Fake Speech Recognition,"In this research study, we propose a modern artificial intelligence (AI) approach to recognize deepfake voice, also known as generative AI cloned synthetic voice. Our proposed AI technology, called AntiDeepFake, consists of all main pipelines from data to evaluation in the whole picture. We provide experimental results and scores for all our proposed methods. The main source code for our approach is available in the provided link:this https URLrepository.",4,1,2024
Review of the Learning-based Camera and Lidar Simulation Methods for Autonomous Driving Systems,"Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation methods and validation approaches, focusing on two main types of perception sensors: cameras and Lidars. This review covers two categories of learning-based approaches, namely raw-data-based and object-based models. Raw-data-based methods are explained concerning the employed learning strategy, while object-based models are categorised based on the type of error considered. Finally, the paper illustrates commonly used validation techniques for evaluating perception sensor models and highlights the existing research gaps in the area.",29,1,2024
EventF2S: Asynchronous and Sparse Spiking AER Framework using Neuromorphic-Friendly Algorithm,"Bio-inspired Address Event Representation (AER) sensors have attracted significant popularity owing to their low power consumption, high sparsity, and high temporal resolution. Spiking Neural Network (SNN) has become the inherent choice for AER data processing. However, the integration of the AER-SNN paradigm has not adequately explored asynchronous processing, neuromorphic compatibility, and sparse spiking, which are the key requirements of resource-constrained applications. To address this gap, we introduce a brain-inspired AER-SNN object recognition solution, which includes a data encoder integrated with a First-To-Spike recognition network. Being fascinated by the functionality of neurons in the visual cortex, we designed the solution to be asynchronous and compatible with neuromorphic hardware. Furthermore, we have adapted the principle of denoising and First-To-Spike coding to achieve optimal spike signaling, significantly reducing computation costs. Experimental evaluation has demonstrated that the proposed method incurs significantly less computation cost to achieve state-of-the-art competitive accuracy. Overall, the proposed solution offers an asynchronous and cost-effective AER recognition system that harnesses the full potential of AER sensors.",28,1,2024
Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas,"Over the last decade, Autonomous Delivery Robots (ADRs) have transformed conventional delivery methods, responding to the growing e-commerce demand. However, the readiness of ADRs to navigate safely among pedestrians in shared urban areas remains an open question. We contend that there are crucial research gaps in understanding their interactions with pedestrians in such environments. Human Pose Estimation is a vital stepping stone for various downstream applications, including pose prediction and socially aware robot path-planning. Yet, the absence of an enriched and pose-labeled dataset capturing human-robot interactions in shared urban areas hinders this objective. In this paper, we bridge this gap by repurposing, fusing, and labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and Simultaneous Localization and Mapping (SLAM), respectively. The resulting unique dataset represents thousands of real-world indoor and outdoor human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose visual and numeric outputs and provided ground truth poses using manual annotation. To overcome the distance bias present in the traditional MPJPE metric, this study introduces a novel human pose estimation error metric called Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into it. Findings demonstrate that YOLOv7 effectively estimates human pose in both datasets. However, it exhibits weaker performance in specific scenarios, like indoor, crowded scenes with a focused light source, where both MPJPE and MSJE are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17 seq1), achieving MSJE values of 5.29 and 3.38, respectively.",28,1,2024
Deep Joint Source-Channel Coding for Efficient and Reliable Cross-Technology Communication,"Cross-technology communication (CTC) is a promising technique that enables direct communications among incompatible wireless technologies without needing hardware modification. However, it has not been widely adopted in real-world applications due to its inefficiency and unreliability. To address this issue, this paper proposes a deep joint source-channel coding (DJSCC) scheme to enable efficient and reliable CTC. The proposed scheme builds a neural-network-based encoder and decoder at the sender side and the receiver side, respectively, to achieve two critical tasks simultaneously: 1) compressing the messages to the point where only their essential semantic meanings are preserved; 2) ensuring the robustness of the semantic meanings when they are transmitted across incompatible technologies. The scheme incorporates existing CTC coding algorithms as domain knowledge to guide the encoder-decoder pair to learn the characteristics of CTC links better. Moreover, the scheme constructs shared semantic knowledge for the encoder and decoder, allowing semantic meanings to be converted into very few bits for cross-technology transmissions, thus further improving the efficiency of CTC. Extensive simulations verify that the proposed scheme can reduce the transmission overhead by up to 97.63\% and increase the structural similarity index measure by up to 734.78%, compared with the state-of-the-art CTC scheme.",26,1,2024
LLM-based policy generation for intent-based management of applications,"Automated management requires decomposing high-level user requests, such as intents, to an abstraction that the system can understand and execute. This is challenging because even a simple intent requires performing a number of ordered steps. And the task of identifying and adapting these steps (as conditions change) requires a decomposition approach that cannot be exactly pre-defined beforehand. To tackle these challenges and support automated intent decomposition and execution, we explore the few-shot capability of Large Language Models (LLMs). We propose a pipeline that progressively decomposes intents by generating the required actions using a policy-based abstraction. This allows us to automate the policy execution by creating a closed control loop for the intent deployment. To do so, we generate and map the policies to APIs and form application management loops that perform the necessary monitoring, analysis, planning and execution. We evaluate our proposal with a use-case to fulfill and assure an application service chain of virtual network functions. Using our approach, we can generalize and generate the necessary steps to realize intents, thereby enabling intent automation for application management.",22,1,2024
Navigating the Maize: Cyclic and conditional computational graphs for molecular simulation,"Many computational chemistry and molecular simulation workflows can be expressed as graphs. This abstraction is useful to modularize and potentially reuse existing components, as well as provide parallelization and ease reproducibility. Existing tools represent the computation as a directed acyclic graph (DAG), thus allowing efficient execution by parallelization of concurrent branches. These systems can, however, generally not express cyclic and conditional workflows. We therefore developed Maize, a workflow manager for cyclic and conditional graphs based on the principles of flow-based programming. By running each node of the graph concurrently in separate processes and allowing communication at any time through dedicated inter-node channels, arbitrary graph structures can be executed. We demonstrate the effectiveness of the tool on a dynamic active learning task in computational drug design, involving the use of a small molecule generative model and an associated scoring system.",22,1,2024
Review of algorithms for predicting fatigue using EEG,"Fatigue detection is of paramount importance in enhancing safety, productivity, and well-being across diverse domains, including transportation, healthcare, and industry. This scientific paper presents a comprehensive investigation into the application of machine learning algorithms for the detection of physiological fatigue using Electroencephalogram (EEG) signals. The primary objective of this study was to assess the efficacy of various algorithms in predicting an individual's level of fatigue based on EEG data.",30,1,2024
Subject-Independent Deep Architecture for EEG-based Motor Imagery Classification,"Motor imagery (MI) classification based on electroencephalogram (EEG) is a widely-used technique in non-invasive brain-computer interface (BCI) systems. Since EEG recordings suffer from heterogeneity across subjects and labeled data insufficiency, designing a classifier that performs the MI independently from the subject with limited labeled samples would be desirable. To overcome these limitations, we propose a novel subject-independent semi-supervised deep architecture (SSDA). The proposed SSDA consists of two parts: an unsupervised and a supervised element. The training set contains both labeled and unlabeled data samples from multiple subjects. First, the unsupervised part, known as the columnar spatiotemporal auto-encoder (CST-AE), extracts latent features from all the training samples by maximizing the similarity between the original and reconstructed data. A dimensional scaling approach is employed to reduce the dimensionality of the representations while preserving their discriminability. Second, a supervised part learns a classifier based on the labeled training samples using the latent features acquired in the unsupervised part. Moreover, we employ center loss in the supervised part to minimize the embedding space distance of each point in a class to its center. The model optimizes both parts of the network in an end-to-end fashion. The performance of the proposed SSDA is evaluated on test subjects who were not seen by the model during the training phase. To assess the performance, we use two benchmark EEG-based MI task datasets. The results demonstrate that SSDA outperforms state-of-the-art methods and that a small number of labeled training samples can be sufficient for strong classification performance.",27,1,2024
Managing Household Waste through Transfer Learning,"As the world continues to face the challenges of climate change, it is crucial to consider the environmental impact of the technologies we use. In this study, we investigate the performance and computational carbon emissions of various transfer learning models for garbage classification. We examine the MobileNet, ResNet50, ResNet101, and EfficientNetV2S and EfficientNetV2M models. Our findings indicate that the EfficientNetV2 family achieves the highest accuracy, recall, f1-score, and IoU values. However, the EfficientNetV2M model requires more time and produces higher carbon emissions. ResNet50 outperforms ResNet110 in terms of accuracy, recall, f1-score, and IoU, but it has a larger carbon footprint. We conclude that EfficientNetV2S is the most sustainable and accurate model with 96.41% accuracy. Our research highlights the significance of considering the ecological impact of machine learning models in garbage classification.",27,1,2024
On the expected number of facets for the convex hull of samples,"This paper studies the convex hull of $d$-dimensional samples i.i.d. generated from spherically symmetric distributions. Specifically, we derive a complete integration formula for the expected facet number of the convex hull. This formula is with respect to the CDF of the radial distribution. As the number of samples approaches infinity, the integration formula enables us to obtain the asymptotic value of the expected facet number for three categories of spherically symmetric distributions. Additionally, the asymptotic result can be applied to estimating the sample complexity in order that the probability measure of the convex hull tends to one.",27,1,2024
Gravitational memory effects of black bounces and a traversable wormhole,"Black bounces are spacetimes that can be interpreted as either black holes or wormholes depending on specific parameters. In this study, we examine the Simpson-Visser and Bardeen-type solutions as black bounces and investigate the gravitational wave in the background of these solutions. We then explore the displacement and velocity memory effects by analyzing the deviation of two neighboring geodesics and their derivatives influenced by the magnetic charge parameter a. This investigation aims to trace the magnetic charge in the gravitational memory effect. Additionally, we consider another family of traversable wormhole solutions obtained from non-exotic matter sources to trace the electric charge Qe in the gravitational memory effect, which can be determined from the far field asymptotic. This project is significant not only for detecting the presence of compact objects like wormholes through gravitational memory effects but also for observing the charge Qe, which provides a concrete realization of Wheeler's concept of ""electric charge without charge.""",26,1,2024
Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous Network for Human Activity Recognition in Flawed Wearable Sensor Data,"The popularity and diffusion of wearable devices provides new opportunities for sensor-based human activity recognition that leverages deep learning-based algorithms. Although impressive advances have been made, two major challenges remain. First, sensor data is often incomplete or noisy due to sensor placement and other issues as well as data transmission failure, calling for imputation of missing values, which also introduces noise. Second, human activity has multi-scale characteristics. Thus, different groups of people and even the same person may behave differently under different circumstances. To address these challenges, we propose a multilevel heterogeneous neural network, called MHNN, for sensor data analysis. We utilize multilevel discrete wavelet decomposition to extract multi-resolution features from sensor data. This enables distinguishing signals with different frequencies, thereby suppressing noise. As the components resulting from the decomposition are heterogeneous, we equip the proposed model with heterogeneous feature extractors that enable the learning of multi-scale features. Due to the complementarity of these features, we also include a cross aggregation module for enhancing their interactions. An experimental study using seven publicly available datasets offers evidence that MHNN can outperform other cutting-edge models and offers evidence of robustness to missing values and noise. An ablation study confirms the importance of each module.",26,1,2024
Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting,"Accurate household short-term energy consumption forecasting (STECF) is crucial for home energy management, but it is technically challenging, due to highly random behaviors of individual residential users. To improve the accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF methodology that leverages association mining in electrical behaviors. First, a probabilistic association quantifying and discovering method is proposed to model the pairwise behaviors association and generate associated clusters. Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based forecasting is provided to explore the temporal correlation and enhance accuracy. The testing results demonstrate that this methodology yields a significant enhancement in the STECF.",26,1,2024
An Enhanced Analysis of Traffic Intelligence in Smart Cities Using Sustainable Deep Radial Function,"Smart cities have revolutionized urban living by incorporating sophisticated technologies to optimize various aspects of urban infrastructure, such as transportation systems. Effective traffic management is a crucial component of smart cities, as it has a direct impact on the quality of life of residents and tourists. Utilizing deep radial basis function (RBF) networks, this paper describes a novel strategy for enhancing traffic intelligence in smart cities. Traditional methods of traffic analysis frequently rely on simplistic models that are incapable of capturing the intricate patterns and dynamics of urban traffic systems. Deep learning techniques, such as deep RBF networks, have the potential to extract valuable insights from traffic data and enable more precise predictions and decisions. In this paper, we propose an RBF based method for enhancing smart city traffic intelligence. Deep RBF networks combine the adaptability and generalization capabilities of deep learning with the discriminative capability of radial basis functions. The proposed method can effectively learn intricate relationships and nonlinear patterns in traffic data by leveraging the hierarchical structure of deep neural networks. The deep RBF model can learn to predict traffic conditions, identify congestion patterns, and make informed recommendations for optimizing traffic management strategies by incorporating these rich and diverse data To evaluate the efficacy of our proposed method, extensive experiments and comparisons with real world traffic datasets from a smart city environment were conducted. In terms of prediction accuracy and efficiency, the results demonstrate that the deep RBF based approach outperforms conventional traffic analysis methods. Smart city traffic intelligence is enhanced by the model capacity to capture nonlinear relationships and manage large scale data sets.",24,1,2024
Signal-to-noise per unit time optimization for in vivo single-voxel proton magnetic resonance spectroscopy of the brain: Theoretical formulation and experimental verification at two field strengths,"Signal-to-noise ratio optimization, regarding repetition time selection, was explored mathematically and experimentally for single-voxel proton magnetic resonance spectroscopy. Theoretical findings were benchmarked against phantom measurements at 1.5 Tesla and localized in vivo proton brain spectra acquired at both 1.5 Tesla/3.0 Tesla. A detailed mathematical description of signal-to-noise ratio per unit time was derived, yielding an optimal repetition time of 1.256 times the metabolite longitudinal relaxation time. While long-repetition-time acquisitions minimize longitudinal relaxation time contributions, a repetition time of ~1.5s results in maximum signal-to-noise ratio per unit time, which can in turn be invested into smaller voxel sizes. The latter is of utmost importance in brain oncology, allowing accurate spectroscopic characterization of small lesions.",24,1,2024
What Is a Causal Graph?,"This article surveys the variety of ways in which a directed acyclic graph (DAG) can be used to represent a problem of probabilistic causality. For each of these we describe the relevant formal or informal semantics governing that representation. It is suggested that the cleanest such representation is that embodied in an augmented DAG, which contains nodes for non-stochastic intervention indicators in addition to the usual nodes for domain variables.",24,1,2024
Neural-network based high-speed volumetric dynamic optical coherence tomography,"We demonstrate deep-learning neural network (NN)-based dynamic optical coherence tomography (DOCT), which generates high-quality logarithmic-intensity-variance (LIV) DOCT images from only four OCT frames. The NN model is trained for tumor spheroid samples using a customized loss function: the weighted mean absolute error. This loss function enables highly accurate LIV image generation. The fidelity of the generated LIV images to the ground truth LIV images generated using 32 OCT frames is examined via subjective image observation and statistical analysis of image-based metrics. Fast volumetric DOCT imaging with an acquisition time of 6.55 s/volume is demonstrated using this NN-based method.",24,1,2024
DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications,"Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and corresponding ground-truth heading angle. We show that our proposed approach outperforms commonly used, model based approaches and data-driven methods.",24,1,2024
Graph Koopman Autoencoder for Predictive Covert Communication Against UAV Surveillance,"Low Probability of Detection (LPD) communication aims to obscure the very presence of radio frequency (RF) signals, going beyond just hiding the content of the communication. However, the use of Unmanned Aerial Vehicles (UAVs) introduces a challenge, as UAVs can detect RF signals from the ground by hovering over specific areas of interest. With the growing utilization of UAVs in modern surveillance, there is a crucial need for a thorough understanding of their unknown nonlinear dynamic trajectories to effectively implement LPD communication. Unfortunately, this critical information is often not readily available, posing a significant hurdle in LPD communication. To address this issue, we consider a case-study for enabling terrestrial LPD communication in the presence of multiple UAVs that are engaged in surveillance. We introduce a novel framework that combines graph neural networks (GNN) with Koopman theory to predict the trajectories of multiple fixed-wing UAVs over an extended prediction horizon. Using the predicted UAV locations, we enable LPD communication in a terrestrial ad-hoc network by controlling nodes' transmit powers to keep the received power at UAVs' predicted locations minimized. Our extensive simulations validate the efficacy of the proposed framework in accurately predicting the trajectories of multiple UAVs, thereby effectively establishing LPD communication.",23,1,2024
Independent Component Analysis for Signal Crosstalk Elimination in Infrared Interferometry,"Infrared interferometers are optical devices that can measure optical path-length differences by measuring changes in the refractive index. Several arrangements can be deployed, from single channel devices to multichannel double color heterodyne interferometers. These type of devices are typically used to recover the spatial electron density profile of fusion plasmas. This process involves measuring precisely the phase differences of the intermediate frequencies. A source of error that affects the measurements is the crosstalk that appears due to the coupling of the signals into the different channels of the interferometers. The inter-channel coupling of the signals is extremely difficult to eliminate specially when the probing frequencies are close to each other. In this paper it is shown that Independent Component Analysis effectively eliminates inter-crosstalk coupling in such devices. Furthermore, it is shown how the Signal-to-Noise ratio is dramatically increased when this technique is used.",22,1,2024
Epilepsy Seizure Detection and Prediction using an Approximate Spiking Convolutional Transformer,"Epilepsy is a common disease of the nervous system. Timely prediction of seizures and intervention treatment can significantly reduce the accidental injury of patients and protect the life and health of patients. This paper presents a neuromorphic Spiking Convolutional Transformer, named Spiking Conformer, to detect and predict epileptic seizure segments from scalped long-term electroencephalogram (EEG) recordings. We report evaluation results from the Spiking Conformer model using the Boston Children's Hospital-MIT (CHB-MIT) EEG dataset. By leveraging spike-based addition operations, the Spiking Conformer significantly reduces the classification computational cost compared to the non-spiking model. Additionally, we introduce an approximate spiking neuron layer to further reduce spike-triggered neuron updates by nearly 38% without sacrificing accuracy. Using raw EEG data as input, the proposed Spiking Conformer achieved an average sensitivity rate of 94.9% and a specificity rate of 99.3% for the seizure detection task, and 96.8%, 89.5% for the seizure prediction task, and needs >10x fewer operations compared to the non-spiking equivalent model.",21,1,2024
Tunable Metamaterial Absorber Based on Archimedean Spiral-Shaped Structure,"In recent times, the Archimedean spiral structure has been considered as a promising design element in construction for specific purposes and opening up new possibilities in various applications. Its distinctive geometry exhibits a continuous growth pattern with a constant separation between its successive turns. One notable application of the Archimedean spiral structure in metamaterial absorbers is in achieving broadband absorption. This paper presents a comprehensive simulation of a tunable metamaterial absorber with an Archimedean spiral structure in the frequency range of 60 to 600 terahertz. The absorber's absorption spectrum is controlled by temperature variations that induce changes in the conductivity of vanadium dioxide. The absorber is composed of three layers: a bottom layer made of gold, a middle layer consisting of vanadium dioxide, and an upper layer is constructed using a gold cylinder, from which the Archimedean spiral with the same thickness as the gold is subtracted. This research provides valuable insights into the design and optimization of tunable metamaterial absorber.",23,2,2024
Modified OSD Algorithm with Reduced Gaussian Elimination,"In this paper, the OSD algorithm is modified to perform a limited GE with $O(N^3 \min\{R, 1-R\}^3)$ complexity for an $(N,K)$ linear block code of rate $R=K/N$.",19,2,2024
ALOHA 2: An Enhanced Low-Cost Hardware for Bimanual Teleoperation,"Diverse demonstration datasets have powered significant advances in robot learning, but the dexterity and scale of such data can be limited by the hardware cost, the hardware robustness, and the ease of teleoperation. We introduce ALOHA 2, an enhanced version of ALOHA that has greater performance, ergonomics, and robustness compared to the original design. To accelerate research in large-scale bimanual manipulation, we open source all hardware designs of ALOHA 2 with a detailed tutorial, together with a MuJoCo model of ALOHA 2 with system identification. See the project website atthis http URL.",7,2,2024
Capabilities and Performances of the Matter in Extreme Conditions X-ray Imager of LCLS,"The last decade has shown the great potential that X-ray Free Electron Lasers have to study High Energy Density matter. Experiments at FELs have made significant breakthroughs in Shock Physics and Dynamic Diffraction, Dense Plasma Physics and Warm Dense Matter Science, using techniques such as isochoric heating, inelastic scattering, small angle scattering and diffraction. In addition, and complementary to these techniques, the coherent properties of the beam can be used to image HED samples with high precision. We present new imaging diagnostics and techniques developed at the MEC instrument at LCLS over the last few years. We briefly show result of a previously used Phase Contrast Imaging setup, where the X-ray beam propagates from the target to a camera some distance away revealing its phase, as well as from a new direct imaging approach where the target is re-imaged on the camera with 300nm resolution. Last, we show a new Talbot Phase Imaging method that allows for both measuring the phase and intensity changes of the x-rays introduced by a target with sub-micron resolution.",28,2,2024
Progress in multijunction solar cells,"The advanced multijunction solar cell (MJSC) has emerged as a frontrunner with higher efficiency in photovoltaic literature. It started its journey with a modest 20% efficient tandem solar cell, and today, it has reached an impressive 47.1% photoconversion efficiency (PCE) with six junction combinations. Since the early 1990s, these solar cells have been utilised for space applications. Recently, there has been a trend of using this genre for terrestrial applications as well. However, the complexity and high cost of the fabrication procedure have been the significant challenges over the last three decades. The photovoltaic (PV) community has witnessed a variety of fabrication approaches to address these hurdles. This paper reviews the progression of computational and experimental research approaches of III-V MJSCs and their fabrication processes. In addition, it addresses the barriers hindering the progress of these cells and their prospects. This review gathers insights from a handful number of articles on III-V MJSCs to provide a comprehensive guide for the new entrants, experts and practitioners about the research methodologies, growth techniques, current status, challenges, and opportunities in a timely and conscious manner.",28,2,2024
"Exploring Conceptual Modeling Metaphysics: Existence Containers, Leibniz's Monads and Avicenna's Essence","Requirement specifications in software engineering involve developing a conceptual model of a target domain. The model is based on ontological exploration of things in reality. Many things in such a process closely tie to problems in metaphysics, the field of inquiry of what reality fundamentally is. According to some researchers, metaphysicians are trying to develop an account of the world that properly conceptualizes the way it is, and software design is similar. Notions such as classes, object orientation, properties, instantiation, algorithms, etc. are metaphysical concepts developed many years ago. Exploring the metaphysics of such notions aims to establish quality assurance though some objective foundation not subject to misapprehensions and conventions. Much metaphysical work might best be understood as a model-building process. Here, a model is viewed as a hypothetical structure that we describe and investigate to understand more complex, real-world systems. The purpose of this paper is to enhance understanding of the metaphysical origins of conceptual modeling as exemplified by a specific proposed high-level model called thinging machines (TMs). The focus is on thimacs (things/machine) as a single category of TM modeling in the context of a two-phase world of staticity and dynamics. The general idea of this reality has been inspired by Deleuze s the virtual and related to the classical notions of Leibniz's monads and Avicenna's essence. The analysis of TMs leads to several interesting results about a thimac s nature at the static and existence levels.",20,2,2024
Foundry's perspective on laser and SOA module integration with silicon photonics,"Silicon photonic integrated circuit (PIC) builds on the demand for a low cost approach from established silicon-based manufacturing infrastructure traditionally built for electronics. Besides its natural abundance, silicon has desirable properties such as optically low loss (at certain critical wavelengths), and small form factor to enable high density scaled-up optical on-chip circuitry. However, given its indirect bandgap, the platform is typically integrated with other direct bandgap (e.g., III-V semiconductor) platforms for on-chip light source. An effective solution to integrating light source onto silicon photonics platform is integral to a practical scaled-up and full-fledged integrated photonics implementation. Here, we discuss the integration solutions, and present our foundry's perspective toward realizing it.",20,2,2024
Sound Reconstruction via Optical Multi-Mode Fiber,"Sound reconstruction via arbitrary objects has been a popular method in recent years, based on the recording of scattered light from the target object with a high-speed detector. In this work, we demonstrate the use of multi-mode fiber as a medium that enables reconstruction at a much further distance. By placing a speaker near the fiber and using a high-speed camera with a maximum region of interest, we show that it is possible to reconstruct the sound without relying on specific grain information in the speckle. Basic filtration techniques and spectral subtraction methods are employed to improve the signal quality. After applying band-stop and high-pass filters, the magnitude-squared coherence between the original sound and captured data is 0.8 at fundamental frequencies. The proof-of-concept method that we introduce here can be extended to monitor the vibrations of earth's crust and civilian buildings for structural safety.",18,2,2024
It Will Never Work in Theory,"We have been trying to get software engineering researchers and practitioners to talk to one another for over a decade. This paper describes what we have done, assesses our impact, and recommends an approach that we hope will have greater success.",16,2,2024
Analysing software failure using runtime verification and LTL,"A self-healing software system is an advanced computer program or system designed to detect, diagnose, and automatically recover from faults or errors without human intervention. These systems are typically employed in mission-critical applications where downtime can have significant financial or operational consequences. Failure detection is one of the important steps in the self-healing system. In this research, a method using runtime verification is proposed to diagnose four types of errors at the component level. The simulation on mRUBIS shows that the suggested method has the necessary efficiency in detecting the occurrence of failures.",15,2,2024
Transformational Outsourcing in IT Project Management,"Transformational outsourcing represents a strategic shift from traditional cost-focused outsourcing to a more profound and collaborative approach. It involves partnering with service providers to accomplish routine tasks and drive substantial organizational change and innovation. The report discusses the significance of pursuing transformational outsourcing for IT companies, highlighting its role in achieving strategic growth, competitive advantage, and cost-efficiency while enabling a focus on core competencies. It explores the pros and cons of IT outsourcing, emphasizing the benefits of cost savings, global talent access, scalability, and challenges related to quality, control, and data security. Additionally, the report identifies some critical reasons why outsourcing efforts may fail in achieving organizational goals, including poor vendor selection, communication issues, unclear objectives, resistance to change, and inadequate risk management. When carefully planned and executed, transformational outsourcing offers IT companies a pathway to enhance efficiency and foster innovation and competitiveness in a rapidly evolving technology landscape.",15,2,2024
Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow,"Generative AI (GenAI) has fundamentally changed how knowledge workers, such as software developers, solve tasks and collaborate to build software products. Introducing innovative tools like ChatGPT and Copilot has created new opportunities to assist and augment software developers across various problems. We conducted an empirical study involving interviews with 13 data scientists, managers, developers, designers, and frontend developers to investigate the usage of GenAI. Our study reveals that ChatGPT signifies a paradigm shift in the workflow of software developers. The technology empowers developers by enabling them to work more efficiently, speed up the learning process, and increase motivation by reducing tedious and repetitive tasks. Moreover, our results indicate a change in teamwork collaboration due to software engineers using GenAI for help instead of asking co-workers which impacts the learning loop in agile teams.",12,2,2024
"Materials research for hiper laser fusion facilities: chamber wall, structural material and final optics","The European HiPER project aims to demonstrate commercial viability of inertial fusion energy within the following two decades. This goal requires an extensive Research & Development program on materials for different applications (e.g., first wall, structural components and final optics). In this paper we will discuss our activities in the framework of HiPER to develop materials studies for the different areas of interest. The chamber first wall will have to withstand explosions of at least 100 MJ at a repetition rate of 5-10 Hz. If direct drive targets are used, a dry wall chamber operated in vacuum is preferable. In this situation the major threat for the wall stems from ions. For reasonably low chamber radius (5-10 m) new materials based on W and C are being investigated, e.g., engineered surfaces and nanostructured materials. Structural materials will be subject to high fluxes of neutrons leading to deleterious effects, such as, swelling. Low activation advanced steels as well as new nanostructured materials are being investigated. The final optics lenses will not survive the extreme ion irradiation pulses originated in the explosions. Therefore, mitigation strategies are being investigated. In addition, efforts are being carried out in understanding optimized conditions to minimize the loss of optical properties by neutron and gamma irradiation.",11,2,2024
Thermo-mechanical behaviour of a tungsten first wall in HiPER laser fusion scenarios,"The behaviour of a tungsten first wall is studied under the irradiation conditions predicted for the different operation scenarios of the European Laser fusion project HiPER, which is based on direct drive targets and an evacuated dry wall chamber. The scenarios correspond to different stages in the development of a nuclear fusion reactor, from proof of principle (bunch mode facility) to economic feasibility (pre-commercial power plant). This work constitutes a quantitative study to evaluate the first wall performance under realistic irradiation conditions in the different scenarios. We calculated the radiation fluxes assuming the geometrical configurations reported so far for HiPER. Then, we calculated the irradiation-induced first wall temperature evolution and the thermo-mechanical response of the material. The results indicate that the first wall will plastically deform up to a few microns underneath the surface. Continuous operation in power plant leads to fatigue failure with crack generation and growth. Finally, the crack propagation and the minimum W thickness required to fulfil the first wall protection role is studied. The response of tungsten as first wall material as well as its main limitations will be discussed for the HiPER scenarios.",11,2,2024
Universal Imitation Games,"Alan Turing proposed in 1950 a framework called an imitation game to decide if a machine could think. Using mathematics developed largely after Turing -- category theory -- we analyze a broader class of universal imitation games (UIGs), which includes static, dynamic, and evolutionary games. In static games, the participants are in a steady state. In dynamic UIGs, ""learner"" participants are trying to imitate ""teacher"" participants over the long run. In evolutionary UIGs, the participants are competing against each other in an evolutionary game, and participants can go extinct and be replaced by others with higher fitness. We use the framework of category theory -- in particular, two influential results by Yoneda -- to characterize each type of imitation game. Universal properties in categories are defined by initial and final objects. We characterize dynamic UIGs where participants are learning by inductive inference as initial algebras over well-founded sets, and contrast them with participants learning by conductive inference over the final coalgebra of non-well-founded sets. We briefly discuss the extension of our categorical framework for UIGs to imitation games on quantum computers.",2,2,2024
Frequency-Guided U-Net: Leveraging Attention Filter Gates and Fast Fourier Transformation for Enhanced Medical Image Segmentation,"Purpose Medical imaging diagnosis faces challenges, including low-resolution images due to machine artifacts and patient movement. This paper presents the Frequency-Guided U-Net (GFNet), a novel approach for medical image segmentation that addresses challenges associated with low-resolution images and inefficient feature extraction. Approach In response to challenges related to computational cost and complexity in feature extraction, our approach introduces the Attention Filter Gate. Departing from traditional spatial domain learning, our model operates in the frequency domain using FFT. A strategically placed weighted learnable matrix filters feature, reducing computational costs. FFT is integrated between up-sampling and down-sampling, mitigating issues of throughput, latency, FLOP, and enhancing feature extraction. Results Experimental outcomes shed light on model performance. The Attention Filter Gate, a pivotal component of GFNet, achieves competitive segmentation accuracy (Mean Dice: 0.8366, Mean IoU: 0.7962). Comparatively, the Attention Gate model surpasses others, with a Mean Dice of 0.9107 and a Mean IoU of 0.8685. The widely-used U-Net baseline demonstrates satisfactory performance (Mean Dice: 0.8680, Mean IoU: 0.8268). Conclusion his work introduces GFNet as an efficient and accurate method for medical image segmentation. By leveraging the frequency domain and attention filter gates, GFNet addresses key challenges of information loss, computational cost, and feature extraction limitations. This novel approach offers potential advancements for computer-aided diagnosis and other healthcare applications. Keywords: Medical Segmentation, Neural Networks,",25,2,2024
SynthBrainGrow: Synthetic Diffusion Brain Aging for Longitudinal MRI Data Generation in Young People,"Synthetic longitudinal brain MRI simulates brain aging and would enable more efficient research on neurodevelopmental and neurodegenerative conditions. Synthetically generated, age-adjusted brain images could serve as valuable alternatives to costly longitudinal imaging acquisitions, serve as internal controls for studies looking at the effects of environmental or therapeutic modifiers on brain development, and allow data augmentation for diverse populations. In this paper, we present a diffusion-based approach called SynthBrainGrow for synthetic brain aging with a two-year step. To validate the feasibility of using synthetically-generated data on downstream tasks, we compared structural volumetrics of two-year-aged brains against synthetically-aged brain MRI. Results show that SynthBrainGrow can accurately capture substructure volumetrics and simulate structural changes such as ventricle enlargement and cortical thinning. Our approach provides a novel way to generate longitudinal brain datasets from cross-sectional data to enable augmented training and benchmarking of computational tools for analyzing lifespan trajectories. This work signifies an important advance in generative modeling to synthesize realistic longitudinal data with limited lifelong MRI scans. The code is available at XXX.",22,2,2024
Delay and Overhead Efficient Transmission Scheduling for Federated Learning in UAV Swarms,This paper studies the wireless scheduling design to coordinate the transmissions of (local) model parameters of federated learning (FL) for a swarm of unmanned aerial vehicles (UAVs). The overall goal of the proposed design is to realize the FL training and aggregation processes with a central aggregator exploiting the sensory data collected by the UAVs but it considers the multi-hop wireless network formed by the UAVs. Such transmissions of model parameters over the UAV-based wireless network potentially cause large transmission delays and overhead. Our proposed framework smartly aggregates local model parameters trained by the UAVs while efficiently transmitting the underlying parameters to the central aggregator in each FL global round. We theoretically show that the proposed scheme achieves minimal delay and communication overhead. Extensive numerical experiments demonstrate the superiority of the proposed scheme compared to other baselines.,22,2,2024
Comparative approach: Electric distribution optimization with loss minimization algorithm and particle swarm optimization,"Power systems are very large and complex, it can be influenced by many unexpected events this makes power system optimization problems difficult to solve, hence methods for solving these problems ought to be an active research topic. This review presents an overview of important mathematical comparaison of loss minimization algorithm and particle swarm optimization algorithm in terms of the performances of electric distribution.",19,2,2024
Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum,"Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Thus, understanding the biological mechanisms of robustness is an important step towards building trustworthy and safe systems. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems.",5,2,2024
Automatic Creative Selection with Cross-Modal Matching,"Application developers advertise their Apps by creating product pages with App images, and bidding on search terms. It is then crucial for App images to be highly relevant with the search terms. Solutions to this problem require an image-text matching model to predict the quality of the match between the chosen image and the search terms. In this work, we present a novel approach to matching an App image to search terms based on fine-tuning a pre-trained LXMERT model. We show that compared to the CLIP model and a baseline using a Transformer model for search terms, and a ResNet model for images, we significantly improve the matching accuracy. We evaluate our approach using two sets of labels: advertiser associated (image, search term) pairs for a given application, and human ratings for the relevance between (image, search term) pairs. Our approach achieves 0.96 AUC score for advertiser associated ground truth, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 8% and 14%. For human labeled ground truth, our approach achieves 0.95 AUC score, outperforming the transformer+ResNet baseline and the fine-tuned CLIP model by 16% and 17%.",28,2,2024
MaRDIFlow: A CSE workflow framework for abstracting meta-data from FAIR computational experiments,"Numerical algorithms and computational tools are instrumental in navigating and addressing complex simulation and data processing tasks. The exponential growth of metadata and parameter-driven simulations has led to an increasing demand for automated workflows that can replicate computational experiments across platforms. In general, a computational workflow is defined as a sequential description for accomplishing a scientific objective, often described by tasks and their associated data dependencies. If characterized through input-output relation, workflow components can be structured to allow interchangeable utilization of individual tasks and their accompanying metadata. In the present work, we develop a novel computational framework, namely, MaRDIFlow, that focuses on the automation of abstracting meta-data embedded in an ontology of mathematical objects. This framework also effectively addresses the inherent execution and environmental dependencies by incorporating them into multi-layered descriptions. Additionally, we demonstrate a working prototype with example use cases and methodically integrate them into our workflow tool and data provenance framework. Furthermore, we show how to best apply the FAIR principles to computational workflows, such that abstracted components are Findable, Accessible, Interoperable, and Reusable in nature.",28,2,2024
Multidimensional Compressed Sensing for Spectral Light Field Imaging,"This paper considers a compressive multi-spectral light field camera model that utilizes a one-hot spectralcoded mask and a microlens array to capture spatial, angular, and spectral information using a single monochrome sensor. We propose a model that employs compressed sensing techniques to reconstruct the complete multi-spectral light field from undersampled measurements. Unlike previous work where a light field is vectorized to a 1D signal, our method employs a 5D basis and a novel 5D measurement model, hence, matching the intrinsic dimensionality of multispectral light fields. We mathematically and empirically show the equivalence of 5D and 1D sensing models, and most importantly that the 5D framework achieves orders of magnitude faster reconstruction while requiring a small fraction of the memory. Moreover, our new multidimensional sensing model opens new research directions for designing efficient visual data acquisition algorithms and hardware.",27,2,2024
Enhancing Credit Card Fraud Detection A Neural Network and SMOTE Integrated Approach,"Credit card fraud detection is a critical challenge in the financial sector, demanding sophisticated approaches to accurately identify fraudulent transactions. This research proposes an innovative methodology combining Neural Networks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to enhance the detection performance. The study addresses the inherent imbalance in credit card transaction data, focusing on technical advancements for robust and precise fraud detection. Results demonstrat e that the integration of NN and SMOTE exhibits superior precision, recall, and F1-score compared to traditional models, highlighting its potential as an advanced solution for handling imbalanced datasets in credit card fraud detection scenarios. This rese arch contributes to the ongoing efforts to develop effective and efficient mechanisms for safeguarding financial transactions from fraudulent activities.",27,2,2024
Leveraging Pre-trained CNNs for Efficient Feature Extraction in Rice Leaf Disease Classification,"Rice disease classification is a critical task in agricultural research, and in this study, we rigorously evaluate the impact of integrating feature extraction methodologies within pre-trained convolutional neural networks (CNNs). Initial investigations into baseline models, devoid of feature extraction, revealed commendable performance with ResNet-50 and ResNet-101 achieving accuracies of 91% and 92%, respectively. Subsequent integration of Histogram of Oriented Gradients (HOG) yielded substantial improvements across architectures, notably propelling the accuracy of EfficientNet-B7 from 92\% to an impressive 97%. Conversely, the application of Local Binary Patterns (LBP) demonstrated more conservative performance enhancements. Moreover, employing Gradient-weighted Class Activation Mapping (Grad-CAM) unveiled that HOG integration resulted in heightened attention to disease-specific features, corroborating the performance enhancements observed. Visual representations further validated HOG's notable influence, showcasing a discernible surge in accuracy across epochs due to focused attention on disease-affected regions. These results underscore the pivotal role of feature extraction, particularly HOG, in refining representations and bolstering classification accuracy. The study's significant highlight was the achievement of 97% accuracy with EfficientNet-B7 employing HOG and Grad-CAM, a noteworthy advancement in optimizing pre-trained CNN-based rice disease identification systems. The findings advocate for the strategic integration of advanced feature extraction techniques with cutting-edge pre-trained CNN architectures, presenting a promising avenue for substantially augmenting the precision and effectiveness of image-based disease classification systems in agricultural contexts.",26,2,2024
Swarm UAVs Communication,"The advancement in cyber-physical systems has opened a new way in disaster management and rescue operations. The usage of UAVs is very promising in this context. UAVs, mainly quadcopters, are small in size and their payload capacity is limited. A single UAV can not traverse the whole area. Hence multiple UAVs or swarms of UAVs come into the picture managing the entire payload in a modular and equiproportional manner. In this work we have explored a vast topic related to UAVs. Among the UAVs quadcopter is the main focus. We explored the types of quadcopters, their flying strategy,their communication protocols, architecture and controlling techniques, followed by the swarm behaviour in nature and UAVs. Swarm behaviour and a few swarm optimization algorithms has been explored here. Swarm architecture and communication in between swarm UAV networks also got a special attention in our work. In disaster management the UAV swarm network must have to search a large area. And for this proper path planning algorithm is required. We have discussed the existing path planning algorithm, their advantages and disadvantages in great detail. Formation maintenance of the swarm network is an important issue which has been explored through leader-follower technique. The wireless path loss model has been modelled using friis and ground ray reflection model. Using this path loss models we have managed to create the link budget and simulate the variation of communication link performance with the variation of distance.",24,2,2024
Revolutionizing Retail Analytics: Advancing Inventory and Customer Insight with AI,"In response to the significant challenges facing the retail sector, including inefficient queue management, poor demand forecasting, and ineffective marketing, this paper introduces an innovative approach utilizing cutting-edge machine learning technologies. We aim to create an advanced smart retail analytics system (SRAS), leveraging these technologies to enhance retail efficiency and customer engagement. To enhance customer tracking capabilities, a new hybrid architecture is proposed integrating several predictive models. In the first stage of the proposed hybrid architecture for customer tracking, we fine-tuned the YOLOV8 algorithm using a diverse set of parameters, achieving exceptional results across various performance metrics. This fine-tuning process utilized actual surveillance footage from retail environments, ensuring its practical applicability. In the second stage, we explored integrating two sophisticated object-tracking models, BOT-SORT and ByteTrack, with the labels detected by YOLOV8. This integration is crucial for tracing customer paths within stores, which facilitates the creation of accurate visitor counts and heat maps. These insights are invaluable for understanding consumer behavior and improving store operations. To optimize inventory management, we delved into various predictive models, optimizing and contrasting their performance against complex retail data patterns. The GRU model, with its ability to interpret time-series data with long-range temporal dependencies, consistently surpassed other models like Linear Regression, showing 2.873% and 29.31% improvements in R2-score and mAPE, respectively.",24,2,2024
"News ecosystem dynamics: Supply, Demand, Diffusion, and the role of Disinformation","The digital age provides new challenges as information travels more quickly in a system of increasing complexity. But it also offers new opportunities, as we can track and study the system more efficiently. Several studies individually addressed different digital tracks, focusing on specific aspects like disinformation production or content-sharing dynamics. In this work, we propose to study the news ecosystem as an information market by analysing three main metrics: Supply, Demand, and Diffusion of information. Working on a dataset relative to Italy from December 2019 to August 2020, we validate the choice of the metrics, proving their static and dynamic relations, and their potential in describing the whole system. We demonstrate that these metrics have specific equilibrium relative levels. We reveal the strategic role of Demand in leading a non-trivial network of causal relations. We show how disinformation news Supply and Diffusion seem to cluster among different social media platforms. Disinformation also appears to be closer to information Demand than the general news Supply and Diffusion, implying a potential danger to the health of the public debate. Finally, we prove that the share of disinformation in the Supply and Diffusion of news has a significant linear relation with the gap between Demand and Supply/Diffusion of news from all sources. This finding allows for a real-time assessment of disinformation share in the system. It also gives a glimpse of the potential future developments in the modelisation of the news ecosystem as an information market studied through its main drivers.",22,2,2024
SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials,"Recently, interpreting complex charts with logical reasoning have emerged as challenges due to the development of vision-language models. A prior state-of-the-art (SOTA) model, Deplot, has presented an end-to-end method that leverages the vision-language model to convert charts into table format utilizing Large Language Models (LLMs) for reasoning. However, unlike natural images, charts contain a mix of essential and irrelevant information required for chart reasoning, and we discover that this characteristic can lower the performance of chart-to-table extraction. In this paper, we introduce SIMPLOT, a method designed to extract only the elements necessary for chart reasoning. The proposed method involves two steps: 1) training to mimic a simple plot that contains only the essential information from a complex chart for table extraction, followed by 2) performing reasoning based on the table. Our model enables accurate chart reasoning without the need for additional annotations or datasets, and its effectiveness is demonstrated through various experiments. Furthermore, we propose a novel prompt addressing the shortcoming of recent SOTA model, ignoring visual attributes such as color. Our source code is available atthis https URL.",22,2,2024
Technosignatures longevity and Lindy's law,"The probability of detecting technosignatures (i.e. evidence of technological activity beyond Earth) increases with their longevity, or the time interval over which they manifest. Therefore, the assumed distribution of longevities has some bearing on the chances of success of technosignature searches, as well as on the inferred age of technosignatures following a first contact. Here, we investigate the possibility that the longevity of technosignatures conforms to the so-called Lindy's law, whereby, at any time, their remaining life expectancy is roughly proportional to their age. We show that, if Lindy's law applies, the general tenet that the first detected technosignature ought to be very long lived may be overruled. We conclude by discussing the number of emitters that had to appear, over the history of the Galaxy, in order for one of them to be detectable today from Earth.",20,2,2024
Cycling on rough roads: A model for resistance and vibration,"Minimising opposing forces is a matter of interest to most cyclists. These forces arise from passage through air (""drag"") and interaction with the road surface (""resistance""). Recent work recognises that resistance forces arise not only from the deformation of the tyre (""rolling resistance"") but also from irregularities in the road surface (""roughness resistance""), which lead to power dissipation in the body of the rider through vibration. The latter effect may also have an adverse impact on human health. In this work we offer a quantitative theory of roughness resistance and vibration that links these effects to a surface characterisation in terms of the International Roughness Index (IRI). We show that the roughness resistance and the Vibration Dose Value (or VDV, the usual vibration dosage metric) can be expressed in terms of elementary formulae. The roughness resistance depends only on the vertical stiffness of the bicycle and the roughness index. Surprisingly, other apparently relevant parameters, such as physiological characteristics of the bicycle rider and other features of the bicycle, do not enter. For roads of moderate roughness, roughness resistance is larger than rolling resistance. For very rough roads, roughness resistance is larger than aerodynamic drag. So only on roads of high quality (in most jurisdictions, accounting for less than 10~\% of the total) can roughness resistance be ignored. Roughness resistance can be mitigated by reducing the vertical stiffness of the bicycle. In common with other recent reports, we find that almost any cycling activity will breach public health guidelines relating to Vibration Dose Value.",16,2,2024
Proof-of-concept: Using ChatGPT to Translate and Modernize an Earth System Model from Fortran to Python/JAX,"Earth system models (ESMs) are vital for understanding past, present, and future climate, but they suffer from legacy technical infrastructure. ESMs are primarily implemented in Fortran, a language that poses a high barrier of entry for early career scientists and lacks a GPU runtime, which has become essential for continued advancement as GPU power increases and CPU scaling slows. Fortran also lacks differentiability - the capacity to differentiate through numerical code - which enables hybrid models that integrate machine learning methods. Converting an ESM from Fortran to Python/JAX could resolve these issues. This work presents a semi-automated method for translating individual model components from Fortran to Python/JAX using a large language model (GPT-4). By translating the photosynthesis model from the Community Earth System Model (CESM), we demonstrate that the Python/JAX version results in up to 100x faster runtimes using GPU parallelization, and enables parameter estimation via automatic differentiation. The Python code is also easy to read and run and could be used by instructors in the classroom. This work illustrates a path towards the ultimate goal of making climate models fast, inclusive, and differentiable.",13,2,2024
Queuing dynamics of asynchronous Federated Learning,"We study asynchronous federated learning mechanisms with nodes having potentially different computational speeds. In such an environment, each node is allowed to work on models with potential delays and contribute to updates to the central server at its own pace. Existing analyses of such algorithms typically depend on intractable quantities such as the maximum node delay and do not consider the underlying queuing dynamics of the system. In this paper, we propose a non-uniform sampling scheme for the central server that allows for lower delays with better complexity, taking into account the closed Jackson network structure of the associated computational graph. Our experiments clearly show a significant improvement of our method over current state-of-the-art asynchronous algorithms on an image classification problem.",12,2,2024
Explaining Grover's algorithm with a colony of ants: a pedagogical model for making quantum technology comprehensible,"The rapid growth of quantum technologies requires an increasing number of physicists, computer scientists, and engineers who can work on these technologies. For educating these professionals, quantum mechanics should stop being perceived as incomprehensible. In this paper we contribute to this change by presenting a pedagogical model for explaining Grover's search algorithm, a prominent quantum algorithm. This model visualizes the three main steps of Grover's algorithm and, in addition to explaining the algorithm itself, introduces three key principles of quantum mechanics: superposition, interference, and state collapse at measurement. The pedagogical model, visualized by a video, is called the ""Ant Colony Maze model"". It represents the search problems as finding the exit of a maze, and visualizes Grover's search algorithm as a strategy by which a colony of ants finds that exit.",9,2,2024
The GA4GH Task Execution API: Enabling Easy Multi Cloud Task Execution,"The Global Alliance for Genomics and Health (GA4GH) Task Execution Service (TES) API is a standardized schema and API for describing and executing batch execution tasks. It provides a common way to submit and manage tasks to a variety of compute environments, including on premise High Performance Compute and High Throughput Computing (HPC/HTC) systems, Cloud computing platforms, and hybrid environments. The TES API is designed to be flexible and extensible, allowing it to be adapted to a wide range of use cases, such as ""bringing compute to the data"" solutions for federated and distributed data analysis or load balancing across multi cloud infrastructures. This API has been adopted by a number of different service providers and utilized by several workflow engines. Using its capabilities, genomes research institutes are building hybrid compute systems to study life science.",8,2,2024
A quantum neural network framework for scalable quantum circuit approximation of unitary matrices,"In this paper, we develop a Lie group theoretic approach for parametric representation of unitary matrices. This leads to develop a quantum neural network framework for quantum circuit approximation of multi-qubit unitary gates. Layers of the neural networks are defined by product of exponential of certain elements of the Standard Recursive Block Basis, which we introduce as an alternative to Pauli string basis for matrix algebra of complex matrices of order $2^n$. The recursive construction of the neural networks implies that the quantum circuit approximation is scalable i.e. quantum circuit for an $(n+1)$-qubit unitary can be constructed from the circuit of $n$-qubit system by adding a few CNOT gates and single-qubit gates.",7,2,2024
A Multiscale Fracture Model using Peridynamic Enrichment of Finite Elements within an Adaptive Partition of Unity: Experimental Validation,"Partition of unity methods (PUM) are of domain decomposition type and provide the opportunity for multiscale and multiphysics numerical modeling. Within the PUM global-local enrichment scheme [1, 2] different physical models can exist to capture multiscale behavior. For instance, we consider classical linear elasticity globally and local zones where fractures occur. The elastic fields of the undamaged media provide appropriate boundary data for local PD simulations on a subdomain containing the crack tip to grow the crack path. Once the updated crack path is found, the elastic field in the body and surrounding the crack is updated using PUM basis with appropriate enrichment near the crack. The subdomain for the PD simulation is chosen to include the current crack tip as well as nearby features that will influence crack growth. This paper is part II of this series and validates the combined PD/PUM simulator against the experimental results presented in [3]. The presented results show that we can attain good agreement between experimental and simulation data with a local PD subdomain that is moving with the crack tip and adaptively chosen size.",1,2,2024
A Review on Industrial Augmented Reality Systems for the Industry 4.0 Shipyard,"Shipbuilding companies are upgrading their inner workings in order to create Shipyards 4.0, where the principles of Industry 4.0 are paving the way to further digitalized and optimized processes in an integrated network. Among the different Industry 4.0 technologies, this article focuses on Augmented Reality, whose application in the industrial field has led to the concept of Industrial Augmented Reality (IAR). This article first describes the basics of IAR and then carries out a thorough analysis of the latest IAR systems for industrial and shipbuilding applications. Then, in order to build a practical IAR system for shipyard workers, the main hardware and software solutions are compared. Finally, as a conclusion after reviewing all the aspects related to IAR for shipbuilding, it is proposed an IAR system architecture that combines Cloudlets and Fog Computing, which reduce latency response and accelerate rendering tasks while offloading compute intensive tasks from the Cloud.",1,2,2024
What's in the Flow? Exploiting Temporal Motion Cues for Unsupervised Generic Event Boundary Detection,"Generic Event Boundary Detection (GEBD) task aims to recognize generic, taxonomy-free boundaries that segment a video into meaningful events. Current methods typically involve a neural model trained on a large volume of data, demanding substantial computational power and storage space. We explore two pivotal questions pertaining to GEBD: Can non-parametric algorithms outperform unsupervised neural methods? Does motion information alone suffice for high performance? This inquiry drives us to algorithmically harness motion cues for identifying generic event boundaries in videos. In this work, we propose FlowGEBD, a non-parametric, unsupervised technique for GEBD. Our approach entails two algorithms utilizing optical flow: (i) Pixel Tracking and (ii) Flow Normalization. By conducting thorough experimentation on the challenging Kinetics-GEBD and TAPOS datasets, our results establish FlowGEBD as the new state-of-the-art (SOTA) among unsupervised methods. FlowGEBD exceeds the neural models on the Kinetics-GEBD dataset by obtaining an F1@0.05 score of 0.713 with an absolute gain of 31.7% compared to the unsupervised baseline and achieves an average F1 score of 0.623 on the TAPOS validation dataset.",15,2,2024
"The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement, Odometry, and Egocentric Video","We introduce the Visual Experience Dataset (VEDB), a compilation of over 240 hours of egocentric video combined with gaze- and head-tracking data that offers an unprecedented view of the visual world as experienced by human observers. The dataset consists of 717 sessions, recorded by 58 observers ranging from 6-49 years old. This paper outlines the data collection, processing, and labeling protocols undertaken to ensure a representative sample and discusses the potential sources of error or bias within the dataset. The VEDB's potential applications are vast, including improving gaze tracking methodologies, assessing spatiotemporal image statistics, and refining deep neural networks for scene and activity recognition. The VEDB is accessible through established open science platforms and is intended to be a living dataset with plans for expansion and community contributions. It is released with an emphasis on ethical considerations, such as participant privacy and the mitigation of potential biases. By providing a dataset grounded in real-world experiences and accompanied by extensive metadata and supporting code, the authors invite the research community to utilize and contribute to the VEDB, facilitating a richer understanding of visual perception and behavior in naturalistic settings.",15,2,2024
Learning Low-Rank Feature for Thorax Disease Classification,"Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in medical image domain. We study thorax disease classification in this paper. Effective extraction of features for the disease areas is crucial for disease classification on radiographic images. While various neural architectures and training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there are no principled methods which can effectively reduce the adverse effect of noise and background, or non-disease areas, on the radiographic images for disease classification. To address this challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks. The LRFL method is both empirically motivated by the low frequency property observed on all the medical datasets in this paper, and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. In the empirical study, using a neural network such as a ViT or a CNN pre-trained on unlabeled chest X-rays by Masked Autoencoders (MAE), our novel LRFL method is applied on the pre-trained neural network and demonstrate better classification results in terms of both multiclass area under the receiver operating curve (mAUC) and classification accuracy.",14,2,2024
Cyber Security issues and Blockchain-Deep Learning based solutions for UAV and Internet of Drones (FANETs),"Safety-critical systems such as automated embedded or industrial systems have a strong dependency on the trustworthiness of data collection. As sensors are the critical component for those systems, it is imperative to address the attack resilience of sensors",29,2,2024
State-of-the-Art Approaches to Enhancing Privacy Preservation of Machine Learning Datasets: A Survey,"This paper examines the evolving landscape of machine learning (ML) and its profound impact across various sectors, with a special focus on the emerging field of Privacy-preserving Machine Learning (PPML). As ML applications become increasingly integral to industries like telecommunications, financial technology, and surveillance, they raise significant privacy concerns, necessitating the development of PPML strategies. The paper highlights the unique challenges in safeguarding privacy within ML frameworks, which stem from the diverse capabilities of potential adversaries, including their ability to infer sensitive information from model outputs or training data.We delve into the spectrum of threat models that characterize adversarial intentions, ranging from membership and attribute inference to data reconstruction. The paper emphasizes the importance of maintaining the confidentiality and integrity of training data, outlining current research efforts that focus on refining training data to minimize privacy-sensitive information and enhancing data processing techniques to uphold privacy.Through a comprehensive analysis of privacy leakage risks and countermeasures in both centralized and collaborative learning settings, this paper aims to provide a thorough understanding of effective strategies for protecting ML training data against privacy intrusions. It explores the balance between data privacy and model utility, shedding light on privacy-preserving techniques that leverage cryptographic methods, Differential Privacy, and Trusted Execution Environments. The discussion extends to the application of these techniques in sensitive domains, underscoring the critical role of PPML in ensuring the privacy and security of ML systems.",25,2,2024
Securing Bluetooth Low Energy: A Literature Review,"Bluetooth Low Energy (BLE) technology, operating within the widely used 2.4 GHz ISM band, stands as a cornerstone in modern wireless communication frameworks alongside its classic Bluetooth counterpart. This paper delves into the foundational aspects of BLE, excluding niche components, to explore its core functionalities and pivotal role in diverse connectivity needs. BLE's specialization in catering to low-power devices ensures optimal energy utilization, making it indispensable in IoT applications where energy efficiency is paramount. Its versatility finds applications across consumer electronics, industrial automation, and healthcare, ensuring reliability and efficiency in safety-critical systems and enhancing user convenience through remote control capabilities. However, the wireless nature of BLE interfaces exposes them to cybersecurity threats, necessitating robust security measures for mitigating risks such as sniffing, DoS attacks, and message injection. Continuous research and development efforts are essential to stay ahead of emerging threats and safeguard BLE-enabled systems and data.",24,2,2024
HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections,"Internet image collections containing photos captured by crowds of photographers show promise for enabling digital exploration of large-scale tourist landmarks. However, prior works focus primarily on geometric reconstruction and visualization, neglecting the key role of language in providing a semantic interface for navigation and fine-grained understanding. In constrained 3D domains, recent methods have leveraged vision-and-language models as a strong prior of 2D visual semantics. While these models display an excellent understanding of broad visual semantics, they struggle with unconstrained photo collections depicting such tourist landmarks, as they lack expert knowledge of the architectural domain. In this work, we present a localization system that connects neural representations of scenes depicting large-scale landmarks with text describing a semantic region within the scene, by harnessing the power of SOTA vision-and-language models with adaptations for understanding landmark scene semantics. To bolster such models with fine-grained knowledge, we leverage large-scale Internet data containing images of similar landmarks along with weakly-related textual information. Our approach is built upon the premise that images physically grounded in space can provide a powerful supervision signal for localizing new concepts, whose semantics may be unlocked from Internet textual metadata with large language models. We use correspondences between views of scenes to bootstrap spatial understanding of these semantics, providing guidance for 3D-compatible segmentation that ultimately lifts to a volumetric scene representation. Our results show that HaLo-NeRF can accurately localize a variety of semantic concepts related to architectural landmarks, surpassing the results of other 3D models as well as strong 2D segmentation baselines. Our project page is atthis https URL.",14,2,2024
Sugarcane Health Monitoring With Satellite Spectroscopy and Machine Learning: A Review,"Research into large-scale crop monitoring has flourished due to increased accessibility to satellite imagery. This review delves into previously unexplored and under-explored areas in sugarcane health monitoring and disease/pest detection using satellite-based spectroscopy and Machine Learning (ML). It discusses key considerations in system development, including relevant satellites, vegetation indices, ML methods, factors influencing sugarcane reflectance, optimal growth conditions, common diseases, and traditional detection methods. Many studies highlight how factors like crop age, soil type, viewing angle, water content, recent weather patterns, and sugarcane variety can impact spectral reflectance, affecting the accuracy of health assessments via spectroscopy. However, these variables have not been fully considered in the literature. In addition, the current literature lacks comprehensive comparisons between ML techniques and vegetation indices. We address these gaps in this review. We discuss that, while current findings suggest the potential for an ML-driven satellite spectroscopy system for monitoring sugarcane health, further research is essential. This paper offers a comprehensive analysis of previous research to aid in unlocking this potential and advancing the development of an effective sugarcane health monitoring system using satellite technology.",13,2,2024
Enhancing Data Security through Rainbow Antimagic Graph Coloring for Secret-Share Distribution and Reconstruction,"Now-a-days, ensuring data security has become an increasingly formidable challenge in safeguarding individuals' sensitive information. Secret-sharing scheme has evolved as a most successful cryptographic technique that allows a secret to be divided or distributed among a group of participants in such a way that only a subset of those participants can reconstruct the original secret. This provides a safe level of security and redundancy, ensuring that no single individual possesses the complete secret. The implementation of Rainbow Antimagic coloring within these schemes not only safeguards the data but also ensures an advanced level of information security among multi-participant groups. Additionally, the retrieved data is reconstructed and can be disseminated to all group participants via multiple rounds of communication.",12,2,2024
Cybersecurity Threat Analysis And Attack Simulations For Unmanned Aerial Vehicle Networks,"Drones, also known as unmanned air vehicles (UAVs), have revolutionised various industries, from farming to national security. (Wexler., Lesley. 2016) However, their broad use has revealed a severe weakness in cybersecurity. (Jean-Paul Yaacoub 2020) The urgent necessity to defend UAV networks from new cyber threats is explored in-depth in this research, making it a crucial subject for both technological development and national security. The two essential areas of our study are assault simulation and threat analysis in cybersecurity. This work demonstrates how easy it is to hack a drone mid-flight using only a Raspberry Pi3 and open-source online tools. This work illustrates the ability to penetrate a DJI drone currently used by the mercenary soldiers in the Ukraine war. (Greg Myre March, 2023) This research examines strategies used to attack UAV networks, such as the de-authentic attack and the man-in-the-middle attack. This work investigates the weaknesses in these networks' sophisticated attack simulations with a Raspberry PI 3 and the Alpha network adaptor from Amazon, showing that basic tools are needed to perform cyberattacks on drones. This research proposes creative solutions and preventative methods for protecting UAV operations and highlights the seriousness of the problem. As drones become more prevalent daily, maintaining their security becomes crucial. This work provides a compelling perspective on protecting vital infrastructure and preserving our skies by bridging the gap between the latest technologies and cybersecurity.",12,2,2024
Machine Unlearning in Large Language Models,"Recently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.",3,2,2024
Patent Value Characterization -- An Empirical Analysis of Elevator Industry Patents,"The global patent application count has steadily increased, achieving eight consecutive years of growth.The global patent industry has shown a general trend of expansion. This is attributed to the increasing innovation activities, particularly in the fields of technology, healthcare, and biotechnology. Some emerging market countries, such as China and India, have experienced significant growth in the patent domain, becoming important participants in global patent activities.",20,2,2024
The Nexus of Open Science and Innovation: Insights from Patent Citations,"This paper aims to analyze the extent to which inventive activity relies on open science. In other words, it investigates whether inventors utilize Open Access (OA) publications more than subscription-based ones, especially given that some inventors may lack institutional access. To achieve this, we utilized the (Marx, 2023) database, which contains citations of patents to scientific publications (Non-Patent References-NPRs). We focused on publications closely related to invention, specifically those cited solely by inventors within the body of patent texts. Our dataset was supplemented by OpenAlex data. The final sample comprised 961,104 publications cited in patents, of which 861,720 had a DOI. Results indicate that across all disciplines, OA publications are 38% more prevalent in patent citations (NPRs) than in the overall OpenAlex database. In biology and medicine, inventors use 73% and 27% more OA publications, respectively, compared to closed-access ones. Chemistry and computer science are also disciplines where OA publications are more frequently utilized in patent contexts than subscription-based ones.",14,2,2024
Expanding Conservation Science through Emerging Interdisciplinary STEM Fields,"Conservation science is an interdisciplinary field that primarily draws on knowledge from the natural sciences, social sciences, and humanities to inform policy, planning, and practice. Since its formalization as a discipline, conservation science has also increasingly incorporated tools from integrative biological fields, such as animal behavior, genetics, and, more recently, physiology. Given that the biodiversity crisis constitutes one of the greatest challenges of the 21st century, with tremendous consequences for global sustainability and human health, creating a diverse conservation toolbox is important for addressing complex conservation threats. To assess the integration of three emerging integrative biological disciplines (physiology, biomechanics, and technology) into recent conservation science research, we queried publications from five broad-scope conservation-focused journals from 2010-2022. We found that the proportion of published articles incorporating these integrative biological techniques was low, ranging from 0-4% per year. With only 2.1% of total articles accessing tools or techniques from conservation physiology, conservation technology, and conservation biomechanics, we propose that there is still a substantial opportunity for further integration. We provide a case study for each integrative field to illustrate the capacity for its tools to contribute to positive conservation outcomes. We further outline how each field promotes novel or reimagined opportunities for collaborations. Finally, we discuss the interconnectedness of the three fields and how they can support the continuing expansion of conservation science as an evidence-based, action-oriented discipline through the application of a Challenge-Mechanism-Partnership framework.",10,2,2024
Virtual quantum resource distillation: General framework and applications,"We develop the general framework of virtual resource distillation -- an alternative distillation strategy proposed in [Phys. Rev. Lett. 132, 050203 (2024)], which extends conventional quantum resource distillation by integrating the power of classical postprocessing. The framework presented here is applicable not only to quantum states, but also dynamical quantum objects such as quantum channels and higher-order processes. We provide a general characterization and benchmarks for the performance of virtual resource distillation in the form of computable semidefinite programs as well as several operationally motivated quantities. We apply our general framework to various concrete settings of interest, including standard resource theories such as entanglement, coherence, and magic, as well as settings involving dynamical resources such as quantum memory, quantum communication, and non-Markovian dynamics. The framework of probabilistic distillation is also discussed.",5,2,2024
Enhanced Robustness via Loss Engineering in Detuned Non-Hermitian Scattering Systems,"Non-Hermitian optics has revealed a series of counterintuitive phenomena with profound implications for sensing, lasing, and light manipulation. While the non-Hermiticity of Hamitonians is well-recognized, recent advancements in non-Hermitian physics have broadened to include scattering matrices, uncovering phenomena such as simultaneous lasing and coherent perfect absorption (CPA), reflectionless scattering modes (RSMs), and coherent chaos control. Despite these developments, the investigation has predominantly focused on static and symmetric configurations, leaving the dynamic properties of non-Hermitian scattering in detuned systems largely unexplored. Bridging this gap, we extend certain stationary non-Hermitian scattering phenomena to detuned systems. We delve into the interplay between bi-directional RSMs and RSM exceptional points (EPs), and elucidate the global existence conditions for RSMs under detuning. Moreover, we introduces a novel category of EPs, characterized by the coalescence of transmission peaks, emerging independent with the presence of Hamiltonian EPs. The transmission EPs (TEPs) exhibit flat-top lineshape and can be extended to a square-shaped spectrum when detuning is involved, accompanied by a distinctive phase transition. Significantly, we demonstrate the applications of the TEPs in a one-dimensional coupled cavity system, engineered to enhance sensing robustness against environmental instabilities such as laser frequency drifts, which can exceed 10 MHz. This capability represents a substantial improvement over traditional sensing methods and an important improvement of fragile EP sensors. Our findings not only contribute to the broader understanding of non-Hermitian scattering phenomena but also paves the way for future advancements in non-Hermitian sensing technologies.",16,2,2024
Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic Displays,"Recently, deep learning-based computer-generated holography (CGH) has demonstrated tremendous potential in three-dimensional (3D) displays and yielded impressive display quality. However, most existing deep learning-based CGH techniques can only generate holograms of 1080p resolution, which is far from the ultra-high resolution (16K+) required for practical virtual reality (VR) and augmented reality (AR) applications to support a wide field of view and large eye box. One of the major obstacles in current CGH frameworks lies in the limited memory available on consumer-grade GPUs which could not facilitate the generation of higher-definition holograms. To overcome the aforementioned challenge, we proposed a divide-conquer-and-merge strategy to address the memory and computational capacity scarcity in ultra-high-definition CGH generation. This algorithm empowers existing CGH frameworks to synthesize higher-definition holograms at a faster speed while maintaining high-fidelity image display quality. Both simulations and experiments were conducted to demonstrate the capabilities of the proposed framework. By integrating our strategy into HoloNet and CCNNs, we achieved significant reductions in GPU memory usage during the training period by 64.3\% and 12.9\%, respectively. Furthermore, we observed substantial speed improvements in hologram generation, with an acceleration of up to 3$\times$ and 2 $\times$, respectively. Particularly, we successfully trained and inferred 8K definition holograms on an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore, we conducted full-color optical experiments to verify the effectiveness of our method. We believe our strategy can provide a novel approach for memory- and time-efficient holographic displays.",25,2,2024
Electric Vehicles Limit Equitable Access to Essential Services During Blackouts,"Electric vehicles (EVs) link mobility and electric power availability, posing a risk of making transportation unavailable during blackouts. We develop a computational framework to quantify the impact of EVs on mobility and access to services and find that existing access issues are exacerbated by EVs. Our results demonstrate that larger batteries reduce mobility constraints but their effectiveness is dependent on the geographic distribution of services and households. We explore the trade-offs between mobility and quality-of-life improvements presented by Vehicle-to-Grid technologies and the feasibility and trade-offs of public charging infrastructure as a solution to access inequalities. Equitable access to essential services (e.g. supermarkets, schools, parks, etc.) is the most important aspect of community resilience and our results show vehicle electrification can hinder access to essential services unless properly incorporated into policy and city-scale decision-making.",23,2,2024
A simple mathematical theory for Simple Volatile Memristors and their spiking circuits,"In pursuit of neuromorphic (brain-inspired) devices, memristors (memory-resistors) have emerged as promising candidates for emulating neuronal circuitry. Here we mathematically define a class of Simple Volatile Memristors (SVMs), which notably includes various fluidic iontronic devices that have recently garnered significant interest due to their unique quality of operating within the same medium as the brain. We show that symmetric SVMs produce non self-crossing current-voltage hysteresis loops, while simple asymmetric SVMs produce self-crossing loops. Additionally, we derive a general expression for the enclosed area in a loop, providing a relation between the voltage frequency and the SVM memory timescale. These general results are shown to materialise in physical finite-element calculations of microfluidic memristors. An SVM-based circuit has been proposed that exhibits all-or-none and tonic neuronal spiking. We generalise and analyse this spiking circuit, characterising it as a two-dimensional dynamical system. Additionally, we demonstrate that stochastic effects can induce novel neuronal firing modes absent in the deterministic case. Through our analysis, the circuit dynamics are well understood, while retaining its explicit link with the physically plausible underlying system.",26,2,2024
A simple proof of Werner Schulte's conjecture,"Lately, Werner Schulte has conjectured that for all positive $n>1$, $n$ divides $\frac{(n-2)! (n-1)!}{2^{n-3}} + 4$ if and only if $n$ is prime. In this paper, We use elementary methods, to give a simple proof of this conjecture.",22,2,2024
"Implementation of a Low-Cost, Distributed, Absolute Time Reference in an application dedicated to damage detection","Typical structural health monitoring configuration implies sensors and supervisor installations connected by electric cable for communication. As done in other wireless projects, this one aim at reducing installation and maintenance costsby designing a wireless sensor network. One of the problem when designing wireless sensors, is data tagging: an event has to be correctly dated by many sensors. This problem increases when a precise time stamping is required. What distinguishes this project is the implementation of algorithms that allows precise time stamping by the use of a standard protocol and the separation between measurements operations and communication task in order to make modular sensors.",19,2,2024
"RIS Assisted Wireless Networks: Collaborative Regulation, Deployment Mode and Field Testing","In recent years, RIS has made significant progress in engineering application research and industrialization and academic research. However, the engineering application research field of RIS still faces several challenges. This article analyzes and discusses the two deployment modes of RIs-assisted wireless networks: Network Controlled Mode and Standalone mode. It also presents three typical collaboration scenarios of RIS networks, including multi-RIS collaboration, multi-user access, and multi-cell coordination, which reflect the differences between the two deployment modes of RIS. The article proposes collaborative regulation mechanisms for RIS and analyzes their applications in the two network deployment modes in-depth. Furthermore, the article establishes simulation models of three scenarios and provides rich numerical simulation results. An actual field test environment was also built, where a specially designed and processed RIS prototype was used for preliminary field test and verification. Finally, this article puts forward future trends and challenges.",14,2,2024
6AInets: Harnessing artificial intelligence for the 6G network security: Impacts and Challenges,"This decade has witnessed the initiation of the digital revolution, as anticipated with the advent of 5G networks. Looking ahead to the 6G communication era, considerations are being made regarding how individuals will engage with the digital virtual world. The design of 6G technology, which will present enormous opportunities to develop and enhance human potential, will have a major impact on communications in the 2030s. We believe that in 6G we will see an unprecedented transformation that will set it apart from earlier wireless cellular network generations. Specifically, 6G will leverage ubiquitous AI services ranging from the network's core to its end devices, going beyond unpredictable limits. Despite the numerous advantages offered by 6G over existing technologies, there remains a pressing need to address security concerns. For example, the automation of critical processes in the 6G infrastructure will lead to a significantly broader and more intricate attack surface. Thus, the significance of Artificial Intelligence (AI) in providing security aspects within the envisioned 6G paradigm is substantial, but its integration presents a dual-edged dynamic. Therefore, to strengthen and validate the relevance of AI in securing 6G networks, this article elucidates how AI can be strategically used in 6G security, addressing potential challenges, and proposing solutions to enhance its role in securing networks.",7,2,2024
"Integrating Multi -WAN, VPN and IEEE 802.3ad for Advanced IPSEC","Since the emergence of the internet, IPSEC has undergone significant changes due to changes in the type and behavior of users worldwide. IEEE 802.3ad, while considered a key aspect of the IPSEC model, is predictable and can result in potential design flaws, making it relatively easy to access a secure workstation. Thus, it is critical to leverage the benefits of multiple ISPs (multi-WAN) and a link aggregation model and integrate an aspect of randomisation in the network. This facet of the network is highlighted by the proof of concept in the simulation of a double pendulum. The analysis of POC provided a network topology designed to utilize multiple WAN, 802.3ad link aggregation, and other environmental components to create a sense of true randomness within a network system. An analysis of this approach shows that it accounts for the data stream's size, transmission speed, WANs and VPNs' location, and other environmental factors to create a sense of randomness. Based on the proof concept, it can be concluded that attaining randomisation using multi-WAN, VPN, and 802.3ad is a highly effective model for improving IPSEC.",1,2,2024
Revisiting Feature Prediction for Learning Visual Representations from Video,"This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",15,2,2024
The Choquet-like operator with respect to an admissible order as a tool for aggregating multivalued data,"In this paper, we propose a new generalization of the classical discrete Choquet integral to the multivalued framework in terms of an admissible order that refines the natural partial order on the considered value set. The new Choquet-like operator takes as input a finite number of values of a given type, in particular real numbers, intervals, and vectors, and returns a~single output value of the same type as the input values. We give necessary and sufficient conditions for the operator to be monotone with respect to the admissible order. We then provide a complete characterization of the Choquet-like operator as an aggregation function with respect to the admissible order and study its selected special cases.",28,2,2024
Attention-aware Semantic Communications for Collaborative Inference,"We propose a communication-efficient collaborative inference framework in the domain of edge inference, focusing on the efficient use of vision transformer (ViTs) models. The partitioning strategy of conventional collaborative inference fails to reduce communication cost because of the inherent architecture of ViTs maintaining consistent layer dimensions across the entire transformer encoder. Therefore, instead of employing the partitioning strategy, our framework utilizes a lightweight ViT model on the edge device, with the server deploying a complicated ViT model. To enhance communication efficiency and achieve the classification accuracy of the server model, we propose two strategies: 1) attention-aware patch selection and 2) entropy-aware image transmission. Attention-aware patch selection leverages the attention scores generated by the edge device's transformer encoder to identify and select the image patches critical for classification. This strategy enables the edge device to transmit only the essential patches to the server, significantly improving communication efficiency. Entropy-aware image transmission uses min-entropy as a metric to accurately determine whether to depend on the lightweight model on the edge device or to request the inference from the server model. In our framework, the lightweight ViT model on the edge device acts as a semantic encoder, efficiently identifying and selecting the crucial image information required for the classification task. Our experiments demonstrate that the proposed collaborative inference framework can reduce communication overhead by 68% with only a minimal loss in accuracy compared to the server model.",23,2,2024
A Bio-Medical Snake Optimizer System Driven by Logarithmic Surviving Global Search for Optimizing Feature Selection and its application for Disorder Recognition,"It is of paramount importance to enhance medical practices, given how important it is to protect human life. Medical therapy can be accelerated by automating patient prediction using machine learning techniques. To double the efficiency of classifiers, several preprocessing strategies must be adopted for their crucial duty in this field. Feature selection (FS) is one tool that has been used frequently to modify data and enhance classification outcomes by lowering the dimensionality of datasets. Excluded features are those that have a poor correlation coefficient with the label class, that is, they have no meaningful correlation with classification and do not indicate where the instance belongs. Along with the recurring features, which show a strong association with the remainder of the features. Contrarily, the model being produced during training is harmed, and the classifier is misled by their presence. This causes overfitting and increases algorithm complexity and processing time. These are used in exploration to allow solutions to be found more thoroughly and in relation to a chosen solution than at random. TLSO, PLSO, and LLSO stand for Tournament Logarithmic Snake Optimizer, Proportional Logarithmic Snake Optimizer, and Linear Order Logarithmic Snake Optimizer, respectively. A number of 22 reference medical datasets were used in experiments. The findings indicate that, among 86 % of the datasets, TLSO attained the best accuracy, and among 82 % of the datasets, the best feature reduction. In terms of the standard deviation, the TLSO also attained noteworthy reliability and stability. On the basis of running duration, it is, nonetheless, quite effective.",22,2,2024
Computation Offloading for Multi-server Multi-access Edge Vehicular Networks: A DDQN-based Method,"In this paper, we investigate a multi-user offloading problem in the overlapping domain of a multi-server mobile edge computing system. We divide the original problem into two stages: the offloading decision making stage and the request scheduling stage. To prevent the terminal from going out of service area during offloading, we consider the mobility parameter of the terminal according to the human behaviour model when making the offloading decision, and then introduce a server evaluation mechanism based on both the mobility parameter and the server load to select the optimal offloading server. In order to fully utilise the server resources, we design a double deep Q-network (DDQN)-based reward evaluation algorithm that considers the priority of tasks when scheduling offload requests. Finally, numerical simulations are conducted to verify that our proposed method outperforms traditional mathematical computation methods as well as the DQN algorithm.",21,2,2024
Evolving Genetic Programming Tree Models for Predicting the Mechanical Properties of Green Fibers for Better Biocomposite Materials,"Advanced modern technology and industrial sustainability theme have contributed implementing composite materials for various industrial applications. Green composites are among the desired alternatives for the green products. However, to properly control the performance of the green composites, predicting their constituents properties are of paramount importance. This work presents an innovative evolving genetic programming tree models for predicting the mechanical properties of natural fibers based upon several inherent chemical and physical properties. Cellulose, hemicellulose, lignin and moisture contents as well as the Microfibrillar angle of various natural fibers were considered to establish the prediction models. A one-hold-out methodology was applied for training/testing phases. Robust models were developed to predict the tensile strength, Young's modulus, and the elongation at break properties of the natural fibers. It was revealed that Microfibrillar angle was dominant and capable of determining the ultimate tensile strength of the natural fibers by 44.7% comparable to other considered properties, while the impact of cellulose content in the model was only 35.6%. This in order would facilitate utilizing artificial intelligence in predicting the overall mechanical properties of natural fibers without experimental efforts and cost to enhance developing better green composite materials for various industrial applications.",20,2,2024
Hybrid Training of Denoising Networks to Improve the Texture Acutance of Digital Cameras,"In order to evaluate the capacity of a camera to render textures properly, the standard practice, used by classical scoring protocols, is to compute the frequential response to a dead leaves image target, from which is built a texture acutance metric. In this work, we propose a mixed training procedure for image restoration neural networks, relying on both natural and synthetic images, that yields a strong improvement of this acutance metric without impairing fidelity terms. The feasibility of the approach is demonstrated both on the denoising of RGB images and the full development of RAW images, opening the path to a systematic improvement of the texture acutance of real imaging devices.",20,2,2024
A real-time Artificial Intelligence system for learning Sign Language,"A primary challenge for the deaf and hearing-impaired community stems from the communication gap with the hearing society, which can greatly impact their daily lives and result in social exclusion. To foster inclusivity in society, our endeavor focuses on developing a cost-effective, resource-efficient, and open technology based on Artificial Intelligence, designed to assist people in learning and using Sign Language for communication. The analysis presented in this research paper intends to enrich the recent academic scientific literature on Sign Language solutions based on Artificial Intelligence, with a particular focus on American Sign Language (ASL). This research has yielded promising preliminary results and serves as a basis for further development.",19,2,2024
Sampling recovery on function classes with a structural condition,"Sampling recovery on some function classes is studied in this paper. Typically, function classes are defined by imposing smoothness conditions. It was understood in nonlinear approximation that structural conditions in the form of control of the number of big coefficients of an expansion of a function with respect to a given system of functions plays an important role. Sampling recovery on smoothness classes is an area of active research, some problems, especially in the case of mixed smoothness classes, are still open. It was discovered recently that universal sampling discretization and nonlinear sparse approximations are useful in the sampling recovery problem. This motivated us to systematically study sampling recovery on function classes with a structural condition. Some results in this direction are already known. In particular, the classes defined by conditions on coefficients with indices from the domains, which are differences of two dyadic cubes are studied in the recent author's papers. In this paper we concentrate on studying function classes defined by conditions on coefficients with indices from the domains, which are differences of two dyadic hyperbolic crosses.",17,2,2024
Deep Reinforcement Learning Based Toolpath Generation for Thermal Uniformity in Laser Powder Bed Fusion Process,"Laser powder bed fusion (LPBF) is a widely used metal additive manufacturing technology. However, the accumulation of internal residual stress during printing can cause significant distortion and potential failure. Although various scan patterns have been studied to reduce possible accumulated stress, such as zigzag scanning vectors with changing directions or a chessboard-based scan pattern with divided small islands, most conventional scan patterns cannot significantly reduce residual stress. The proposed adaptive toolpath generation (ATG) algorithms, aiming to minimize the thermal gradients, may result in extremely accumulated temperature fields in some cases. To address these issues, we developed a deep reinforcement learning (DRL)-based toolpath generation framework, with the goal of achieving uniformly distributed heat and avoiding extremely thermal accumulation regions during the LPBF process. We first developed an overall pipeline for the DRL-based toolpath generation framework, which includes uniformly sampling, agent moving and environment observation, action selection, moving constraints, rewards calculation, and the training process. To accelerate the training process, we simplified the data-intensive numerical model by considering the turning angles on the toolpath. We designed the action spaces with three options, including the minimum temperature value, the smoothest path, and the second smoothest path. The reward function was designed to minimize energy density to ensure the temperature field remains relatively stable. To verify the effectiveness of the proposed DRL-based toolpath generation framework, we performed numerical simulations of polygon shape printing domains. In addition, four groups of thin plate samples with different scan patterns were compared using the LPBF process.",17,2,2024
Uncertainty-guided annotation enhances segmentation with the human-in-the-loop,"Deep learning algorithms, often critiqued for their 'black box' nature, traditionally fall short in providing the necessary transparency for trusted clinical use. This challenge is particularly evident when such models are deployed in local hospitals, encountering out-of-domain distributions due to varying imaging techniques and patient-specific pathologies. Yet, this limitation offers a unique avenue for continual learning. The Uncertainty-Guided Annotation (UGA) framework introduces a human-in-the-loop approach, enabling AI to convey its uncertainties to clinicians, effectively acting as an automated quality control mechanism. UGA eases this interaction by quantifying uncertainty at the pixel level, thereby revealing the model's limitations and opening the door for clinician-guided corrections. We evaluated UGA on the Camelyon dataset for lymph node metastasis segmentation which revealed that UGA improved the Dice coefficient (DC), from 0.66 to 0.76 by adding 5 patches, and further to 0.84 with 10 patches. To foster broader application and community contribution, we have made our code accessible at",16,2,2024
"Evolving Military Broadband Wireless Communication Systems: WiMAX, LTE and WLAN","Emerging technologies for mobile broadband wireless are being considered as a Commercial Off-The-Shelf solution to cover the operational requirements of the future warfare. The capabilities of these technologies are being enhanced to meet the growing market demands on performance. In this context, several standards such as WiMAX, LTE or WLAN are introducing themselves as strong candidates to fulfill these requirements. This paper presents an innovative scenario-based approach to develop a Military Broadband Wireless Communication System (MBWCS). Its main objective is to analyze how similar a military MBWCS can be to the identified civil standards, taking operational and high level technical requirements into account. This specification will be used for analyzing the applicability and the modifications of each of the standards layers individually. Proving the feasibility and aptitude of each standard provides strong foundations to address a MBWCS in the most efficient way.",7,2,2024
Mapping the Mind-Brain Duality to a Digital-Analog Perceptual Duality,"Could the abstract ideas of our minds originate from neuronal interactions within our brains? To address this question, we examine interactions within 'brain-world' (BW) and 'brain-brain' (BB) domains, which represent the brain's physical interactions with its environment and the mental interactions between brains, respectively. BW interactions are characterized as analog - dynamic and continuous, whereas BB interactions are digital - non-dynamic and discrete. This distinction allows BB interactions to facilitate effective, albeit information-limited, communication through categorization. We review existing data showing that cascades of neural circuits can convert between analog and digital signals, thereby linking physical and mental processes. Importantly, we argue that these circuits cannot fully reduce one domain to the other, suggesting that the mind-brain duality can be mapped to the BB-BW duality. Such mapping suggests that the mind's foundation is inherently social, offering a novel explanation for the physical-mental gap while acknowledging the coexistence of the physical body and the non-physical mind.",20,2,2024
Canonical Temperature Control by Molecular Dynamics,"""Pedagogical derivations for Nosé's dynamics can be developed in two different ways, (i) by starting with a temperature-dependent Hamiltonian in which the variable $s$ scales the time or the mass, or (ii) by requiring that the equations of motion generate the canonical distribution including a Gaussian distribution in the friction coefficient $\zeta$. Nosé's papers follow the former approach. Because the latter approach is not only constructive and simple, but also can be generalized to other forms of the equations of motion, we illustrate it here. We begin by considering the probability density $f(q,p,\zeta)$ in an extended phase space which includes $\zeta$ as well as all pairs of phase variables $q$ and $p$. This density $f(q,p,\zeta)$ satisfies the conservation of probability (Liouville's Continuity Equation)"" $$(\partial f/\partial t) + \sum (\partial (\dot q f)/\partial q) + \sum (\partial (\dot p f)/\partial p) + \sum (\partial (\dot \zeta f)/\partial \zeta) = 0 \ . $$ The multi-authored ``review''\cite{b1} motivated our quoting the history of Nosé and Nosé-Hoover mechanics, aptly described on page 31 of Bill's 1986 {\it Molecular Dynamics} book, reproduced above\cite{b2}.",14,2,2024
A model for membrane degradation using a gelatin invadopodia assay,"One of the most crucial and lethal characteristics of solid tumors is represented by the increased ability of cancer cells to migrate and invade other organs during the so-called metastatic spread. This is allowed thanks to the production of matrix metalloproteinases (MMPs), enzymes capable of degrading a type of collagen abundant in the basal membrane separating the epithelial tissue from the connective one. In this work, we employ a synergistic experimental and mathematical modelling approach to explore the invasion process of tumor cells. A athematical model composed of reaction-diffusion equations describing the evolution of the tumor cells density on a gelatin substrate, MMPs enzymes concentration and the degradation of the gelatin is proposed. This is completed with a calibration strategy. We perform a sensitivity analysis and explore a parameter estimation technique both on synthetic and experimental data in order to find the optimal parameters that describe the in vitro experiments. A comparison between numerical and experimental solutions ends the work.",8,2,2024
A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles,"A quarter of US adults regularly get their news from YouTube. Yet, despite the massive political content available on the platform, to date no classifier has been proposed to identify the political leaning of YouTube videos. To fill this gap, we propose a novel classifier based on Bert -- a language model from Google -- to classify YouTube videos merely based on their titles into six categories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We used a public dataset of 10 million YouTube video titles (under various categories) to train and validate the proposed classifier. We compare the classifier against several alternatives that we trained on the same dataset, revealing that our classifier achieves the highest accuracy (75%) and the highest F1 score (77%). To further validate the classification performance, we collect videos from YouTube channels of numerous prominent news agencies, such as Fox News and New York Times, which have widely known political leanings, and apply our classifier to their video titles. For the vast majority of cases, the predicted political leaning matches that of the news agency.",16,2,2024
Synthetic 33-Bus Microgrid: Dynamic Model and Time-Series Parameters,"This report provides the detailed description of the synthetic 33-bus microgrid (MG), including its structure, dynamic models, and time-series parameters of loads and generations. The network structure is adapted from the IEEE 33-bus distribution network, with additional converter-interfaced renewable energy resources and energy storage systems. Time-series parameters is generated based on the open-source ARPA-E PERFORM datasets.",15,2,2024
Neural Information Organizing and Processing -- Neural Machines,"The informational synthesis of neural structures, processes, parameters and characteristics that allow a unified description and modeling as neural machines of natural and artificial neural systems is presented. The general informational parameters as the global quantitative measure of the neural systems computing potential as absolute and relative neural power were proposed. Neural information organizing and processing follows the way in which nature manages neural information by developing functions, functionalities and circuits related to different internal or peripheral components and also to the whole system through a non-deterministic memorization, fragmentation and aggregation of afferent and efferent information, deep neural information processing representing multiple alternations of fragmentation and aggregation stages. The relevant neural characteristics were integrated into a neural machine type model that incorporates unitary also peripheral or interface components as the central ones. The proposed approach allows overcoming the technical constraints in artificial computational implementations of neural information processes and also provides a more relevant description of natural ones.",15,2,2024
A Note on the Ideals E^a Which Are Discrete,"This note looks at two mistakes in the paper ""Some characterizations of the ideals Ea which are discrete"" by Khabaoui et al., published in ""Rend. Circ. Mat. Palermo, II. Ser 72, 1325-1336 (2023)"".",22,2,2024
4D Track Reconstruction on Free-Streaming Data at PANDA at FAIR,"A new generation of experiments is being developed, where the challenge of separating rare signal processes from background at high intensities requires a change of trigger paradigm. At the future PANDA experiment at FAIR, hardware triggers will be abandoned and instead a purely software-based system will be used. This requires novel reconstruction methods with the ability to process data from many events simultaneously.A 4D tracking algorithm based on the cellular automaton has been developed which will utilize the timing information from detector signals. Simulation studies have been performed to test its performance on the foreseen free-streaming data from the PANDA detector. For this purpose, a quality assurance procedure for tracking on free-streaming data was implemented in the PANDA software. The studies show that at higher interaction rates, 4D tracking performs better than the 3D algorithm in terms of efficiency, 84% compared to 77%. The fake track suppression is also greatly improved, compared to the 3D tracking with roughly a 50% decrease in the ghost rate.",19,2,2024
Exploring the Frontiers: Challenges and Theories Beyond the Standard Model,"Quantum Field Theory (QFT) forms the bedrock of the Standard Model (SM) of particle physics, a powerful framework that delineates the fundamental constituents and interactions of the universe. However, the SM's narrative is incomplete, as it conspicuously fails to account for several empirical phenomena that challenge our current understanding of particle physics. This review meticulously examines three paramount anomalies that elude SM predictions: the elusive nature of dark matter, the Higgs boson's anomalously low mass, and the intricate puzzle of neutrino masses. Through a critical analysis, it delves into the forefront of theoretical advancements proposed to bridge these gaps, notably the Weakly Interacting Massive Particles (WIMPs), Supersymmetry (SUSY), and the intriguing hypothesis of right-handed neutrinos. By synthesizing current research and theoretical models, this review not only elucidates these profound mysteries but also underscores the imperative for a more comprehensive and unified theory of particle physics, setting the stage for future discoveries and theoretical breakthroughs.",16,2,2024
Serial Parallel Reliability Redundancy Allocation Optimization for Energy Efficient and Fault Tolerant Cloud Computing,"Serial-parallel redundancy is a reliable way to ensure service and systems will be available in cloud computing. That method involves making copies of the same system or program, with only one remaining active. When an error occurs, the inactive copy can step in as a backup right away, this provides continuous performance and uninterrupted operation. This approach is called parallel redundancy, otherwise known as active-active redundancy, and its exceptional when it comes to strategy. It creates duplicates of a system or service that are all running at once. By doing this fault tolerance increases since if one copy fails, the workload can be distributed across any replica thats functioning properly. Reliability allocation depends on features in a system and the availability and fault tolerance you want from it. Serial redundancy or parallel redundancies can be applied to increase the dependability of systems and services. To demonstrate how well this concept works, we looked into fixed serial parallel reliability redundancy allocation issues followed by using an innovative hybrid optimization technique to find the best possible allocation for peak dependability. We then measured our findings against other research.",16,2,2024
Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips,"Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer"", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \url{this https URL}.",15,2,2024
X-lifecycle Learning for Cloud Incident Management using LLMs,"Incident management for large cloud services is a complex and tedious process and requires significant amount of manual efforts from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root causing and mitigating of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) created opportunities to automatically generate contextual recommendations to the OCEs assisting them to quickly identify and mitigate critical issues. However, existing research typically takes a silo-ed view for solving a certain task in incident management by leveraging data from a single stage of SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying ontology of service monitors used for automatically detecting incidents. By leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over State-of-The-Art methods.",15,2,2024
Machine Learning Driven Global Optimisation Framework for Analog Circuit Design,"We propose a machine learning-driven optimisation framework for analog circuit design in this paper. The primary objective is to determine the device sizes for the optimal performance of analog circuits for a given set of specifications. Our methodology entails employing machine learning models and spice simulations to direct the optimisation algorithm towards achieving the optimal design for analog circuits. Machine learning based global offline surrogate models, with the circuit design parameters as the input, are built in the design space for the analog circuits under study and is used to guide the optimisation algorithm, resulting in faster convergence and a reduced number of spice simulations. Multi-layer perceptron and random forest regressors are employed to predict the required design specifications of the analog circuit. Since the saturation condition of transistors is vital in the proper working of analog circuits, multi-layer perceptron classifiers are used to predict the saturation condition of each transistor in the circuit. The feasibility of the candidate solutions is verified using machine learning models before invoking spice simulations. We validate the proposed framework using three circuit topologies--a bandgap reference, a folded cascode operational amplifier, and a two-stage operational amplifier. The simulation results show better optimum values and lower standard deviations for fitness functions after convergence. Incorporating the machine learning-based predictions proposed in the optimisation method has resulted in the reduction of spice calls by 56%, 59%, and 83% when compared with standard approaches in the three test cases considered in the study.",27,2,2024
Hierarchy Selection: New team ranking indicators for cyclist multi-stage races,"In this paper, I report some investigation discussing team selection, whence hierarchy, through ranking indicators, for example when measuring professional cyclist team's sportive value, in particular in multistage races. A logical, it seems, constraint is introduced on the riders: they must finish the race. Several new indicators are defined, justified, and compared. These indicators are mainly based on the arriving place of (the best 3) riders instead of their time needed for finishing the stage or the race, - as presently classically used. A case study, serving as an illustration containing the necessary ingredients for a wider discussion, is the 2023 Vuelta de San Juan, but without loss of generality.It is shown that the new indicators offer some new viewpoint for distinguishing the ranking through the cumulative sums of the places of riders rather than their finishing times. On the other hand, the indicators indicate a different team hierarchy if only the finishing riders are considered. Some consideration on the distance between ranking indicators is presented.Moreover, it is argued that these new ranking indicators should hopefully promote more competitive races, not only till the end of the race, but also until the end of each stage. Generalizations and other applications within operational research topics, like in academia, are suggested.",22,2,2024
A Comprehensive System for Secondary Structure Analysis of Protein Models,"In protein structure analysis, the accurate characterization of secondary structure elements is crucial for understanding protein function and dynamics. This paper presents a software system designed for the comprehensive analysis of the secondary structure of protein models. Leveraging phi ({\phi}) and psi ({\psi}) torsion angles, the system utilises K-means clustering and outlier detection techniques to identify and classify folding structures within protein models. Through the visualisation of the Ramachandran plot, the software enables the differentiation of various secondary structure motifs, including alpha-helices, beta-sheets, and other structural elements. The incorporation of customisable threshold values facilitates the identification of outliers, providing insights into potential structural anomalies or misalignments within protein models. Overall, this software system offers researchers a powerful tool for comprehensive secondary structure analysis, to aid in enhancing understanding of protein structures created using both traditional methods such as X-Ray Diffraction and contemporary methods such as Artificial Intelligence.",18,2,2024
FastqZip: An Improved Reference-Based Genome Sequence Lossy Compression Framework,"Storing and archiving data produced by next-generation sequencing (NGS) is a huge burden for research institutions. Reference-based compression algorithms are effective in dealing with these data. Our work focuses on compressing FASTQ format files with an improved reference-based compression algorithm to achieve a higher compression ratio than other state-of-the-art algorithms. We propose FastqZip, which uses a new method mapping the sequence to reference for compression, allows reads-reordering and lossy quality scores, and the BSC or ZPAQ algorithm to perform final lossless compression for a higher compression ratio and relatively fast speed. Our method ensures the sequence can be losslessly reconstructed while allowing lossless or lossy compression for the quality scores. We reordered the reads to get a higher compression ratio. We evaluate our algorithms on five datasets and show that FastqZip can outperform the SOTA algorithm Genozip by around 10% in terms of compression ratio while having an acceptable slowdown.",22,2,2024
LoS Sensing-based Channel Estimation in UAV-Assisted OFDM Systems,"In unmanned aerial vehicle (UAV)-assisted orthogonal frequency division multiplexing (OFDM) systems, the potential advantage of the line-of-sight (LoS) path, characterized by its high probability of existence, has not been fully harnessed, thereby impeding the improvement of channel estimation (CE) accuracy. Inspired by the ideas of integrated sensing and communication (ISAC), this letter develops a LoS sensing method aimed at detecting the presence of LoS path. Leveraging the prior information obtained from LoS path detection, the detection thresholds for resolvable paths are proposed for LoS and Non-LoS (NLoS) scenarios, respectively. By employing these specifically designed detection thresholds, denoising processing is applied to classical least square (LS) CE, thereby improving the CE accuracy. Simulation results validate the effectiveness of the proposed method in enhancing CE accuracy and demonstrate its robustness against parameter variations.",22,2,2024
Virtual Sectorization to Enable Hybrid Beamforming in mm-Wave mMIMO,"Hybrid beamforming (HBF) is a key technology to enable mm-wave Massive multiple-input multiple-output (mMIMO) receivers for future-generation wireless communications. It combines beamforming in both analog (via phase shifters) and digital domains, resulting in low power consumption and high spectral efficiency. In practice, the problem of joint beamforming in multi-user scenarios is still open because an analog beam can't cover all users simultaneously. In this paper, we propose a hierarchical approach to divide users into clusters. Each cluster consists of users inside a virtual sector produced by the analog beamforming of an HBF-based mMIMO receiver. Thus, inside each sector, a lower-cost digital beamforming serves a limited number of users within the same cluster. Simulations with realistic non-line-of-sight scenarios generated by the QuaDRiGa 2.0 demonstrate that our methods outperform standard FFT-based alternatives and almost achieve SVD-based beamspace performance bound.",16,2,2024
Trainable Least Squares to Reduce PAPR in OFDM-based Hybrid Beamforming Systems,"In this paper, we propose a trainable least squares (LS) approach for reducing the peak-to-average power ratio (PAPR) of orthogonal frequency division multiplexing (OFDM) signals in a hybrid beamforming (HBF) system. Compared to digital beamforming (DBF), in HBF technology the number of antennas exceeds the number of digital ports. Therefore, PAPR reduction capabilities are restricted by both a limited bandwidth and the reduced size of digital space. The problem is to meet both conditions. Moreover, the major HBF advantage is a reduced system complexity, thus the complexity of the PAPR reduction algorithm is expected to be low. To justify the performance of the proposed trainable LS, we provide a performance bound achieved by convex optimization using the CVX Matlab package. Moreover, the complexity of the proposed algorithm can be comparable to the minimal complexity of the digital ``twin'' calculation in HBF. The abovementioned features prove the feasibility of the trained LS for PAPR reduction in fully-connected HBF.",16,2,2024
Fairness-aware Age-of-Information Minimization in WPT-Assisted Short-Packet THz Communications for mURLLC,"The technological landscape is swiftly advancing towards large-scale systems, creating significant opportunities, particularly in the domain of Terahertz (THz) communications. Networks designed for massive connectivity, comprising numerous Internet of Things (IoT) devices, are at the forefront of this advancement. In this paper, we consider Wireless Power Transfer (WPT)-enabled networks that support these IoT devices with massive Ultra-Reliable and Low-Latency Communication (mURLLC) services.The focus of such networks is information freshness, with the Age-of-Information (AoI) serving as the pivotal performance metric. In particular, we aim to minimize the maximum AoI among IoT devices by optimizing the scheduling policy. Our analytical findings establish the convexity property of the problem, which can be solved efficiently. Furthermore, we introduce the concept of AoI-oriented cluster capacity, examining the relationship between the number of supported devices and the AoI performance in the network. Numerical simulations validate the advantage of our proposed approach in enhancing AoI performance, indicating its potential to guide the design of future THz communication systems for IoT applications requiring mURLLC services.",15,2,2024
"An English Translation of Gröbli's Ph.D. Dissertation: ""Specielle Probleme über die Bewegung geradliniger paralleler Wirbelfäden""","Here we provide a complete English translation of Walter Gröbli's 1877 Ph.D. Thesis, together with some notes on the process. The work considers the dynamics of point vortices in a two-dimensional inviscid incompressible fluid and derives a number of exact solutions in the cases of three, four and $2n$ vortices with certain restriction on the vortices' circulation and the symmetry of the initial configuration.",20,2,2024
Negative-energy and tachyonic solutions in the Weinberg-Tucker-Hammer equation for spin 1,"We considered Weinberg-like equations in the article [1] in order to construct the Feynman-Dyson propagator for the spin-1 particles. This construction is based on the concept of the Weinberg field as a system of four field functions differing by parity and by dual transformations. We also analyzed the recent controversy in the definitions of the Feynman-Dyson propagator for the field operator containing the S=1/2 self/anti-self charge conjugate states in the papers by D. Ahluwalia et al and by W. Rodrigues Jr. et al. The solution to this mathematical controversy is obvious. I proposed the necessary doubling of the Fock Space (as in the Barut and Ziino works), thus extending the corresponding Clifford Algebra. Meanwhile, the N. Debergh et al article considered our old ideas of doubling the Dirac equation, and other forms of T- and PT-conjugation [5]. Both algebraic equation Det (\hat p - m) =0 and Det (\hat p + m) =0 for u- and v- 4-spinors have solutions with p_0= \pm E_p =\pm \sqrt{{\bf p}^2 +m^2}. The same is true for higher-spin equations (or they may even have more complicated dispersion relations). Meanwhile, every book considers the equality p_0=E_p for both u- and v- spinors of the (1/2,0)\oplus (0,1/2)) representation only, thus applying the Dirac-Feynman-Stueckelberg procedure for elimination of negative-energy solutions. It seems, that it is imposible to consider the relativistic quantum mechanics appropriately without negative energies, tachyons and appropriate forms of the discrete symmetries.",14,2,2024
"A Critique of Chen's ""The 2-MAXSAT Problem Can Be Solved in Polynomial Time""","In this paper, we examine Yangjun Chen's technical report titled ``The 2-MAXSAT Problem Can Be Solved in Polynomial Time'' [Che23], which revises and expands upon their conference paper of the same name [Che22]. Chen's paper purports to build a polynomial-time algorithm for the ${\rm NP}$-complete problem 2-MAXSAT by converting a 2-CNF formula into a graph that is then searched. We show through multiple counterexamples that Chen's proposed algorithms contain flaws, and we find that the structures they create lack properly formalized definitions. Furthermore, we elaborate on how the author fails to prove the correctness of their algorithms and how they make overgeneralizations in their time analysis of their proposed solution. Due to these issues, we conclude that Chen's technical report [Che23] and conference paper [Che22] both fail to provide a proof that ${\rm P}={\rm NP}$.",22,2,2024
Introduction: Swarm-based gradient descent for non convex optimization,"The field of optimization has the goal to find an optimal solution to a target function, i.e. to minimize (or maximize) the target function. When trying to find such a global minimum, one often encounters local minima due to unfavorable procedures and starting regions. The swarm-based gradient descent method of Prof. Eitan Tadmor offers an alternative method for solving global minimization problems. By using a swarm of agents, local minima will not be taken account and the global minimum will be found. Furthermore leads the communication between the agent to a further expansion of the search region. Under the supervision of Prof. Angela Kunoth, I give an introduction to this swarm-based method in my bachelor thesis. Therefore I used my own program in Julia to give a more visual understanding of how the new method works and which influence certain parameters such as the number of agents or ""q"" have.",21,2,2024
A contribution to the theory of $σ$-properties of a finite group,"We characterize some classes of finite soluble groups. In particular, we prove that: a finite group $G$ is supersoluble if and only if $G$ has a normal subgroup $D$ such that $G/D$ is supersoluble and $D$ avoids every chief factor of $G$ between $V^{G}$ and $V_{G}$ for every maximal subgroup $V$ of the generalized Fitting subgroup $F^{*}(G)$ of $G$; a finite soluble group $G$ is a $PST$-group (that is, Sylow permutability is a transitive relation on $G$) if and only if $G$ has a normal subgroup $D$ such that $G/D$ is nilpotent and $D$ avoids every chief factor of $G$ between $V^{G}$ and $V_{G}$ for every subnormal subgroup $A$ of $G$.",18,2,2024
Algorithms for constrained optimal transport,"We derive iterative scaling algorithms of the Sinkhorn-Knopp (SK) type for constrained optimal transport. The constraints are in the form of prior-imposed zeroes in the transport plan. Based on classical Bregman arguments, we prove asymptotic convergence of our algorithms to a unique optimal solution. New insights obtained from the convergence proof are highlighted. An example from electrical vehicle charging in a smart city context is outlined, in which the prior zero-constraints prevent energy from being transported from some providers to some vehicles.",16,2,2024
The Minimum $L_2$-Distance Projection onto the Canonical Simplex: A Simple Algorithm,"We consider the minimum distance projection in the $L_2$-norm from an arbitrary point in an $n$-dimensional, Euclidian space onto the canonical simplex. It is shown that this problem reduces to a univariate problem that can be solved by a simple algorithm. This optimization problem occurs in the setting of credit risk, where one has stochastic matrices that describe transition probabilities between different credit ratings, and one wants to determine the roots of these matrices, or close approximations to them.",7,2,2024
Uso de herramientas digitales matemáticas en la Educación Secundaria,"Information and Community Technologies (ICT) are very present in our society nowadays and particularly in the educative field. In just two decades, we have passed from a learning based, in many cases, on the master lessons to one such that methodologies like the flipped classroom or the gamification are stronger than ever. Along this work, we have done a study to teachers and students with the main objective to compare the knowledge on digital tools, their use and their acceptation. We use WxMaxima and Geogebra in order to solve an exercise of \textit{Evaluación de Bachillerato para el Acceso a la Universidad} (EBAU) related with Geometry, comparing their ins and outs with the manual solution. Finally, we expose some conclusions and some possible research lines about digital tools, as well as a proposition of an introductory course on WxMaxima and Geogebra in order to teach the teachers.",3,2,2024
"Corrigendum of ""Construction of Kuranishi structures on the moduli spaces of pseudo holomorphic disks I, Surveys in Differential Geometry XXII (2018), 133-190""","This is a corrigendum of Lemma 9.1 of the paper [FOOO3] in the title. This lemma is not correct as pointed out by A. Daemi and a referee of the paper [DF]. The corrigendum does not affect the applications of this lemma in [FOOO3] and other papers and exactly the same proofs as therein apply if one replaces the statement of [FOOO3,Lemma 9.1] by Lemma 2 of the present note.",28,2,2024
Drude's lesser known error of a factor of two and Lorentz's correction,"As is well known, Paul Drude put forward the very first quantitative theory of electrical conduction in metals in 1900. He could successfully account for the Wiedemann-Franz law which states that the ratio of thermal to electrical conductivity divided by temperature is a constant called the Lorenz number. As it turns out, in Drude's derivation, there is a lucky cancellation of two errors. Drude's under-estimate (by an order of 100) of the value of square of the average electron velocity compensated his over-estimate of the electronic heat capacity (by the same order of 100). This compensation or cancellation of two errors lead to a value of the Lorenz number very close to its experimental value. This is well known. There is another error of a factor of two which Drude made when he calculated two different relaxation times for heat conductivity and electrical conductivity. In this article we highlight how and why this error occurred in Drude's derivation and how it was removed 5 years later (that is in 1905) by Hendrik Lorentz when he used the Boltzmann equation and a single relaxation time. This article is of pedagogical value and may be useful to undergraduate/graduate students learning solid state physics.",27,2,2024
On Monadic Vector-Valued Integration,"In recent times, there has been a growing interest in a structuralist understanding of probability, measure and integration theory. The present thesis contributes to this programme in three ways. First, we construct a commutative probability monad on the cartesian closed category of hk-spaces (also known as CGWH spaces, or weak Hausdorff k-spaces in the literature). Secondly, in order to achieve this in a seamless way, we develop the theory of paired linear hk-spaces, a functional-analytic category tailored to the duality between measures and functionals. Finally, vector-valued integration emerges naturally from the free-forgetful adjunction between paired linear hk-spaces and hk-spaces, inducing a commutative monad of compactly supported measures and leading to a theory of monadic vector-valued integration.",26,2,2024
Geometric Illumination of Implicit Surfaces,"Illumination of scenes is usually generated in computer graphics using polygonal meshes. In this paper, we present a geometric method using projections. Starting from an implicit polynomial equation of a surface in 3-D or a curve in 2-D, we provide a semi-algebraic representation of each part of the construction. To solve polynomial condition systems and find constrained regions, we apply algebraic computational algorithms for computing the Gr{\"" o}bner basis and cylindrical algebraic decomposition. The final selection of illuminated and self-shaded components for polynomial surfaces of a degree higher than three is discussed. The text is accompanied by visualizations of illumination of surfaces up to degree eight.",22,2,2024
Disentangling mappings defined on ICIS,"Let $(X,S)$ be an isolated complete intersection singularity of dimension $n$, and let $f:(X,S)\rightarrow (\mathbb{C}^{n+1},0)$ be a germ of $\mathscr{A}$-finite mapping. In this master's degree final project, our main contribution is that we show the case $n=2$ of the general Mond conjecture, which states that $\mu_I(X,f)\geq \text{codim}_{\mathscr{A}_e}(X,f)$, with equality provided $(X,f)$ is weighted homogeneous. Before this project, the only known case for which the conjecture was known to hold is in the case that $n=1$ and $(X,S)$ is a plane curve.",21,2,2024
Bent functions construction using extended Maiorana-McFarland's class,"In a particular case, we consider the extended Maiorana-McFarland class to obtain bent functions with balance in its images, restricted to vectors with even Hamming weight. Additionally, we see that all bent functions are balanced when we restrict to vectors of even Hamming weight or to vectors with odd Hamming weight. Given the necessary tools, we provide a simpler algorithm to obtain new bent functions using Maiorana-McFarland.",20,2,2024
"A review of Alfred North Whitehead's ""Introduction to Mathematics""","In 1911, Alfred North Whitehead published a short book ""Introduction to Mathematics"" (IM) intended for students wanting an explanation of the fundamental ideas of mathematics. Whitehead's IM has enduring value because it was written not long after he and Bertrand Russell published their monumental three-volume work ""Principia Mathematica"" (PM) -- a publication of immense historical significance for mathematics. IM sheds light on Whitehead's view of mathematics at that time. Whitehead's book places proofs in predicate logic as the mythical starting point of mathematics, although Whitehead himself was slow to understand the significance of symbolic predicate logic.",19,2,2024
Quantum Properties of Mathematical Physics Equations. Generation of Quantum Structures,"It is shown with the help of skew-symmetric forms that the mathematical physics equations, on which no additional conditions are imposed, have quantum properties. And this is due to the integrability properties of differential equations, which depends on the consistency of derivatives with respect to different variables and the consistency of equations, if the mathematical physics equations are a system of equations. It was found that such equations on the original tangent space turn out to be non-integrable. Their derivatives do not form a differential. The integrability of such equations is realized only on the structures of a cotangent integrable manifold. This happens using a degenerate, non-differential-preserving transformation that has quantum properties. When implementing degenerate transformations, mini structures (quanta) arise, from which integrable structures are formed. Such properties of integrability of mathematical physics equations and features of degenerate transformations reveal the quantum properties of mathematical physics equations and their ability to generate quantum structures.",19,2,2024
$h$-Amalgamation bases in the Class of non trivial abelian groups,In this paper we give a complete description of the $h$-amalgamation bases in the class of non trivial abelian groups.,19,2,2024
Rotational hypersufaces in $E^4_1$ with generalized $L_k$ 1-type Gauss map,"In this paper, we study the Gauss map of rotational hypersurfaces in 4-dimensional Lorentz-Minkowski space concerning the linear second order differential operators $L_1$ and $L_2$, where $L_1$ is usually called as the Cheng-Yau operator. We obtain some classifications of rotational hypersurfaces which have $L_k$-harmonic Gauss map, $L_k$-pointwise 1-type Gauss map and generalized $L_k$ 1-type Gauss map, where $k = 1,2$.",17,2,2024
The altitudes of a triangle,"A long-standing, unanswered question regarding Euclid's Elements concerns the absence of a theorem for the concurrence of the altitudes of a triangle, and the possible reasons for this omission. In the centuries following Euclid, a remarkable number of proofs have been put forward; this suggests a search for the most elementary and direct proof. This paper provides a simple, direct, elementary proof of the theorem; it is based solely on the Elements.",15,2,2024
Analyzing the Roles of Language and Vision in Learning from Limited Data,"Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoning.",15,2,2024
A new approach to the similarity problem,"We say that a $C^*$-algebra $\mathcal{A}$ satisfies the similarity property ((SP)) if every bounded homomorphism $u\colon \mathcal{A} \to \mathcal{B}(\mathit{H})$, where $\mathit{H}$ is a Hilbert space, is similar to a $*$-homomorphism. We introduce the following hypothesis (EP). (EP): Every separably acting von Neumann algebra with a cyclic vector is hyperreflexive. We prove that under (EP), all $C^*$-algebras satisfy (SP).",11,2,2024
Weierstrass structure and eigenvalue placement of regular matrix pencils under low rank perturbations,We solve the problem of determining the Weierstrass structure of a regular matrix pencil obtained by a low rank perturbation of another regular matrix pencil. We apply the result to find necessary and sufficient conditions for the existence of a low rank perturbation such that the perturbed pencil has prescribed eigenvalues and algebraic multiplicities. The results hold over fields with sufficient number of elements.,9,2,2024
On Godel's treatment of the undecidable in 1931,"In this article we discuss the proof in the short unpublished paper appeared in the 3rd volume of Godel's Collected Works entitled ""On undecidable sentences"" (*1931?), which provides an introduction to Godel's 1931 ideas regarding the incompleteness of arithmetic. We analyze the meaning of the negation of the provability predicate, and how it is meant not to lead to vicious circle. We show how in fact in Godel's entire argument there is an omission regarding the cases of non-provability, which, once taken into consideration again, allow a completely different view of Godel's entire argument of incompleteness. Previous results of the author are applied to show that the definition of a contradiction is included in the argument of *1931?. Furthermore, an examination of the application of substitution in the well-known Godel formula as a violation of uniqueness is also briefly presented, questioning its very derivation.",7,2,2024
"$\,_{3}F_{4}$ hypergeometric functions as a sum of a product of $\,_{2}F_{3}$ functions","This paper shows that certain $\,_{3}F_{4}$ hypergeometric functions may be expanded in sums of pair products of $\,_{2}F_{3}$ functions. This expands the class of hypergeometric functions having summation theorems beyond those expressible as pair-products of generalized Whittaker functions, $\,_{2}F_{1}$ functions, and $\,_{3}F_{2}$ functions into the realm of $\,_{P}F_{Q}$ functions where $P<Q$ for both the summand and terms in the series. In addition to its intrinsic value, this result has a specific application in calculating the response of the atoms to laser stimulation in the Strong Field Approximation.",7,2,2024
Kontsevich's Formula for Rational Curves from Classical and Quantum Perspectives,"Kontsevich's formula for rational plane curves is a recursive relation for the number $N_d$ of degree $d$ rational curves in $\mathbb{P}^2$ passing through $3d-1$ general points.We provide two proofs of this recursion: the first more direct and combinatoric, the second more abstract.In order to achieve this, we introduce several moduli spaces, such as the Deligne-Mumford-Knudsen spaces and the Kontsevich spaces, and exploit their properties. In particular, the boundary structure of these spaces gives rise to certain fundamental relations crucial to both proofs.For the second proof, we reconsider the objects in question from the cohomological viewpoint and generalize the numbers $N_d$ to Gromov-Witten invariants. We introduce quantum cohomology and deduce Kontsevich's formula from the associativity of the quantum product.We also adapt these steps to the case of curves in $\mathbb{P}^1\times\mathbb{P}^1$, whose bidegrees lead to slightly more complicated but analogous results.",5,2,2024
Automorphisms of Multiplicative Lie algebra Extensions,"In this paper, we discuss the inducibility problem for automorphisms of multiplicative Lie algebra extensions and show that obstruction to the inducibility of pairs lies in the second cohomology group of multiplicative Lie algebras. We also establish the Wells type exact sequence for multiplicative Lie algebras, which relates automorphism groups with the second cohomology group of multiplicative Lie algebras.",1,2,2024
Robust estimations from distribution structures: V. Non-asymptotic,"Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.",26,2,2024
An IoT Based Water-Logging Detection System: A Case Study of Dhaka,"With a large number of populations, many problems are rising rapidly in Dhaka, the capital city of Bangladesh. Water-logging is one of the major issues among them. Heavy rainfall, lack of awareness and poor maintenance causes bad sewerage system in the city. As a result, water is overflowed on the roads and sometimes it gets mixed with the drinking water. To overcome this problem, this paper realizes the potential of using Internet of Things to combat water-logging in drainage pipes which are used to move wastes as well as rainwater away from the city. The proposed system will continuously monitor real time water level, water flow and gas level inside the drainage pipe. Moreover, all the monitoring data will be stored in the central database for graphical representation and further analysis. In addition to that if any emergency arises in the drainage system, an alert will be sent directly to the nearest maintenance office.",25,2,2024
Self-Supervised Interpretable Sensorimotor Learning via Latent Functional Modularity,"We introduce MoNet, a novel method that combines end-to-end learning with modular network architectures for self-supervised and interpretable sensorimotor learning. MoNet is composed of three functionally distinct neural modules: Perception, Planning, and Control. Leveraging its inherent modularity through a cognition-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space, without requiring task-level supervision. Moreover, our method incorporates an online post-hoc explainability approach, which enhances the interpretability of the end-to-end inferences without a trade-off in sensorimotor performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, surpassing baseline models by 11% to 47% in task specificity analysis. We further delve into the interpretability of our network through the post-hoc analysis of perceptual saliency maps and latent decision vectors. This offers insights into the incorporation of explainable artificial intelligence within the realm of robotic learning, encompassing both perceptual and behavioral perspectives.",21,2,2024
Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks,"At present, there is a trend to deploy ubiquitous artificial intelligence (AI) applications at the edge of the network. As a promising framework that enables secure edge intelligence, federated learning (FL) has received widespread attention, and over-the-air computing (AirComp) has been integrated to further improve the communication efficiency. In this paper, we consider a joint device selection and aggregate beamforming design with the objectives of minimizing the aggregate error and maximizing the number of selected devices. This yields a combinatorial problem, which is difficult to solve especially in large-scale networks. To tackle the problems in a cost-effective manner, we propose a random aggregate beamforming-based scheme, which generates the aggregator beamforming vector via random sampling rather than optimization. The implementation of the proposed scheme does not require the channel estimation. We additionally use asymptotic analysis to study the obtained aggregate error and the number of the selected devices when the number of devices becomes large. Furthermore, a refined method that runs with multiple randomizations is also proposed for performance improvement. Extensive simulation results are presented to demonstrate the effectiveness of the proposed random aggregate beamforming-based scheme as well as the refined method.",20,2,2024
Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms,"In this work, we conduct a comparative study on two deep unfolding mechanisms to efficiently perform power control in the next generation wireless networks. The power control problem is formulated as energy efficiency over multiple interference links. The problem is nonconvex. We employ fractional programming transformation to design two solutions for the problem. The first solution is a numerical solution while the second solution is a closed-form solution. Based on the first solution, we design a semi-unfolding deep learning model where we combine the domain knowledge of the wireless communications and the recent advances in the data-driven deep learning. Moreover, on the highlights of the closed-form solution, fully deep unfolded deep learning model is designed in which we fully leveraged the expressive closed-form power control solution and deep learning advances. In the simulation results, we compare the performance of the proposed deep learning models and the iterative solutions in terms of accuracy and inference speed to show their suitability for the real-time application in next generation networks.",17,2,2024
A Review of Neuroscience-Inspired Machine Learning,"One major criticism of deep learning centers around the biological implausibility of the credit assignment schema used for learning -- backpropagation of errors. This implausibility translates into practical limitations, spanning scientific fields, including incompatibility with hardware and non-differentiable implementations, thus leading to expensive energy requirements. In contrast, biologically plausible credit assignment is compatible with practically any learning condition and is energy-efficient. As a result, it accommodates hardware and scientific modeling, e.g. learning with physical systems and non-differentiable behavior. Furthermore, it can lead to the development of real-time, adaptive neuromorphic processing systems. In addressing this problem, an interdisciplinary branch of artificial intelligence research that lies at the intersection of neuroscience, cognitive science, and machine learning has emerged. In this paper, we survey several vital algorithms that model bio-plausible rules of credit assignment in artificial neural networks, discussing the solutions they provide for different scientific fields as well as their advantages on CPUs, GPUs, and novel implementations of neuromorphic hardware. We conclude by discussing the future challenges that will need to be addressed in order to make such algorithms more useful in practical applications.",16,2,2024
Enhancing Efficiency in Sparse Models with Sparser Selection,"Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We provide a comprehensive analysis and make our code available at https://anonymous.4open.science/r/XMoE.",27,2,2024
Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes,"Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptively evolve to select relevant feature interactions within populations for different lake types and tasks. These models are not only capable of undergoing crossover and mutation mechanisms within intra-populations but also, albeit infrequently, engage in inter-population crossover. The second stage involves refining these models by retraining them with real observed labels. We have tested the performance of our NGCE strategy in predicting daily DO concentrations across a wide range of lakes in the Midwest, USA. These lakes, varying in size, depth, and trophic status, represent a broad spectrum of north temperate lakes. Our findings demonstrate that NGCE not only produces accurate predictions with few observed labels but also, through gene maps of models, reveals sophisticated phenological patterns of different lakes.",15,2,2024
Feynman Diagrams as Computational Graphs,"We propose a computational graph representation of high-order Feynman diagrams in Quantum Field Theory (QFT), applicable to any combination of spatial, temporal, momentum, and frequency domains. Utilizing the Dyson-Schwinger and parquet equations, our approach effectively organizes these diagrams into a fractal structure of tensor operations, significantly reducing computational redundancy. This approach not only streamlines the evaluation of complex diagrams but also facilitates an efficient implementation of the field-theoretic renormalization scheme, crucial for enhancing perturbative QFT calculations. Key to this advancement is the integration of Taylor-mode automatic differentiation, a key technique employed in machine learning packages to compute higher-order derivatives efficiently on computational graphs. To operationalize these concepts, we develop a Feynman diagram compiler that optimizes diagrams for various computational platforms, utilizing machine learning frameworks. Demonstrating this methodology's effectiveness, we apply it to the three-dimensional uniform electron gas problem, achieving unprecedented accuracy in calculating the quasiparticle effective mass at metal density. Our work demonstrates the synergy between QFT and machine learning, establishing a new avenue for applying AI techniques to complex quantum many-body problems.",28,2,2024
Long Short-Term Memory Pattern Recognition in Currency Trading,"This study delves into the analysis of financial markets through the lens of Wyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th century. Focusing on the accumulation pattern within the Wyckoff framework, the research explores the phases of trading range and secondary test, elucidating their significance in understanding market dynamics and identifying potential trading opportunities. By dissecting the intricacies of these phases, the study sheds light on the creation of liquidity through market structure, offering insights into how traders can leverage this knowledge to anticipate price movements and make informed decisions. The effective detection and analysis of Wyckoff patterns necessitate robust computational models capable of processing complex market data, with spatial data best analyzed using Convolutional Neural Networks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models. The creation of training data involves the generation of swing points, representing significant market movements, and filler points, introducing noise and enhancing model generalization. Activation functions, such as the sigmoid function, play a crucial role in determining the output behavior of neural network models. The results of the study demonstrate the remarkable efficacy of deep learning models in detecting Wyckoff patterns within financial data, underscoring their potential for enhancing pattern recognition and analysis in financial markets. In conclusion, the study highlights the transformative potential of AI-driven approaches in financial analysis and trading strategies, with the integration of AI technologies shaping the future of trading and investment practices.",23,2,2024
"Unleashing the Power of AI. A Systematic Review of Cutting-Edge Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics","Purpose: The study aims to analyze the synergy of Artificial Intelligence (AI), with scientometrics, webometrics, and bibliometrics to unlock and to emphasize the potential of the applications and benefits of AI algorithms in these fields.Design/methodology/approach: By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends, and evaluate the impact of scientific publications. To achieve this, we implemented a comprehensive search strategy across reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search encompassed articles published from January 1, 2000, to September 2022, resulting in a thorough review of 61 relevant articles.Findings: (i) Regarding scientometrics, the application of AI yields various distinct advantages, such as conducting analyses of publications, citations, research impact prediction, collaboration, research trend analysis, and knowledge mapping, in a more objective and reliable framework. (ii) In terms of webometrics, AI algorithms are able to enhance web crawling and data collection, web link analysis, web content analysis, social media analysis, web impact analysis, and recommender systems. (iii) Moreover, automation of data collection, analysis of citations, disambiguation of authors, analysis of co-authorship networks, assessment of research impact, text mining, and recommender systems are considered as the potential of AI integration in the field of bibliometrics.Originality/value: This study covers the particularly new benefits and potential of AI-enhanced scientometrics, webometrics, and bibliometrics to highlight the significant prospects of the synergy of this integration through AI.",22,2,2024
Triangulated structure for Bondarenko's Categories,"V. Bondarenko and Y. Drozd gives a description of all indecomposable objects in a category of representations of posets, nowadays known as the Bondarenko's category. This category was essential for V. Bekkert and H. Merklen classify all indecomposable objects of the derived category of gentle algebras. In view of this connection with the derived category, which possess a triangulated structure, and of the fact that in this paper we show that the Bondarenko's category is not an abelian category, it is reasonable to contemplate the existence of a triangulated structure for the Bondarenko's category.In this paper we introduce a triangulated category structure over a quotient of Bondarenko's category, which will allow to use the techniques of triangulated category to study representations of posets.",14,2,2024
Frog-Snake prey-predation Relationship Optimization (FSRO) : A novel nature-inspired metaheuristic algorithm for feature selection,"Swarm intelligence algorithms have traditionally been designed for continuous optimization problems, and these algorithms have been modified and extended for application to discrete optimization problems. Notably, their application in feature selection for machine learning has demonstrated improvements in model accuracy, reduction of unnecessary data, and decreased computational time. This study proposes the Frog-Snake prey-predation Relationship Optimization (FSRO) algorithm, inspired by the prey-predation relationship between frogs and snakes for application to discrete optimization problems. The algorithm models three stages of a snake's foraging behavior ""search"", ""approach"", and ""capture"" as well as the frog's characteristic behavior of staying still to attract and then escaping. Furthermore, the introduction of the concept of evolutionary game theory enables dynamic control of the search process. The proposed algorithm conducts computational experiments on feature selection using 26 types of machine learning datasets to analyze its performance and identify improvements. In computer experiments, the proposed algorithm showed better performance than the comparison algorithms in terms of the best and standard deviation of fitness value and Accuracy. It was also proved that dynamic search control by evolutionary game theory is an effective method, and the proposed algorithm has the ability of a well-balanced search, achieving the two objectives of improving accuracy and reducing data.",13,2,2024
Cochain complexes over a functor,"In this paper we propose unifying the categories of cochain complexes $\text{Ch}(\mathcal{C})$ and modules $\widehat{A}\text{-mod}$ over a repetitive algebra $\widehat{A}$. Motivated by their striking similarities and importance, we introduce a novel category encompassing both. Our analysis explores key properties of this unified category, highlighting its parallels and divergences from the original structures. We study whether it preserves crucial aspects like limits, colimits, products, coproducts, and abelianity. Besides, we establish a family of projective and injective indecomposable objects within this framework. Moving beyond theoretical foundations, we examine the influence and interaction over these novel categories of the category of endofunctors and its monoidal structure. Finally, we explore the implications of our constructions over representation theory of algebras and algebraic geometry.",9,2,2024
A New Method for Sensorless Estimation of the Speed and Position in Brushed DC Motors Using Support Vector Machines,"Currently, for many applications, it is necessary to know the speed and position of motors. This can be achieved using mechanical sensors coupled to the motor shaft or using sensorless techniques. The sensorless techniques in brushed dc motors can be classified into two types: 1) techniques based on the dynamic brushed dc motor model and 2) techniques based on the ripple component of the current. This paper presents a new method, based on the ripple component, for speed and position estimation in brushed dc motors, using support vector machines. The proposed method only measures the current and detects the pulses in this signal. The motor speed is estimated by using the inverse distance between the detected pulses, and the position is estimated by counting all detected pulses. The ability to detect ghost pulses and to discard false pulses is the main advantage of this method over other sensorless methods. The performed tests on two fractional horsepower brushed dc motors indicate that the method works correctly in a wide range of speeds and situations, in which the speed is constant or varies dynamically.",7,2,2024
Rationale Dataset and Analysis for the Commit Messages of the Linux Kernel Out-of-Memory Killer,"Code commit messages can contain useful information on why a developer has made a change. However, the presence and structure of rationale in real-world code commit messages is not well studied. Here, we detail the creation of a labelled dataset to analyze the code commit messages of the Linux Kernel Out-Of-Memory Killer component. We study aspects of rationale information, such as presence, temporal evolution, and structure. We find that 98.9% of commits in our dataset contain sentences with rationale information, and that experienced developers report rationale in about 60% of the sentences in their commits. We report on the challenges we faced and provide examples for our labelling.",6,2,2024
DeepTraderX: Challenging Conventional Trading Strategies with Deep Learning in Multi-Threaded Market Simulations,"In this paper, we introduce DeepTraderX (DTX), a simple Deep Learning-based trader, and present results that demonstrate its performance in a multi-threaded market simulation. In a total of about 500 simulated market days, DTX has learned solely by watching the prices that other strategies produce. By doing this, it has successfully created a mapping from market data to quotes, either bid or ask orders, to place for an asset. Trained on historical Level-2 market data, i.e., the Limit Order Book (LOB) for specific tradable assets, DTX processes the market state $S$ at each timestep $T$ to determine a price $P$ for market orders. The market data used in both training and testing was generated from unique market schedules based on real historic stock market data. DTX was tested extensively against the best strategies in the literature, with its results validated by statistical analysis. Our findings underscore DTX's capability to rival, and in many instances, surpass, the performance of public-domain traders, including those that outclass human traders, emphasising the efficiency of simple models, as this is required to succeed in intricate multi-threaded simulations. This highlights the potential of leveraging ""black-box"" Deep Learning systems to create more efficient financial markets.",6,2,2024
LightningNet: Distributed Graph-based Cellular Network Performance Forecasting for the Edge,"The cellular network plays a pivotal role in providing Internet access, since it is the only global-scale infrastructure with ubiquitous mobility support. To manage and maintain large-scale networks, mobile network operators require timely information, or even accurate performance forecasts. In this paper, we propose LightningNet, a lightweight and distributed graph-based framework for forecasting cellular network performance, which can capture spatio-temporal dependencies that arise in the network traffic. LightningNet achieves a steady performance increase over state-of-the-art forecasting techniques, while maintaining a similar resource usage profile. Our architecture ideology also excels in the respect that it is specifically designed to support IoT and edge devices, giving us an even greater step ahead of the current state-of-the-art, as indicated by our performance experiments with NVIDIA Jetson.",8,2,2024
Dependency Aware Incident Linking in Large Cloud Systems,"Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer's satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that creates several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amount of manual toil and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverages textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only coming from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14% gain over current state-of-the-art methods). We are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual toil.",5,2,2024
The sup-completion of a Dedekind complete vector lattice II,"We persist in our investigation of the sup-completion of a Dedekind complete Riesz space, extending to the broader context of Riesz spaces. some results initially obtained by Feng, Li, Shen, and also by Erdös, and Rényi.",27,2,2024
"A $(ϕ_n, ϕ)$-Poincaré inequality on John domain","Given a bounded domain $\Omega \subset {\mathbb R}^{n}$ with $n\ge2$, let $\phi $ is a Young function satisfying the doubling condition with the constant $K_\phi<2^{n}$.If $\Omega$ is a John domain, we show that $\Omega $ supports a $(\phi_{n}, \phi)$-Poincaré inequality.Conversely, assume additionally that $\Omega$ is simply connected domain when $n=2$ or a bounded domain which is quasiconformally equivalent to some uniform domain when $n\ge3$. If $\Omega$ supports a $(\phi_n, \phi)$-Poincaré inequality, we show that it is a John domain.",19,2,2024
A Note On Lookahead In Real Life And Computing,"Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-online. We introduce interesting real life applications and well known computing problems where LA plays a significant role for making a process, system or algorithm efficient. We define new types of LA and propose a taxonomy for LA based on literature review for designing novel LA models in future. Using the concept of LA, We identify and present many interesting and non-trivial research challenges as future potential research directions. Intuitively, we observe that LA can be used as a powerful tool and framework for future researchers in design of efficient computational models and algorithms for solving non-trivial and challenging optimization problems.",2,2,2024
Exploring and Applying Audio-Based Sentiment Analysis in Music,"Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of affect by multiple volunteers, models are trained for both tasks. Overall, the performance of these models reflected that they were able to perform the tasks they were designed for effectively and accurately.",22,2,2024
Resultants of slice regular polynomials in two quaternionic variables,"We introduce a non-commutative resultant, for slice regular polynomials in two quaternionic variables, defined in terms of a suitable Dieudonné determinant.We use this tool to investigate the existence of common zeros of slice regular polynomials.",26,2,2024
Uncertainty quantification in the Henry problem using the multilevel Monte Carlo method,"We investigate the applicability of the well-known multilevel Monte Carlo (MLMC) method to the class of density-driven flow problems, in particular the problem of salinisation of coastal aquifers. As a test case, we solve the uncertain Henry saltwater intrusion problem. Unknown porosity, permeability and recharge parameters are modelled by using random fields. The classical deterministic Henry problem is non-linear and time-dependent, and can easily take several hours of computing time. Uncertain settings require the solution of multiple realisations of the deterministic problem, and the total computational cost increases drastically. Instead of computing of hundreds random realisations, typically the mean value and the variance are computed. The standard methods such as the Monte Carlo or surrogate-based methods is a good choice, but they compute all stochastic realisations on the same, often, very fine mesh. They also do not balance the stochastic and discretisation errors. These facts motivated us to apply the MLMC method. We demonstrate that by solving the Henry problem on multi-level spatial and temporal meshes, the MLMC method reduces the overall computational and storage costs. To reduce the computing cost further, parallelization is performed in both physical and stochastic spaces. To solve each deterministic scenario, we run the parallel multigrid solver ug4 in a black-box fashion.",22,2,2024
Seer: Predictive Runtime Kernel Selection for Irregular Problems,"Modern GPUs are designed for regular problems and suffer from load imbalance when processing irregular data. Prior to our work, a domain expert selects the best kernel to map fine-grained irregular parallelism to a GPU. We instead propose Seer, an abstraction for producing a simple, reproduceable, and understandable decision tree selector model which performs runtime kernel selection for irregular workloads. To showcase our framework, we conduct a case study in Sparse Matrix Vector Multiplication (SpMV), in which Seer predicts the best strategy for a given dataset with an improvement of 2$\times$ over the best single iteration kernel across the entire SuiteSparse Matrix Collection dataset.",19,2,2024
HEAL-ViT: Vision Transformers on a spherical mesh for medium-range weather forecasting,"In recent years, a variety of ML architectures and techniques have seen success in producing skillful medium range weather forecasts. In particular, Vision Transformer (ViT)-based models (e.g. Pangu-Weather, FuXi) have shown strong performance, working nearly ""out-of-the-box"" by treating weather data as a multi-channel image on a rectilinear grid. While a rectilinear grid is appropriate for 2D images, weather data is inherently spherical and thus heavily distorted at the poles on a rectilinear grid, leading to disproportionate compute being used to model data near the poles. Graph-based methods (e.g. GraphCast) do not suffer from this problem, as they map the longitude-latitude grid to a spherical mesh, but are generally more memory intensive and tend to need more compute resources for training and inference. While spatially homogeneous, the spherical mesh does not lend itself readily to be modeled by ViT-based models that implicitly rely on the rectilinear grid structure. We present HEAL-ViT, a novel architecture that uses ViT models on a spherical mesh, thus benefiting from both the spatial homogeneity enjoyed by graph-based models and efficient attention-based mechanisms exploited by transformers. HEAL-ViT produces weather forecasts that outperform the ECMWF IFS on key metrics, and demonstrate better bias accumulation and blurring than other ML weather prediction models. Further, the lowered compute footprint of HEAL-ViT makes it attractive for operational use as well, where other models in addition to a 6-hourly prediction model may be needed to produce the full set of operational forecasts required.",14,2,2024
Deformations and extensions of modified $λ$-differential Lie-Yamaguti algebras,"In this paper, we first introduce the concept and representations of modified $\lambda$-differential Lie-Yamaguti algebras. We then establish the cohomology of a modified $\lambda$-differential Lie-Yamaguti algebra with coefficients in a representation. As applications, we investigate the formal deformations and abelian extensions of modified $\lambda$-differential Lie-Yamaguti algebras by using the second cohomology group.",13,2,2024
Contrastive Learning for Regression on Hyperspectral Data,"Contrastive learning has demonstrated great effectiveness in representation learning especially for image classification tasks. However, there is still a shortage in the studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a contrastive learning framework for the regression tasks for hyperspectral data. To this end, we provide a collection of transformations relevant for augmenting hyperspectral data, and investigate contrastive learning for regression. Experiments on synthetic and real hyperspectral datasets show that the proposed framework and transformations significantly improve the performance of regression models, achieving better scores than other state-of-the-art transformations.",12,2,2024
Temporal-Spatial Processing of Event Camera Data via Delay-Loop Reservoir Neural Network,"This paper describes a temporal-spatial model for video processing with special applications to processing event camera videos. We propose to study a conjecture motivated by our previous study of video processing with delay loop reservoir (DLR) neural network, which we call Temporal-Spatial Conjecture (TSC). The TSC postulates that there is significant information content carried in the temporal representation of a video signal and that machine learning algorithms would benefit from separate optimization of the spatial and temporal components for intelligent processing. To verify or refute the TSC, we propose a Visual Markov Model (VMM) which decompose the video into spatial and temporal components and estimate the mutual information (MI) of these components. Since computation of video mutual information is complex and time consuming, we use a Mutual Information Neural Network to estimate the bounds of the mutual information. Our result shows that the temporal component carries significant MI compared to that of the spatial component. This finding has often been overlooked in neural network literature. In this paper, we will exploit this new finding to guide our design of a delay-loop reservoir neural network for event camera classification, which results in a 18% improvement on classification accuracy.",12,2,2024
Research on the evolution of smart meter technology,"Smart meter is not only a device used to measure the amount of electricity, but also a core component of the smart grid, realizing the efficient monitoring, prediction and management of power use. With an insight into the evolution of smart meter technology, I realized that this change didn't happen overnight. It has undergone a long journey from the initial mechanical electricity meters to the electronic electricity meters, and now to the highly intelligent electricity meters. Technological breakthroughs at each stage have laid the foundation for the final form of smart meters. In the era of mechanical watt-hour meters, the measurement of electric energy mainly depends on the rotation and counting of mechanical structures. The accuracy and stability of this method are affected by mechanical wear, environmental interference and other factors, and it is difficult to meet the increasing demand for power management. With the rapid development of electronic technology, the electronic electricity meter came into being. It uses electronic technology to sample and process the current and voltage, which greatly improves the accuracy and stability of the measurement. At the same time, the electronic electricity meter also has the function of remote meter reading and data processing, which has brought great convenience to the power management. However, there still have some limitations, such as the complexity of data processing and the limitation of communication capacity. It is these challenges that drive the creation of smart power meters. smart meters combine advanced technologies such as the Internet of Things, big data and cloud computing to realize the real-time monitoring, analysis and prediction of the use of power.",28,2,2024
Predicting Parkinson's disease trajectory using clinical and functional MRI features: a reproduction and replication study,"Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly understood physiopathology. In clinical practice, challenges are encountered in the diagnosis of early stages and in the prediction of the disease progression due to the absence of established biomarkers. Several biomarkers obtained using neuroimaging techniques such as functional Magnetic Resonance Imaging (fMRI) have been studied recently. However, the reliability and generalizability of neuroimaging-based measurements are susceptible to several different sources of variability, including those introduced by different analysis methods or population sampling. In this context, an evaluation of the robustness of such biomarkers is essential. This study is part of a larger project investigating the replicability of potential neuroimaging biomarkers of PD. Here, we attempt to reproduce (same data, same method) and replicate (different data or method) the models described in Nguyen et al. 2021 to predict individual's PD current state and progression using demographic, clinical and neuroimaging features (fALFF and ReHo extracted from resting-state fMRI). We used the Parkinson's Progression Markers Initiative dataset (PPMI,this http URL), as in  Nguyen et al. 2021 and tried to reproduce the original cohort, imaging features and machine learning models as closely as possible using the information available in the paper and the code. We also investigated methodological variations in cohort selection, feature extraction pipelines and sets of input features. Using the reproduction workflow, we managed to obtain better than chance performance for all our models (R2>0), but this performance remained very different from the ones reported in the original study. The challenges encountered while reproducing and replicating the original work are likely explained by the complexity of neuroimaging studies, in particular in clinical settings. We provide recommendations to facilitate the reproducibility of such studies in the future, for instance with the use of version control tools, standardization of pipelines and publication of analysis code and derived data.",20,2,2024
AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow,"The sustainability of AI systems depends on the capacity of project teams to proceed with a continuous sensitivity to their potential real-world impacts and transformative effects. Stakeholder Impact Assessments (SIAs) are governance mechanisms that enable this kind of responsiveness. They are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not one-off governance actions. They require project teams to pay continuous attention to the dynamic and changing character of AI production and use and to the shifting conditions of the real-world environments in which AI technologies are embedded. This workbook is part two of two workbooks on AI Sustainability. It provides a template of the SIA and activities that allow a deeper dive into crucial parts of it. It discusses methods for weighing values and considering trade-offs during the SIA. And, it highlights the need to treat the SIA as an end-to-end process of responsive evaluation and re-assessment.",19,2,2024
AI Ethics and Governance in Practice: An Introduction,"AI systems may have transformative and long-term effects on individuals and society. To manage these impacts responsibly and direct the development of AI systems toward optimal public benefit, considerations of AI ethics and governance must be a first priority.In this workbook, we introduce and describe our PBG Framework, a multi-tiered governance model that enables project teams to integrate ethical values and practical principles into their innovation practices and to have clear mechanisms for demonstrating and documenting this.",19,2,2024
"This Class Isn't Designed For Me: Recognizing Ableist Trends In Design Education, And Redesigning For An Inclusive And Sustainable Future","Traditional and currently-prevalent pedagogies of design perpetuate ableist and exclusionary notions of what it means to be a designer. In this paper, we trace such historically exclusionary norms of design education, and highlight modern-day instances from our own experiences as design educators in such epistemologies. Towards imagining a more inclusive and sustainable future of design education, we present three case studies from our own experience as design educators in redesigning course experiences for blind and low-vision (BLV), deaf and hard-of-hearing (DHH) students, and students with other disabilities. In documenting successful and unsuccessful practices, we imagine what a pedagogy of care in design education would look like.",19,2,2024
Large Language Model for Mental Health: A Systematic Review,"Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the enhancement of telepsychological services through personalised healthcare. Nonetheless, risks like text inconsistencies, hallucinatory content, and the lack of an ethical framework raise concerns about their clinical use. Despite these challenges, the advancement of LLMs underscores their potential as innovative clinical tools, necessitating further research and development. The review emphasizes that LLMs should complement, not replace, professional mental health services.",19,2,2024
"ChatGPT in Linear Algebra: Strides Forward, Steps to Go","As soon as a new technology emerges, the education community explores its affordances and the possibilities to apply it in education. In this paper, we analyze sessions with ChatGPT around topics in basic Linear Algebra. We reflect the process undertaken by the ChatGPT along the recent year in our area of interest, emphasising the vast improvement that has been done in grappling with Linear Algebra problems. In particular, the question whether this software can be a teaching assistant or even somehow replace the human teacher, is addressed. As of the time this paper is written, the answer is generally negative. For the small part where the answer can be positive, some reflections about an original instrumental genesis are given.Communication with the software gives the impression to talk to a human, and sometimes the question is whether the software understands the question or not. Therefore, the reader's attention is drawn to the fact that ChatGPT works on a statistical basis and not according to reflection and understanding.",18,2,2024
An International and Multidisciplinary Teaching Experience with Real Industrial Team Project Development,"This paper presents the design, objectives, experiences, and results of an international cooperation project funded by the European Commission in the context of the Erasmus Intensive Programme (IP, for short) designed to improve students' curricula. An IP is a short programme of study (minimum 2 weeks) that brings together university students and staff from at least three countries in order to encourage efficient and multinational teaching of specialist topics, which might otherwise not be taught at all. This project lasted for 6 years, covering two different editions, each one with three year duration. This project lasted for 6 years, covering two different editions, each one with three year duration. The first edition, named SAVRO (Simulation and Virtual Reality in Robotics for Industrial Assembly Processes) was held in the period 2008-2010, with the participation of three Universities, namely the Universitat Politecnica de Valencia (Spain), acting as IP coordinator, the Technische Universitat Kaiserslautern (Germany), and the Universita degli Studi di Salerno (Italy). The Universite de Reims Champagne-Ardenne (France) participated as a new partner in the subsequent edition (2011-2013) of the IP, renamed as HUMAIN (Human-Machine Interaction). Both editions of the teaching project were characterized by the same objectives and organizational aspects, aiming to provide educational initiatives based on active teaching through collaborative works between international institutions, involving industrial partners too. The aim of the paper is to illustrate the best practices that characterized the organization of our experience as well as to present some general recommendations and suggestions on how to devise computing academic curricula.",18,2,2024
Regulating Large Language Models: A Roundtable Report,"On July 20, 2023, a group of 27 scholars and digital rights advocates with expertise in law, computer science, political science, and other disciplines gathered for the Large Language Models, Law and Policy Roundtable, co-hosted by the NYU School of Law's Information Law Institute and the Center for Democracy & Technology. The roundtable convened to discuss how law and policy can help address some of the larger societal problems posed by large language models (LLMs). The discussion focused on three policy topic areas in particular:1. Truthfulness: What risks do LLMs pose in terms of generating mis- and disinformation? How can these risks be mitigated from a technical and/or regulatory perspective?2. Privacy: What are the biggest privacy risks involved in the creation, deployment, and use of LLMs? How can these risks be mitigated from a technical and/or regulatory perspective?3. Market concentration: What threats do LLMs pose concerning market/power concentration? How can these risks be mitigated from a technical and/or regulatory perspective?In this paper, we provide a detailed summary of the day's proceedings. We first recap what we deem to be the most important contributions made during the issue framing discussions. We then provide a list of potential legal and regulatory interventions generated during the brainstorming discussions.",16,2,2024
"I would love this to be like an assistant, not the teacher: a voice of the customer perspective of what distance learning students want from an Artificial Intelligence Digital Assistant","With the release of Generative AI systems such as ChatGPT, an increasing interest in using Artificial Intelligence (AI) has been observed across domains, including higher education. While emerging statistics show the popularity of using AI amongst undergraduate students, little is yet known about students' perceptions regarding AI including self-reported benefits and concerns from their actual usage, in particular in distance learning contexts. Using a two-step, mixed-methods approach, we examined the perceptions of ten online and distance learning students from diverse disciplines regarding the design of a hypothetical AI Digital Assistant (AIDA). In the first step, we captured students' perceptions via interviews, while the second step supported the triangulation of data by enabling students to share, compare, and contrast perceptions with those of peers. All participants agreed on the usefulness of such an AI tool while studying and reported benefits from using it for real-time assistance and query resolution, support for academic tasks, personalisation and accessibility, together with emotional and social support. Students' concerns related to the ethical and social implications of implementing AIDA, data privacy and data use, operational challenges, academic integrity and misuse, and the future of education. Implications for the design of AI-tailored systems are also discussed.",16,2,2024
An IoT system for a smart campus: Challenges and solutions illustrated over several real-world use cases,"This article discusses the development of an IoT system for monitoring and controlling various devices and systems from different vendors. The authors considered key challenges in IoT projects, such as interoperability and integration, scalability, and data storage, processing, and visualization, during the design and deployment phases. In addition to these general challenges, the authors also delve into the specific integration challenges they encountered. Various devices and systems were integrated into the system and five real-world scenarios in a university campus environment are used to illustrate the challenges encountered. The scenarios involve monitoring various aspects of a university campus environment, including air quality, environmental parameters, energy efficiency, solar photovoltaic energy, and energy consumption. The authors analyzed data and CPU usage to ensure that the system could handle the large amount of data generated by the devices. The platform developed uses open source projects such as Home Assistant, InfluxDB, Grafana, and Node-RED. All developments have been published as open source in public repositories. In conclusion, this work highlights the potential and feasibility of IoT systems in various real-world applications, the importance of considering key challenges in IoT projects during the design and deployment phases, and the specific integration challenges that may be encountered.",15,2,2024
"""Model Cards for Model Reporting"" in 2024: Reclassifying Category of Ethical Considerations in Terms of Trustworthiness and Risk Management","In 2019, the paper entitled ""Model Cards for Model Reporting"" introduced a new tool for documenting model performance and encouraged the practice of transparent reporting for a defined list of categories. One of the categories detailed in that paper is ethical considerations, which includes the subcategories of data, human life, mitigations, risks and harms, and use cases. We propose to reclassify this category in the original model card due to the recent maturing of the field known as trustworthy AI, a term which analyzes whether the algorithmic properties of the model indicate that the AI system is deserving of trust from its stakeholders. In our examination of trustworthy AI, we highlight three respected organizations - the European Commission's High-Level Expert Group on AI, the OECD, and the U.S.-based NIST - that have written guidelines on various aspects of trustworthy AI. These recent publications converge on numerous characteristics of the term, including accountability, explainability, fairness, privacy, reliability, robustness, safety, security, and transparency, while recognizing that the implementation of trustworthy AI varies by context. Our reclassification of the original model-card category known as ethical considerations involves a two-step process: 1) adding a new category known as trustworthiness, where the subcategories will be derived from the discussion of trustworthy AI in our paper, and 2) maintaining the subcategories of ethical considerations under a renamed category known as risk environment and risk management, a title which we believe better captures today's understanding of the essence of these topics. We hope that this reclassification will further the goals of the original paper and continue to prompt those releasing trained models to accompany these models with documentation that will assist in the evaluation of their algorithmic properties.",15,2,2024
Detection of Opioid Users from Reddit Posts via an Attention-based Bidirectional Recurrent Neural Network,"The opioid epidemic, referring to the growing hospitalizations and deaths because of overdose of opioid usage and addiction, has become a severe health problem in the United States. Many strategies have been developed by the federal and local governments and health communities to combat this crisis. Among them, improving our understanding of the epidemic through better health surveillance is one of the top priorities. In addition to direct testing, machine learning approaches may also allow us to detect opioid users by analyzing data from social media because many opioid users may choose not to do the tests but may share their experiences on social media anonymously. In this paper, we take advantage of recent advances in machine learning, collect and analyze user posts from a popular social network Reddit with the goal to identify opioid users. Posts from more than 1,000 users who have posted on three sub-reddits over a period of one month have been collected. In addition to the ones that contain keywords such as opioid, opiate, or heroin, we have also collected posts that contain slang words of opioid such as black or chocolate. We apply an attention-based bidirectional long short memory model to identify opioid users. Experimental results show that the approaches significantly outperform competitive algorithms in terms of F1-score. Furthermore, the model allows us to extract most informative words, such as opiate, opioid, and black, from posts via the attention layer, which provides more insights on how the machine learning algorithm works in distinguishing drug users from non-drug users.",9,2,2024
Radon mitigation by soil depressurisation case study: radon concentration and pressure field extension monitoring in a pilot house in Spain,"A one-year monitoring study was conducted in a pilot house with high radon levels to investigate the ability and efficiency of radon mitigation by soil depressurisation (SD) both active and passive. The study included monitoring of radon concentration, pressure field extension (pfe) under the slab and some atmospheric parameters for different testing phases. Periods in which the house remained closed to foster radon accumulation were alternated with phases of active and passive soil depressurisation under different conditions. The behaviour of the radon concentration in the pilot house was analysed along with the influence of atmospheric variables, significant correlations were found for the radon concentration with atmospheric pressure, outdoor temperature and wind. From the pfe analysis it was proven that the pressure drop with distance from the suction point of the SD system is proportional to the depressurisation generated. It was found also that the permeability characterisation of the pilot house agrees with the literature about granular fill materials characterisation for radon SD systems across Europe. Radon reductions in excess of 85% were achieved for the different testing phases in all cases. Finally, from the results it was stated that a fan power of 23 W is sufficient to ensure radon reductions over 85% for dwellings with similar aggregate layer and soil permeability.",8,2,2024
CapsF: Capsule Fusion for Extracting psychiatric stressors for suicide from twitter,"Along with factors such as cancer, blood pressure, street accidents and stroke, suicide has been one of Iran main causes of death. One of the main reasons for suicide is psychological stressors. Identifying psychological stressors in an at risk population can help in the early prevention of suicidal and suicidal behaviours. In recent years, the widespread popularity and flow of real time information sharing of social media have allowed for potential early intervention in large scale and even small scale populations. However, some automated approaches to extract psychiatric stressors from Twitter have been presented, but most of this research has been for non Persian languages. This study aims to investigate the techniques of detecting psychological stress related to suicide from Persian tweets using learning based methods. The proposed capsule based approach achieved a binary classification accuracy of 0.83.",7,2,2024
Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities,"Energy Internet (EI) is emerging as new share economy platform for flexible local energy supplies in smart cities. Empowered by the Internet-of-Things (IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy trading and sharing among prosumers, who can adeptly switch roles between providers and consumers in localized energy markets with rooftop photovoltaic panels, vehicle-to-everything technologies, packetized energy management, etc. The integration of prosumers in EI, however, will encounter many challenges in modelling, analyzing, and designing an efficient, economic, and social-optimal platform for energy sharing, calling for advanced AI/IoT-based solutions to resource optimization, information exchange, and interaction protocols in the context of the share economy. In this study, we aim to introduce a recently emerged paradigm, Machina Economicus, to investigate the economic rationality in modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors. The new paradigm, built upon the theory of machine learning and mechanism design, will offer new angles to investigate the selfishness of AI through a game-theoretic perspective, revealing potential competition and collaborations resulting from the self-adaptive learning and decision-making capacity. This study will focus on how the introduction of AI will reshape prosumer behaviors on the EI, and how this paradigm will reveal new research questions and directions when AI meets the share economy. With an extensive case analysis in the literature, we will also shed light on potential solutions for advancements of AI in future smart cities.",28,2,2024
Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future,"As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We show there is a need for multifaceted datasets, increased diversity in language and culture, more long-tailed social situations, and more interactive data in future social intelligence data efforts.",28,2,2024
Identifying Potential Inlets of Man in the Artificial Intelligence Development Process,"In this paper we hope to identify how the typical or standard artificial intelligence development process encourages or facilitates the creation of racialized technologies. We begin by understanding Sylvia Wynter's definition of the biocentric Man genre and its exclusion of Blackness from humanness. We follow this with outlining what we consider to be the typical steps for developing an AI-based technology, which we have broken down into 6 stages: identifying a problem, development process and management tool selection, dataset development and data processing, model development, deployment and risk assessment, and integration and monitoring. The goal of this paper is to better understand how Wynter's biocentric Man is being represented and reinforced by the technologies we are producing in the AI lifecycle and by the lifecycle itself; we hope to identify ways in which the distinction of Blackness from the ""ideal"" human leads to perpetual punishment at the hands of these technologies. By deconstructing this development process, we can potentially identify ways in which humans in general have not been prioritized and how those affects are disproportionately affecting marginalized people. We hope to offer solutions that will encourage changes in the AI development cycle.",27,2,2024
"A Synergistic Approach to Wildfire Prevention and Management Using AI, ML, and 5G Technology in the United States","Over the past few years, wildfires have become a worldwide environmental emergency, resulting in substantial harm to natural habitats and playing a part in the acceleration of climate change. Wildfire management methods involve prevention, response, and recovery efforts. Despite improvements in detection techniques, the rising occurrence of wildfires demands creative solutions for prompt identification and effective control. This research investigates proactive methods for detecting and handling wildfires in the United States, utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G technology. The specific objective of this research covers proactive detection and prevention of wildfires using advanced technology; Active monitoring and mapping with remote sensing and signaling leveraging on 5G technology; and Advanced response mechanisms to wildfire using drones and IOT devices. This study was based on secondary data collected from government databases and analyzed using descriptive statistics. In addition, past publications were reviewed through content analysis, and narrative synthesis was used to present the observations from various studies. The results showed that developing new technology presents an opportunity to detect and manage wildfires proactively. Utilizing advanced technology could save lives and prevent significant economic losses caused by wildfires. Various methods, such as AI-enabled remote sensing and 5G-based active monitoring, can enhance proactive wildfire detection and management. In addition, super intelligent drones and IOT devices can be used for safer responses to wildfires. This forms the core of the recommendation to the fire Management Agencies and the government.",27,2,2024
Stabilizing Quantum Simulators Of Gauge Theories Against $1/f$ Noise,"This work investigates the application of quantum simulation in the ongoing ""second"" quantum revolution, which employs various synthetic quantum matter platforms, such as ultracold atoms in optical lattices, Rydberg atoms, and superconducting qubits, to realize exotic condensed matter and particle physics phenomena with high precision and control. Gauge theories are of particular interest in modern quantum simulators as they offer a new probe of high-energy physics on low-energy tabletop devices. However, to accurately model gauge-theory phenomena on a quantum simulator, stabilizing the underlying gauge symmetry is crucial. Through this thesis we demonstrate that a recently developed experimentally feasible scheme based on linear gauge protection, initially devised to protect against coherent gauge breaking errors, can also be used to suppress incoherent errors arising from $1/f^{\beta}$ noise prominent in various quantum simulation platforms. The Bloch-Redfield formalism is introduced to model gauge violations arising due to these incoherent errors given the noise power spectrum of the environment. The efficacy of linear gauge protection in stabilizing salient features of gauge theories in quantum simulators, such as gauge invariance and exotic far from equilibrium phenomenon focusing on disorder-free localization, and quantum many-body scars against $1/f^{\beta}$ noise sources, is illustrated. These results are immediately applicable in modern analog quantum simulators and digital NISQ devices, paving the way for further development in the field of quantum simulation of lattice gauge theories.",26,2,2024
Quantum Subroutine for Variance Estimation: Algorithmic Design and Applications,"Quantum computing sets the foundation for new ways of designing algorithms, thanks to the peculiar properties inherited by quantum mechanics. The exploration of this new paradigm faces new challenges concerning which field quantum speedup can be achieved. Towards finding solutions, looking for the design of quantum subroutines that are more efficient than their classical counterpart poses solid pillars to new powerful quantum algorithms. Herewith, we delve into a grounding subroutine, the computation of the variance, whose usefulness spaces across different fields of application, particularly the Artificial Intelligence (AI) one. Indeed, the finding of the quantum counterpart of these building blocks impacts vertically those algorithms that leverage this metric. In this work, we propose QVAR, a quantum subroutine, to compute the variance that exhibits a logarithmic complexity both in the circuit depth and width, excluding the state preparation cost. With the vision of showing the use of QVAR as a subroutine for new quantum algorithms, we tackle two tasks from the AI domain: Feature Selection and Outlier Detection. In particular, we showcase two AI hybrid quantum algorithms that leverage QVAR: the Hybrid Quantum Feature Selection (HQFS) algorithm and the Quantum Outlier Detection Algorithm (QODA). In this manuscript, we describe the implementation of QVAR, HQFS, and QODA, providing their correctness and complexities and showing the effectiveness of these hybrid quantum algorithms with respect to their classical counterpart.",26,2,2024
"ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research","ChatGPT, the most accessible generative artificial intelligence (AI) tool, offers considerable potential for veterinary medicine, yet a dedicated review of its specific applications is lacking. This review concisely synthesizes the latest research and practical applications of ChatGPT within the clinical, educational, and research domains of veterinary medicine. It intends to provide specific guidance and actionable examples of how generative AI can be directly utilized by veterinary professionals without a programming background. For practitioners, ChatGPT can extract patient data, generate progress notes, and potentially assist in diagnosing complex cases. Veterinary educators can create custom GPTs for student support, while students can utilize ChatGPT for exam preparation. ChatGPT can aid in academic writing tasks in research, but veterinary publishers have set specific requirements for authors to follow. Despite its transformative potential, careful use is essential to avoid pitfalls like hallucination. This review addresses ethical considerations, provides learning resources, and offers tangible examples to guide responsible implementation. Carefully selected, up-to-date links to platforms that host large language models are provided for advanced readers with programming capability. A table of key takeaways was provided to summarize this review. By highlighting potential benefits and limitations, this review equips veterinarians, educators, and researchers to harness the power of ChatGPT effectively.",26,2,2024
Between Copyright and Computer Science: The Law and Ethics of Generative AI,"Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural creations, sharing technologies, search engines, social media offerings, and more challenge copyright-based industries and reopen questions about the reach of copyright law. Breakthroughs in artificial intelligence research, especially Large Language Models that leverage copyrighted material as part of training models, are the latest examples of the ongoing tension between copyright and computer science. The exuberance, rush-to-market, and edge problem cases created by a few misguided companies now raises challenges to core legal doctrines and may shift Open Internet practices for the worse. That result does not have to be, and should not be, the outcome.This Article shows that, contrary to some scholars' views, fair use law does not bless all ways that someone can gain access to copyrighted material even when the purpose is fair use. Nonetheless, the scientific need for more data to advance AI research means access to large book corpora and the Open Internet is vital for the future of that research. The copyright industry claims, however, that almost all uses of copyrighted material must be compensated, even for non-expressive uses. The Article's solution accepts that both sides need to change. It is one that forces the computer science world to discipline its behaviors and, in some cases, pay for copyrighted material. It also requires the copyright industry to abandon its belief that all uses must be compensated or restricted to uses sanctioned by the copyright industry. As part of this re-balancing, the Article addresses a problem that has grown out of this clash and under theorized.",24,2,2024
MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation,"Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism is embedded to curb hateful meme production.",24,2,2024
DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,"Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce $\textit{DOSA}$, the first community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial $\textbf{A}$rtifacts, by engaging with 260 participants from 19 different Indian geographic subcultures. We use a gamified framework that relies on collective sensemaking to collect the names and descriptions of these artifacts such that the descriptions semantically align with the shared sensibilities of the individuals from those cultures. Next, we benchmark four popular LLMs and find that they show significant variation across regional sub-cultures in their ability to infer the artifacts.",23,2,2024
"Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0","The AI-SPRINT project, launched in 2021 and funded by the European Commission, focuses on the development and implementation of AI applications across the computing continuum. This continuum ensures the coherent integration of computational resources and services from centralized data centers to edge devices, facilitating efficient and adaptive computation and application delivery. AI-SPRINT has achieved significant scientific advances, including streamlined processes, improved efficiency, and the ability to operate in real time, as evidenced by three practical use cases. This paper provides an in-depth examination of these applications -- Personalized Healthcare, Maintenance and Inspection, and Farming 4.0 -- highlighting their practical implementation and the objectives achieved with the integration of AI-SPRINT technologies. We analyze how the proposed toolchain effectively addresses a range of challenges and refines processes, discussing its relevance and impact in multiple domains. After a comprehensive overview of the main AI-SPRINT tools used in these scenarios, the paper summarizes of the findings and key lessons learned.",23,2,2024
The economic value of scientific software,"Academic institutions and their staff use, adapt and create software. We're thinking of business tools used to carry out their mission: teaching management (Moodle) or subject teaching support (such as Maxima for formal calculus), for example. We're talking about software resulting from research work, designed by a researcher or a team as part of a research project (funded by ANR, Europe, etc. or not) or as a research service for a third party. These projects can last for decades (such as the Coq program proof assistant project, or the GPAC multimedia content distribution platform).We discuss why this software is produced, with what resources, the interest that institutions derive from it, what we call the ''valorization'' of software resulting from scientific research. The latter is multifaceted, as are the missions of scientific institutions: social value (contribution to the world heritage of knowledge), financial value (contracts), economic value (business creation), scientific value (publication), image value (visibility of the institution among target audiences: students, researchers, companies, prescribers).",23,2,2024
Initial Indications of Safety of Driverless Automated Driving Systems,"As driverless automated driving systems (ADS) start to operate on public roads, there is an urgent need to understand how safely these systems are managing real-world traffic conditions. With data from the California Public Utilities Commission (CPUC) becoming available for Transportation Network Companies (TNCs) operating in California with and without human drivers, there is an initial basis for comparing ADS and human driving safety.This paper analyzes the crash rates and characteristics for three types of driving: Uber ridesharing trips from the CPUC TNC Annual Report in 2020, supervised autonomous vehicles (AV) driving from the California Department of Motor Vehicles (DMV) between December 2020 and November 2022, driverless ADS pilot (testing) and deployment (revenue service) program from Waymo and Cruise between March 2022 and August 2023. All of the driving was done within the city of San Francisco, excluding freeways. The same geographical confinement allows for controlling the exposure to vulnerable road users, population density, speed limit, and other external factors such as weather and road conditions. The study finds that supervised AV has almost equivalent crashes per million miles (CPMM) as Uber human driving, the driverless Waymo AV has a lower CPMM, and the driverless Cruise AV has a higher CPMM than Uber human driving. The data samples are not yet large enough to support conclusions about whether the current automated systems are more or less safe than human-operated vehicles in the complex San Francisco urban environment.",23,2,2024
Performing Non-Local Phase Estimation with a Rydberg-Superconducting Qubit Hybrid,"Distributed quantum computation is the key to high volume computation in the NISQ era. This investigation explores the key aspects necessary for the construction of a quantum network by numerically simulating the execution of the distributed phase estimation algorithm in a proposed novel superconducting-resonator-atom hybrid system. The phase estimation algorithm is used to estimate the phase or eigenvalue of a given unitary operator and is a sub-process of many other known quantum algorithms such as Shor's algorithm and the quantum counting algorithm . An entangling gate between two qubits is utilised in the distributed phase estimation algorithm, called an E2 gate which provides the possibility to transfer quantum information from one quantum computer to another, which was numerically shown to have a construction time of 17ns at a fidelity of 93%. This investigation analytically derives the Hamiltonian dynamics as well as the noise sources of each system and utilizes quantum optimal control (QOC), namely the gradient ascent pulse engineering (GRAPE) algorithm, to minimize fidelity error in the corresponding systems gate construction. The GRAPE algorithm showed very accurate engineering of Rydberg atom single and multi-qubit gates with fidelities higher than 90% while the flux qubit suffered greatly from noise with multi-qubit gate fidelities lower than 90%. The C-shunt factor was shown to decrease the noise of the flux qubit which in turn increased the probability of accurately estimating the phase using 4 counting qubits. A trade off was observed between the number of time steps in the descent/ascent and the number of GRAPE iterations ran on the optimisation for low C-shunt factors $0<\zeta<1000$. For $\zeta = 1000$, the GRAPE algorithm showed effectiveness for a large number of time steps and large amount of GRAPE iterations by reaching estimation accuracies greater than 90%.",22,2,2024
Designing Multi-Step Action Models for Enterprise AI Adoption,"This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI model designed by Empsing to address challenges hindering AI adoption in enterprises. Through a holistic examination, this paper explores MSAM's foundational principles, design architecture, and future trajectory. It evaluates MSAM's performance via rigorous testing methodologies and envisions its potential impact on advancing AI adoption within organizations.",21,2,2024
On a closed formula for a log-sine integral,"Making use of an 1847 result of Newman, a (known) closed formula for a log-sine integral is rapidly obtained in terms of Riemann Zeta and Clausen functions.",21,2,2024
Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring,"Generative AI is expected to have a vast, positive impact on education; however, at present, this potential has not yet been demonstrated at scale at university level. In this study, we present first evidence that generative AI can increase the speed of learning substantially in university students. We tested whether using the AI-powered teaching assistant Syntea affected the speed of learning of hundreds of distance learning students across more than 40 courses at the IU International University of Applied Sciences. Our analysis suggests that using Syntea reduced their study time substantially--by about 27\% on average--in the third month after the release of Syntea. Taken together, the magnitude of the effect and the scalability of the approach implicate generative AI as a key lever to significantly improve and accelerate learning by personalisation.",21,2,2024
"Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness","This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.",21,2,2024
"Asymptotic solutions of generalized Fermat-type equation of signature $(p,p,3)$ over totally real number fields","In this article, we study the asymptotic solutions of the generalized Fermat-type equation of signature $(p,p,3)$ over totally real number fields $K$, i.e., $Ax^p+By^p=Cz^3$ with prime exponent $p$ and $A,B,C \in \mathcal{O}_K \setminus \{0\}$. For certain class of fields $K$, we prove that $Ax^p+By^p=Cz^3$ has no asymptotic solutions over $K$ (resp., solutions of certain type over $K$) with restrictions on $A,B,C$ (resp., for all $A,B,C \in \mathcal{O}_K \setminus \{0\}$). Finally, we present several local criteria over $K$.",21,2,2024
On Defining Smart Cities using Transformer Neural Networks,"Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing definitions of smart cities, generating a list of potential new composite definitions. Each of these new definitions was then tested against the pre-existing individual definitions we have gathered, using cosine similarity as our metric. This process identified smart city definitions with the highest average cosine similarity, semantically positioning them as the closest on average to all the 60 individual definitions selected.",20,2,2024
Personalized Programming Guidance based on Deep Programming Learning Style Capturing,"With the rapid development of big data and AI technology, programming is in high demand and has become an essential skill for students. Meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. Previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. However, two significant challenges still need to be addressed in programming: C1) how to recognize complex programming behaviors; C2) how to capture intrinsic learning patterns that align with the actual learning process. To fill these gaps, in this paper, we propose a novel model called Programming Exercise Recommender with Learning Style (PERS), which simulates learners' intricate programming behaviors. Specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses C1). To better profile programming behaviors, we extend the Felder-Silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. Based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses C2). We perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of PERS for personalized programming guidance.",20,2,2024
SimGrade: Using Code Similarity Measures for More Accurate Human Grading,"While the use of programming problems on exams is a common form of summative assessment in CS courses, grading such exam problems can be a difficult and inconsistent process. Through an analysis of historical grading patterns we show that inaccurate and inconsistent grading of free-response programming problems is widespread in CS1 courses. These inconsistencies necessitate the development of methods to ensure more fairer and more accurate grading. In subsequent analysis of this historical exam data we demonstrate that graders are able to more accurately assign a score to a student submission when they have previously seen another submission similar to it. As a result, we hypothesize that we can improve exam grading accuracy by ensuring that each submission that a grader sees is similar to at least one submission they have previously seen. We propose several algorithms for (1) assigning student submissions to graders, and (2) ordering submissions to maximize the probability that a grader has previously seen a similar solution, leveraging distributed representations of student code in order to measure similarity between submissions. Finally, we demonstrate in simulation that these algorithms achieve higher grading accuracy than the current standard random assignment process used for grading.",19,2,2024
AI Fairness in Practice,"Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow.We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These elements form ethical and legal criteria based upon which instances of unfair bias and discrimination can be identified and mitigated across the AI project workflow.We then take a deeper dive into how the different contexts of the AI project lifecycle give rise to different fairness concerns. This allows us to identify several types of AI Fairness (Data Fairness, Application Fairness, Model Design and Development Fairness, Metric-Based Fairness, System Implementation Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach to bias identification, mitigation, and management. Building on this, we discuss how to put the principle of AI Fairness into practice across the AI project workflow through Bias Self-Assessment and Bias Risk Management as well as through the documentation of metric-based fairness criteria in a Fairness Position Statement.",19,2,2024
AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects,"Sustainable AI projects are continuously responsive to the transformative effects as well as short-, medium-, and long-term impacts on individuals and society that the design, development, and deployment of AI technologies may have. Projects, which centre AI Sustainability, ensure that values-led, collaborative, and anticipatory reflection both guides the assessment of potential social and ethical impacts and steers responsible innovation practices.This workbook is the first part of a pair that provides the concepts and tools needed to put AI Sustainability into practice. It introduces the SUM Values, which help AI project teams to assess the potential societal impacts and ethical permissibility of their projects. It then presents a Stakeholder Engagement Process (SEP), which provides tools to facilitate proportionate engagement of and input from stakeholders with an emphasis on equitable and meaningful participation and positionality awareness.",19,2,2024
Hyperbolic Jacobsthal Spinor Sequences and Their Mathematical Properties,"In this study, novel Hyperbolic spinor sequences of Jacobsthal, Jacobsthal-Lucas and Jacobsthal polynomial, which have not been studied before, are defined by investigating the relationship between spinors, which are important mathematical objects used in physics and mathematics, and split Jacobsthal and split Jacobsthal-Lucas quaternions, which are extensions of the known Jacobsthal and Jacobsthal-Lucas numbers to quaternion algebra. The recurrence relations of sequences whose members are Hyperbolic Jacobsthal, Jacobsthal-Lucas and Jacobsthal polynomial spinors are described. Additionally, certain properties of these spinors, such as the generator function and Binet formula, are presented and some identities resulting from these spinors are obtained.",16,2,2024
Predicting root numbers with neural networks,"We report on two machine learning experiments in search of statistical relationships between Dirichlet coefficients and root numbers or analytic ranks of certain low-degree $L$-functions. The first experiment is to construct interpretable models based on murmurations, a recently discovered correlation between Dirichlet coefficients and root numbers. We show experimentally that these models achieve high accuracy by learning a combination of Mestre-Nagao type heuristics and murmurations, noting that the relative importance of these features varies with degree. The second experiment is to search for a low-complexity statistic of Dirichlet coefficients that can be used to predict root numbers in polynomial time. We give experimental evidence and provide heuristics that suggest this can not be done with standard machine learning techniques.",14,2,2024
Experimental Investigation of an Incremental Contact Model for Hyperelastic Solids Using In-Situ Optical Interferometric Technique,"The hyperelastic materials would contribute to the intricacies of rough surface contact, primarily due to the heightened nonlinearity caused by stress concentration. In our previous research, an incremental contact model tailored for hyperelastic materials is proposed and validated by finite element (FEM) simulations. From an experimental perspective, this study employs an in-situ optical interferometric technique to precisely document the actual contact zone between hyperelastic solids and quartz glass. Simultaneously, the contact force is meticulously recorded in sync by a force sensor positioned beneath the hyperelastic samples. Comparing with the predictions of incremental contact model for hyperelastic materials, a significant agreement becomes evident, almost in a range of nearly complete contact. Its significance extends to practical domains such as sealing mechanisms, leakage prevention, and structural integrity, offering valuable insights for these applications.",21,2,2024
Exploring magnetic anisotropy in bcc-structured ferromagnetic thin films with three spin layers using the fourth order perturbed Heisenberg Hamiltonian,"This study investigates into the analysis of ferromagnetic thin films with a body centered cubic lattice and three spin layers, utilizing the solution of the fourth-order perturbed Heisenberg Hamiltonian equation with seven magnetic energy parameters. Spin-exchange interaction, magnetic dipole interaction, second-order magnetic anisotropy,fourth-order magnetic anisotropy, applied magnetic field, demagnetization energy, and stress-induced anisotropy were all taken into account. According to 3D plots, the minimum order of energy was observed when the second order magnetic anisotropy constant in the middle spin layer is less than those of the bottom and top spin layers. In all cases, the values of stress-induced anisotropy at the maxima of 3D plots are exactly the same when the values of the second-order magnetic anisotropy constants of the bottom, middle, and top spin layers are interchanged with each other. In 2D plots, the angle between consecutive magnetic easy and hard directions is approximately 90 degrees in all cases. Additionally, the magnetic easy and hard directions were observed to have exactly the same values when the second-order magnetic anisotropy constant of the spin layers changes. These results were compared with the results obtained using the second and third order perturbed Heisenberg Hamiltonian.",20,2,2024
"Combined X-ray diffraction, electrical resistivity, and $ab$ $initio$ study of (TMTTF)$_2$PF$_6$ under pressure: implications to the unified phase diagram","We present a combined experimental and theoretical study on the quasi-one-dimensional organic conductor (TMTTF)$_2$PF$_6$, and elucidate the variation of its physical properties under pressure. We fully resolve the crystal structure by single crystal x-ray diffraction measurements using a diamond anvil cell up to 8 GPa, and based on the structural data, we perform first-principles density-functional theory calculations and derive the $ab$ $initio$ extended Hubbard-type Hamiltonians. Furthermore, we compare the behavior of the resistivity measured up to 3 GPa using a BeCu clamp-type cell and the ground state properties of the obtained model numerically calculated by the many-variable variational Monte Carlo method. Our main findings are as follows: i) The crystal was rapidly compressed up to about 3 GPa where the volume drops to 80% and gradually varies down to 70% at 8 GPa. The transfer integrals increase following such behavior whereas the screened Coulomb interactions decrease, resulting in a drastic reduction of correlation effect. ii) The degree of dimerization in the intrachain transfer integrals, as the result of the decrease in structural dimerization together with the change in the intermolecular configuration, almost disappears above 4 GPa; the interchain transfer integrals also show characteristic variations under pressure. iii) The results of identifying the characteristic temperatures in the resistivity and the charge and spin orderings in the calculations show an overall agreement: The charge ordering sensitively becomes unstable above 1 GPa, while the spin ordering survives up to higher pressures. These results shed light on the similarities and differences between applying external pressure and substituting the chemical species (chemical pressure).",18,2,2024
Autonomous microARPES,"Angle-resolved photoemission spectroscopy (ARPES) is a technique used to map the occupied electronic structure of solids. Recent progress in X-ray focusing optics has led to the development of ARPES into a microscopic tool, permitting the electronic structure to be spatially mapped across the surface of a sample. This comes at the expense of a time-consuming scanning process to cover not only a three-dimensional energy-momentum ($E, k_z, k_y$) space but also the two-dimensional surface area. Here, we implement a protocol to autonomously search both $\mathbf{k}$- and real space in order to find positions of particular interest, either because of their high photoemission intensity or because of sharp spectral features. The search is based on the use of Gaussian process regression and can easily be expanded to include additional parameters or optimisation criteria. This autonomous experimental control is implemented on the SGM4 micro-focus beamline of the synchrotron radiation source ASTRID2.",16,2,2024
DyFeO3 electrode material with ultra-wide voltage window for aqueous symmetric supercapacitors,"Aqueous supercapacitors (SCs) encounter limitations in operational voltage and energy density due to the low decomposition voltage of water. Here, we fabricate aqueous symmetric supercapacitors (ASSCs) employing DyFeO3 as an electrode material. This hybrid SC in a 0.5 M Na2SO4 aqueous electrolyte exhibits a significantly high working voltage of 2.5 V, with an energy density of 41.81 Wh/kg at a power density of 1250 W/kg, maintaining 94% capacitance retention after 5000 cycles. By incorporating 20% volume of acetonitrile with water in the electrolyte, we extend the potential window to 3.1 V, with an energy density of 84.43 Wh/kg at a power density of 1550 W/kg. The as-fabricated ASSC shows promising stability during a 300-hour float voltage test with almost intact capacitance retention and Coulombic efficiency. For the first time, our study unveils the potential of porous DyFeO3 as an electrode material for advancing ASSCs, featuring an unprecedented ultra-wide voltage window, along with significantly large energy and power densities.",15,2,2024
Intercomparison exercise on Monte Carlo simulations of electron spectra and energy depositions by a single gold nanoparticle under X-ray irradiation,"Computational approaches, such as Monte Carlo (MC) radiation transport simulations, are used to estimate the dosimetric effects of GNPs, where results differing by orders of magnitudes have been reported by different investigators. This has motivated an intercomparison exercise, which was conducted as a joint activity of EURADOS Working Groups 6 ""Computational Dosimetry"" and 7 ""Internal Dosimetry"". The aim of this exercise was to determine the extent of such discrepancies between the results obtained by different researchers and different codes in a very simple simulation setup.Several individual EURADOS associate members and two code developer groups from outside Europe participated in this exercise applying seven different MC codes to perform the simulations of a simple defined geometry set-up of one single GNP irradiated in water by kilo-voltage X-rays. Two GNP diameters of 50 nm and 100 nm of were considered and two photon spectra as generated by X-ray tubes operated at 50 kV and 100 kV peak voltages. The geometry set-up and X-ray spectra were provided by the EURADOS task group. The participants were asked to determine for each combination of GNP size and X-ray spectrum the dose enhancement ratio (DER) of 10 nm-thick water shells up to 1000 nm and 1 $\mu$m-thick water shells up to 50 $\mu$m around the GNP. Furthermore, the electron spectra emitted from the GNP and the energy depositions in water shells around it were also to be reported.This EURADOS report summarizes the motivation and background for the exercise, the tasks to be solved, the codes used, the results reported by the participants, the consistency checks applied in their evaluation and a best estimates and uncertainty bands derived from the final results for the energy spectra of emitted electrons and the energy imparted in the vicinity of the GNP.",12,2,2024
Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool,"Many people are interested in ChatGPT since it has become a prominent AIGC model that provides high-quality responses in various contexts, such as software development and maintenance. Misuse of ChatGPT might cause significant issues, particularly in public safety and education, despite its immense potential. The majority of researchers choose to publish their work on Arxiv. The effectiveness and originality of future work depend on the ability to detect AI components in such contributions. To address this need, this study will analyze a method that can see purposely manufactured content that academic organizations use to post on Arxiv. For this study, a dataset was created using physics, mathematics, and computer science articles. Using the newly built dataset, the following step is to putthis http URLthrough its paces. The statistical analysis shows thatthis http URLis very accurate, with a rate of 98%.",9,2,2024
On Observation and The Completion of Quantum Mechanics,"We start with a discussion of the use of mathematics to model the real world then justify the role of Hilbert space formalism for such modelling in the general context of quantum logic. Following this, the incompleteness of the Schrödinger equation is discussed as well as the incompleteness of von Neumann's measurement approach \cite{vN}. Subsequently, it is shown that quantum mechanics is indeed completed by the addition of an observer, however the observer is not described in the Hamiltonian formalism but \emph{necessarily} by the quantum stochastic formalism discovered in \cite{HP}. Consequently, the complete theory of quantum mechanics appears to be the Quantum Filtering Theory \cite{ND,NLF}. Finally, it is shown how Schrödinger's cat may be understood as a quantum filter, providing an intuitively realistic model and an insight into how quantum filtering works.",1,2,2024
The performance of microwave photonic signal processors based on microcombs with different input signal waveforms,"Microwave photonic (MWP) signal processors, which process microwave signals based on pho-tonic technologies, bring advantages intrinsic to photonics such as low loss, large processing bandwidth, and strong immunity to electromagnetic interference. Optical microcombs can offer a large number of wavelength channels and compact device footprints, which make them powerful multi-wavelength sources for MWP signal processors to realize a variety of processing functions. In this paper, we experimentally demonstrate the capability of microcomb-based MWP signal processors to handle diverse input signal waveforms. In addition, we quantify the processing accuracy for different input signal waveforms, including Gaussian, triangle, parabolic, super Gaussian, and nearly square waveforms. Finally, we analyze the factors contributing to the dif-ference in the processing accuracy among the different input waveforms, and our theoretical analysis well elucidates the experimental results. These results provide a guidance for micro-comb-based MWP signal processors when processing microwave signals of various waveforms.",10,2,2024
DiffFinger: Advancing Synthetic Fingerprint Generation through Denoising Diffusion Probabilistic Models,"This study explores the generation of synthesized fingerprint images using Denoising Diffusion Probabilistic Models (DDPMs). The significant obstacles in collecting real biometric data, such as privacy concerns and the demand for diverse datasets, underscore the imperative for synthetic biometric alternatives that are both realistic and varied. Despite the strides made with Generative Adversarial Networks (GANs) in producing realistic fingerprint images, their limitations prompt us to propose DDPMs as a promising alternative. DDPMs are capable of generating images with increasing clarity and realism while maintaining diversity. Our results reveal that DiffFinger not only competes with authentic training set data in quality but also provides a richer set of biometric data, reflecting true-to-life variability. These findings mark a promising stride in biometric synthesis, showcasing the potential of DDPMs to advance the landscape of fingerprint identification and authentication systems.",15,3,2024
An intuitive multi-frequency feature representation for SO(3)-equivariant networks,"The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper.",15,3,2024
When Training-Free NAS Meets Vision Transformer: A Neural Tangent Kernel Perspective,"This paper investigates the Neural Tangent Kernel (NTK) to search vision transformers without training. In contrast with the previous observation that NTK-based metrics can effectively predict CNNs performance at initialization, we empirically show their inefficacy in the ViT search space. We hypothesize that the fundamental feature learning preference within ViT contributes to the ineffectiveness of applying NTK to NAS for ViT. We both theoretically and empirically validate that NTK essentially estimates the ability of neural networks that learn low-frequency signals, completely ignoring the impact of high-frequency signals in feature learning. To address this limitation, we propose a new method called ViNTK that generalizes the standard NTK to the high-frequency domain by integrating the Fourier features from inputs. Experiments with multiple ViT search spaces on image classification and semantic segmentation tasks show that our method can significantly speed up search costs over prior state-of-the-art NAS for ViT while maintaining similar performance on searched architectures.",15,3,2024
Image Classification for CSSVD Detection in Cacao Plants,"The detection of diseases within plants has attracted a lot of attention from computer vision enthusiasts. Despite the progress made to detect diseases in many plants, there remains a research gap to train image classifiers to detect the cacao swollen shoot virus disease or CSSVD for short, pertinent to cacao plants. This gap has mainly been due to the unavailability of high quality labeled training data. Moreover, institutions have been hesitant to share their data related to CSSVD. To fill these gaps, we propose the development of image classifiers to detect CSSVD-infected cacao plants. Our proposed solution is based on VGG16, ResNet50 and Vision Transformer (ViT). We evaluate the classifiers on a recently released and publicly accessible KaraAgroAI Cocoa dataset. Our best image classifier, based on ResNet50, achieves 95.39\% precision, 93.75\% recall, 94.34\% F1-score and 94\% accuracy on only 20 epochs. There is a +9.75\% improvement in recall when compared to previous works. Our results indicate that the image classifiers learn to identify cacao plants infected with CSSVD.",13,3,2024
Imitation Learning for Adaptive Video Streaming with Future Adversarial Information Bottleneck Principle,"Adaptive video streaming plays a crucial role in ensuring high-quality video streaming services. Despite extensive research efforts devoted to Adaptive BitRate (ABR) techniques, the current reinforcement learning (RL)-based ABR algorithms may benefit the average Quality of Experience (QoE) but suffers from fluctuating performance in individual video sessions. In this paper, we present a novel approach that combines imitation learning with the information bottleneck technique, to learn from the complex offline optimal scenario rather than inefficient exploration. In particular, we leverage the deterministic offline bitrate optimization problem with the future throughput realization as the expert and formulate it as a mixed-integer non-linear programming (MINLP) problem. To enable large-scale training for improved performance, we propose an alternative optimization algorithm that efficiently solves the MINLP problem. To address the issues of overfitting due to the future information leakage in MINLP, we incorporate an adversarial information bottleneck framework. By compressing the video streaming state into a latent space, we retain only action-relevant information. Additionally, we introduce a future adversarial term to mitigate the influence of future information leakage, where Model Prediction Control (MPC) policy without any future information is employed as the adverse expert. Experimental results demonstrate the effectiveness of our proposed approach in significantly enhancing the quality of adaptive video streaming, providing a 7.30\% average QoE improvement and a 30.01\% average ranking reduction.",12,3,2024
YAP:Ce scintillator as an absolute ultracold neutron detector,"The upcoming UCNProBe experiment at Los Alamos National Laboratory will measure the $\beta$-decay rate of free neutrons with different systematic uncertainties than previous beam-based neutron lifetime experiments. We have developed a new $^{10}$B-coated YAP:Ce scintillator whose properties are presented. The advantage of the YAP:Ce scintillator is its high Fermi potential, which reduces the probability for upscattering of ultracold neutrons, and its short decay time, which is important at high counting rates. Birks' coefficient of YAP:Ce was measured to be ($5.56^{+0.05}_{-0.30})\times 10^{-4}$ cm/MeV and light losses due to 120 nm of $^{10}$B-coating to be about 60%. The loss of light from YAP:Ce due to transmission through deuterated polystyrene scintillator was about 50%. The efficiency for counting neutrons that are captured on the $^{10}$B coating is (86.82 $\pm$ 2.61)%. Measurement with ultracold neutrons showed that YAP:Ce crystal counted 8% to 28% more UCNs compared to ZnS screen. This may be due to an uneven coating of $^{10}$B on the rough surface.",27,3,2024
Integrating embedded neural networks and self-mixing interferometry for smart sensors design,"Self-mixing interferometry is a measurement approach in which a laser beam is re-injected into the emitting laser itself after reflection on a target. Information about the position of the target can be obtained from monitoring the voltage across the laser. However, analyzing this signal is difficult. In previous works, neural networks have been used with great success to process this data. In this article, we present the first prototype of an integrated sensor based on self-mixing interferometry with embedded neural networks. It consists of a semiconductor laser (acting both as light emitter and detector) equipped with an embedded platform for data processing. The platform includes an ADC (Analog-to-Digital Converter) and an STM32L476RG microcontroller. The microcontroller runs the neural network in charge of reconstructing the displacement of a target from the interferometric signal entering the ADC. We assess the robustness of the neural network to unwanted signal amplitude variations and the impact of different network weights quantization choices required to run the network on the microcontroller. Finally, we provide a demonstration of target displacement reconstruction fully running on the embedded platform. Our results pave the way towards robust, low power and versatile sensors based on self-mixing interferometry and embedded neural networks.",26,3,2024
Helium Detection in Technical Materials,"Materials used to study nuclear fusion can retain atmospheric helium unless pretreated before an experiment. Understanding helium outgassing is important for accurate diagnostics in experiments surrounding nuclear fusion. The presence of helium is often cited as the primary evidence that a nuclear reaction has occurred, so it is imperative that known sources of helium are mitigated prior to proceeding with novel nuclear experiments. It is also necessary to ensure hermiticity when transferring gas aliquots from an experiment to a mass spectrometer. In this article, we present studies of detecting helium leak rates in systems used in novel nuclear experiments. We also present studies of helium retention in materials subjected to various heating profiles and atmospheric concentrations. Without pretreatment, stainless-steel 316 retains between 15 $\unicode{x2013}$ 240 pmol of $^{ 4}$He or an areal outgassing amount of 0.07 $\unicode{x2013}$ 1.20 pmol/$cm^{ 2}$. It also may reabsorb $^{ 4}$He from the atmosphere in time. These studies also demonstrate that it is necessary to pretreat most materials prior to performing experiments where the presence of $^{ 4}$He is being used as an indicator for novel nuclear reactions.",25,3,2024
A compact and open-source microcontroller-based rapid auto-alignment system,"Maintaining stable and precise alignment of a laser beam is crucial in many optical setups. In this work, we present a microcontroller-based rapid auto-alignment system that detects and corrects for drifts in a laser beam trajectory using a pair of two-dimensional duo-lateral position sensing detectors (PSDs) and a pair of mirror mounts with piezoelectric actuators. We develop hardware and software for interfacing with the PSDs and for controlling the motion of the piezoelectric mirrors mounts. Our auto-alignment strategy -- implemented as a state machine on the microcontroller by a FreeRTOS kernel -- is based on a simple linearized geometrical optical model. We benchmark our system using the standard case of coupling laser light efficiently into the guided mode of a single-mode fiber optic patch cable. We can recover the maximum fiber coupling efficiency in $\sim10$ seconds, even for a laser beam that was misaligned to the point of zero fiber coupling.",22,3,2024
Inserting Faces inside Captions: Image Captioning with Attention Guided Merging,"Image captioning models are widely used to describe recent and archived pictures with the objective of improving their accessibility and retrieval. Yet, these approaches tend to be inefficient and biased at retrieving people's names. In this work we introduce AstroCaptions, a dataset for the image captioning task. This dataset specifically contains thousands of public fig-ures that are complex to identify for a traditional model. We also propose a novel post-processing method to insert identified people's names inside the caption using explainable AI tools and the grounding capabilities of vi-sion-language models. The results obtained with this method show signifi-cant improvements of captions quality and a potential of reducing halluci-nations. Up to 93.2% of the persons detected can be inserted in the image captions leading to improvements in the BLEU, ROUGE, CIDEr and METEOR scores of each captioning model.",20,3,2024
Enhanced Thermal Management in High-Temperature Applications: Design and Optimization of a Water-Cooled Forced Convection System in a Hollow Cuboid Vapour Chamber Using COMSOL and MATLAB,"This report details the design and optimisation of a water-cooled forced convection heat dissipation system for use in high-temperature applications (ranges between 700 degrees - 1000 degrees K). A hollow cuboid vapour chamber model was investigated. The space within the hollow cuboid was used as the design space. COMSOL, a FEM software product was used to solve for the physical parameters of each geometry for the heat dissipation system design space. COMSOL in conjunction with MATLAB was used for the parametric and density-based topology optimisation of the geometric design in the design space. The goal of the optimization is the minimisation of a temperature gradient over the design space. This allows the heat to be evenly spread throughout the designed mesh which allows for more effective cooling. To reduce the computational time needed to solve and optimise each geometry in 3D, a 2D representation was created for the front and rear faces of the hollow cuboid setup. These 2D face designs were then extrapolated into 3D over the length of the hollow cube and COMSOL was used to find a solution for each model. This report also proposes a use case for this system wherein it would be used in conjunction with MGA and thermometric technology within coal-fired power stations for the extraction and storage of waste heat for later use.",15,3,2024
The Democratization of Wealth Management: Hedged Mutual Fund Blockchain Protocol,"We develop several innovations designed to bring the best practices of traditional investment funds to the blockchain landscape. Our innovations combine the superior mechanisms of mutual funds and hedge funds. Specifically, we illustrate how fund prices can be updated regularly like mutual funds and performance fees can be charged like hedge funds. We show how mutually hedged blockchain investment funds can operate with investor protection schemes - high water marks - and measures to offset trading slippage when redemptions happen. We provide detailed steps - including mathematical formulations and instructive pointers - to implement these ideas as blockchain smart contracts. We discuss how our designs overcome several blockchain bottlenecks and how we can make smart contracts smarter. We provide numerical illustrations of several scenarios related to the mechanisms we have tailored for blockchain implementation.The concepts we have developed for blockchain implementation can also be useful in traditional financial funds to calculate performance fees in a simplified manner. We highlight two main issues with the operation of mutual funds and hedge funds and show how blockchain technology can alleviate those concerns. The ideas developed here illustrate on one hand, how blockchain can solve many issues faced by the traditional world and on the other hand, how many innovations from traditional finance can benefit decentralized finance and speed its adoption. This becomes an example of symbiosis between decentralized and traditional finance - bringing these two realms closer and breaking down barriers between such artificial distinctions - wherein the future will be about providing better risk adjusted wealth appreciation opportunities to end customers through secure, reliable, accessible and transparent services - without getting too caught up about how such services are being rendered.",12,3,2024
TFCounter:Polishing Gems for Training-Free Object Counting,"Object counting is a challenging task with broad application prospects in security surveillance, traffic management, and disease diagnosis. Existing object counting methods face a tri-fold challenge: achieving superior performance, maintaining high generalizability, and minimizing annotation costs. We develop a novel training-free class-agnostic object counter, TFCounter, which is prompt-context-aware via the cascade of the essential elements in large-scale foundation models. This approach employs an iterative counting framework with a dual prompt system to recognize a broader spectrum of objects varying in shape, appearance, and size. Besides, it introduces an innovative context-aware similarity module incorporating background context to enhance accuracy within messy scenes. To demonstrate cross-domain generalizability, we collect a novel counting dataset named BIKE-1000, including exclusive 1000 images of shared bicycles from Meituan. Extensive experiments on FSC-147, CARPK, and BIKE-1000 datasets demonstrate that TFCounter outperforms existing leading training-free methods and exhibits competitive results compared to trained counterparts.",12,3,2024
Estimation of waveform deformation with the matched filter,"In many particle physics experiments the data processing is based on the analysis of the digitized waveforms provided by the detector. While the waveform amplitude is usually correlated to the event energy, the shape may carry useful information for event discrimination. Thanks to the high signal to noise ratio they provide, matched filters are often applied. Their original design is however intended for the estimation of the waveform amplitude only.In this work we introduce an analytical extension of the original matched filter for the estimation of a possible shape deformation with respect to a reference template. The new filter is validated on simulations and, with respect to shape parameters calculated on unfiltered waveforms or derived from the original matched filter, it improves the discrimination capability by at least a factor 2 both at low and high signal to noise ratios, making it applicable to the data of several experiments.",11,3,2024
Development and Validation of an Artificial Neural Network for the Recognition of Custom Dataset with YOLOv4,"The expanding applications, utilized by more users, enhance hardware performance and further develop cloud systems for big data processing. This leads to numerous unexplored deep learning applications, especially in advanced computer vision for object recognition. Deep learning in image processing encompasses varied tasks from recognizing elements with diverse shapes and sizes to complex element classification, coping with varying backgrounds and lighting conditions, and text recognition. Its advantages lie in robust setup and high performance for recognizing complex elements. This work aims to develop a deep learning-based detection system for automated recognition of assembly components differing in geometry, size, contour, or color. Implementing the YOLOv4 algorithm, the system detects components based on their characteristics. Testing with 13 components involves capturing them in different orientations, numbers, individual parts, or assembled groups using a Raspberry Pi microcontroller and camera. Evaluation focuses on correct object recognition, confidence values, different compositions, distances between objects, and environmental factors affecting system quality. Results show positive object recognition across all scenarios, irrespective of orientation or number of objects. Even densely packed objects are correctly recognized with high confidence (97-100%). Lighting conditions don't significantly impact results, and all objects are properly labeled. The developed system is suitable for real-time two-dimensional component detection, with potential for extension to three-dimensional analysis using multiple cameras with varied positioning and views.",10,3,2024
Employing Universal Voting Schemes for Improved Visual Place Recognition Performance,"Visual Place Recognition has been the subject of many endeavours utilizing different ensemble approaches to improve VPR performance. Ideas like multi-process fusion, Fly-Inspired Voting Units, SwitchHit or Switch-Fuse involve combining different VPR techniques together, utilizing different strategies. However, a major aspect often common to many of these strategies is voting. Voting is an extremely relevant topic to explore in terms of its application and significance for any ensemble VPR setup. This paper analyses several voting schemes to maximise the place detection accuracy of a VPR ensemble set up and determine the optimal voting schemes for selection. We take inspiration from a variety of voting schemes that are widely employed in fields such as politics and sociology and it is evident via empirical data that the selection of the voting method influences the results drastically. The paper tests a wide variety of voting schemes to present the improvement in the VPR results for several data sets. We aim to determine whether a single optimal voting scheme exists or, much like in other fields of research, the selection of a voting technique is relative to its application and environment. We propose a ranking of these different voting methods from best to worst which allows for better selection. While presenting our results in terms of voting method's performance bounds, in form of radar charts, PR curves to showcase the difference in performance and a comparison methodology using a McNemar test variant to determine the statistical significance of the differences. This test is performed to further confirm the reliability of outcomes and draw comparisons for better and informed selection a voting technique.",8,3,2024
Möbius Transform for Mitigating Perspective Distortions in Representation Learning,"Perspective distortion (PD) causes unprecedented changes in shape, size, orientation, angles, and other spatial relationships of visual concepts in images. Precisely estimating camera intrinsic and extrinsic parameters is a challenging task that prevents synthesizing perspective distortion. Non-availability of dedicated training data poses a critical barrier to developing robust computer vision methods. Additionally, distortion correction methods make other computer vision tasks a multi-step approach and lack performance. In this work, we propose mitigating perspective distortion (MPD) by employing a fine-grained parameter control on a specific family of Möbius transform to model real-world distortion without estimating camera intrinsic and extrinsic parameters and without the need for actual distorted data. Also, we present a dedicated perspectively distorted benchmark dataset, ImageNet-PD, to benchmark the robustness of deep learning models against this new dataset. The proposed method outperforms on existing benchmarks, ImageNet-E and ImageNet-X. Additionally, it significantly improves performance on ImageNet-PD while consistently performing on standard data distribution. Further, our method shows improved performance on three PD-affected real-world applications: crowd counting, fisheye image recognition, and person re-identification. We will release source code, dataset, and models for foster further research.",7,3,2024
Neural Additive Image Model: Interpretation through Interpolation,"Understanding how images influence the world, interpreting which effects their semantics have on various quantities and exploring the reasons behind changes in image-based predictions are highly difficult yet extremely interesting problems. By adopting a holistic modeling approach utilizing Neural Additive Models in combination with Diffusion Autoencoders, we can effectively identify the latent hidden semantics of image effects and achieve full intelligibility of additional tabular effects. Our approach offers a high degree of flexibility, empowering us to comprehensively explore the impact of various image characteristics. We demonstrate that the proposed method can precisely identify complex image effects in an ablation study. To further showcase the practical applicability of our proposed model, we conduct a case study in which we investigate how the distinctive features and attributes captured within host images exert influence on the pricing of Airbnb rentals.",6,3,2024
Servo-based light shutters with Arduino control,"In optical experiments, shutters are devices that open or close a path of light. They are often used to limit the duration of light exposure onto a target or onto a detector in order to reduce possible light-induced damage. Many commercial shutters are available for different applications - some provide very fast opening and closing times, some can handle large optical powers, and others allow for fail-safe operation. Many of these devices are costly and offer limited control options. Here we provide an open-source design for a low-cost, general purpose shutter system based on ubiquitous servo motors that are connected to an Arduino-based controller. Several shutters can be controlled by one controller, further reducing system cost. The state of the shutters can be controlled via a display built into the controller, by serial commands via USB, or by electrical control lines. The use of a microcontroller makes the shutter controller adaptable - only control options that are used need to be included, and the design accommodates a selection of display and servo options. We provide designs for all required components, including 3D print files for the servo holders and cases, software for the Arduino, libraries for serial communication (C and python), and example graphical user interfaces for testing.",5,3,2024
Copyright related risks in the creation and use of ML/AI systems,"This paper summarizes the current copyright related risks that Machine Learning (ML) and Artificial Intelligence (AI) systems (including Large Language Models --LLMs) incur. These risks affect different stakeholders: owners of the copyright of the training data, the users of ML/AI systems, the creators of trained models, and the operators of AI systems. This paper also provides an overview of ongoing legal cases in the United States related to these risks.",27,3,2024
Untangling Knots: Leveraging LLM for Error Resolution in Computational Notebooks,"Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. There are many tools for bug fixing; however, they are generally targeted at the classical linear code. With the rise of code-fluent Large Language Models, a new stream of smart bug-fixing tools has emerged. However, the applicability of those tools is still problematic for non-linear computational notebooks. In this paper, we propose a potential solution for resolving errors in computational notebooks via an iterative LLM-based agent. We discuss the questions raised by this approach and share a novel dataset of computational notebooks containing bugs to facilitate the research of the proposed approach.",26,3,2024
An Experimental Study on the Rashomon Effect of Balancing Methods in Imbalanced Classification,"Predictive models may generate biased predictions when classifying imbalanced datasets. This happens when the model favors the majority class, leading to low performance in accurately predicting the minority class. To address this issue, balancing or resampling methods are critical pre-processing steps in the modeling process. However, there have been debates and questioning of the functionality of these methods in recent years. In particular, many candidate models may exhibit very similar predictive performance, which is called the Rashomon effect, in model selection. Selecting one of them without considering predictive multiplicity which is the case of yielding conflicting models' predictions for any sample may lead to a loss of using another model. In this study, in addition to the existing debates, the impact of balancing methods on predictive multiplicity is examined through the Rashomon effect. It is important because the blind model selection is risky from a set of approximately equally accurate models. This may lead to serious problems in model selection, validation, and explanation. To tackle this matter, we conducted real dataset experiments to observe the impact of balancing methods on predictive multiplicity through the Rashomon effect. Our findings showed that balancing methods inflate the predictive multiplicity, and they yield varying results. To monitor the trade-off between performance and predictive multiplicity for conducting the modeling process responsibly, we proposed using the extended performance-gain plot for the Rashomon effect.",22,3,2024
Semantically Aligned Question and Code Generation for Automated Insight Generation,"Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.",21,3,2024
Early-stage detection of cognitive impairment by hybrid quantum-classical algorithm using resting-state functional MRI time-series,"Following the recent development of quantum machine learning techniques, the literature has reported several quantum machine learning algorithms for disease detection. This study explores the application of a hybrid quantum-classical algorithm for classifying region-of-interest time-series data obtained from resting-state functional magnetic resonance imaging in patients with early-stage cognitive impairment based on the importance of cognitive decline for dementia or aging. Classical one-dimensional convolutional layers are used together with quantum convolutional neural networks in our hybrid algorithm. In the classical simulation, the proposed hybrid algorithms showed higher balanced accuracies than classical convolutional neural networks under the similar training conditions. Moreover, a total of nine brain regions (left precentral gyrus, right superior temporal gyrus, left rolandic operculum, right rolandic operculum, left parahippocampus, right hippocampus, left medial frontal gyrus, right cerebellum crus, and cerebellar vermis) among 116 brain regions were found to be relatively effective brain regions for the classification based on the model performances. The associations of the selected nine regions with cognitive decline, as found in previous studies, were additionally validated through seed-based functional connectivity analysis. We confirmed both the improvement of model performance with the quantum convolutional neural network and neuroscientific validities of brain regions from our hybrid quantum-classical model.",16,3,2024
Empirical Studies of Parameter Efficient Methods for Large Language Models of Code and Knowledge Transfer to R,"Recently, Large Langauge Models (LLMs) have gained a lot of attention in the Software Engineering (SE) community. LLMs or their variants pre-trained on code are used for many SE tasks. A main approach for adapting LLMs to the downstream task is to fine-tune the models. However, with having billions-parameters-LLMs, fine-tuning the models is not practical. An alternative approach is using Parameter Efficient Fine Tuning (PEFT), in which the model parameters are frozen and only a few added parameters are trained. Though the LLMs are used for programming languages such as Python and Java widely, their capability for low-resource languages is limited. In this work, we empirically study PEFT methods, LoRA and Compacter, on CodeT5 and CodeLlama. We will assess their performance compared to fully fine-tuned models, whether they can be used for knowledge transfer from natural language models to code (using T5 and Llama models), and their ability to adapt the learned knowledge to an unseen language. For the unseen language, we aim to study R, as it has a wide community. The adaptability with less computational costs makes LLMs accessible in scenarios where heavy computational resources are not available. Moreover, studying R opens new opportunities for using LLMs for other languages. We anticipate our findings to showcase the capabilities of PEFT for code LLMs for R and reveal the improvement areas.",16,3,2024
Enhancing 3T Retinotopic Maps Using Diffeomorphic Registration,"Retinotopic mapping aims to uncover the relationship between visual stimuli on the retina and neural responses on the visual cortical surface. This study advances retinotopic mapping by applying diffeomorphic registration to the 3T NYU retinotopy dataset, encompassing analyze-PRF and mrVista data. Diffeomorphic Registration for Retinotopic Maps (DRRM) quantifies the diffeomorphic condition, ensuring accurate alignment of retinotopic maps without topological violations. Leveraging the Beltrami coefficient and topological condition, DRRM significantly enhances retinotopic map accuracy. Evaluation against existing methods demonstrates DRRM's superiority on various datasets, including 3T and 7T retinotopy data. The application of diffeomorphic registration improves the interpretability of low-quality retinotopic maps, holding promise for clinical applications.",1,3,2024
A Statistical Method for Improving Momentum Measurement of Photon Conversions Reconstructed from Single Electrons,"The reconstruction of photon conversions is importantin order to improve the reconstruction efficiency of the physics measurements involving photons. However, there are significant number of conversions in which only one of the two tracks emitted electrons is reconstructed in the detector due to very asymmetric energy sharing between the electron-positron pair. The momentum determination of the parent photon can be improved by estimating the missing energy in such conversions. In this study, we propose a simple statistical method that can be used to determine the mean value of the missing energy. By using simulated minimum bias events at LHC conditions and a toy detector simulation, the performance of the method is tested for several decay channels commonly used in particle physics analyses. A considerable improvement in the mass reconstruction precision is obtained when reconstructing particles decaying to photons whose energies are less than 20 GeV.",18,3,2024
Closed-form congestion control via deep symbolic regression,"As mobile networks embrace the 5G era, the interest in adopting Reinforcement Learning (RL) algorithms to handle challenges in ultra-low-latency and high throughput scenarios increases. Simultaneously, the advent of packetized fronthaul networks imposes demanding requirements that traditional congestion control mechanisms cannot accomplish, highlighting the potential of RL-based congestion control algorithms. Although learning RL policies optimized for satisfying the stringent fronthaul requirements is feasible, the adoption of neural network models in real deployments still poses some challenges regarding real-time inference and interpretability. This paper proposes a methodology to deal with such challenges while maintaining the performance and generalization capabilities provided by a baseline RL policy. The method consists of (1) training a congestion control policy specialized in fronthaul-like networks via reinforcement learning, (2) collecting state-action experiences from the baseline, and (3) performing deep symbolic regression on the collected dataset. The proposed process overcomes the challenges related to inference-time limitations through closed-form expressions that approximate the baseline performance (link utilization, delay, and fairness) and which can be directly implemented in any programming language. Finally, we analyze the inner workings of the closed-form expressions.",28,3,2024
Advancing Frontiers in SLAM: A Survey of Symbolic Representation and Human-Machine Teaming in Environmental Mapping,"This survey paper presents a comprehensive overview of the latest advancements in the field of Simultaneous Localization and Mapping (SLAM) with a focus on the integration of symbolic representation of environment features. The paper synthesizes research trends in multi-agent systems (MAS) and human-machine teaming, highlighting their applications in both symbolic and sub-symbolic SLAM tasks. The survey emphasizes the evolution and significance of ontological designs and symbolic reasoning in creating sophisticated 2D and 3D maps of various environments. Central to this review is the exploration of different architectural approaches in SLAM, with a particular interest in the functionalities and applications of edge and control agent architectures in MAS settings. This study acknowledges the growing demand for enhanced human-machine collaboration in mapping tasks and examines how these collaborative efforts improve the accuracy and efficiency of environmental mapping",22,3,2024
Analysis of a Modular Autonomous Driving Architecture: The Top Submission to CARLA Leaderboard 2.0 Challenge,"In this paper we present the architecture of the Kyber-E2E submission to the map track of CARLA Leaderboard 2.0 Autonomous Driving (AD) challenge 2023, which achieved first place. We employed a modular architecture for our solution consists of five main components: sensing, localization, perception, tracking/prediction, and planning/control. Our solution leverages state-of-the-art language-assisted perception models to help our planner perform more reliably in highly challenging traffic scenarios. We use open-source driving datasets in conjunction with Inverse Reinforcement Learning (IRL) to enhance the performance of our motion planner. We provide insight into our design choices and trade-offs made to achieve this solution. We also explore the impact of each component in the overall performance of our solution, with the intent of providing a guideline where allocation of resources can have the greatest impact.",21,3,2024
Life-long Learning and Testing for Automated Vehicles via Adaptive Scenario Sampling as A Continuous Optimization Process,"Sampling critical testing scenarios is an essential step in intelligence testing for Automated Vehicles (AVs). However, due to the lack of prior knowledge on the distribution of critical scenarios in sampling space, we can hardly efficiently find the critical scenarios or accurately evaluate the intelligence of AVs. To solve this problem, we formulate the testing as a continuous optimization process which iteratively generates potential critical scenarios and meanwhile evaluates these scenarios. A bi-level loop is proposed for such life-long learning and testing. In the outer loop, we iteratively learn space knowledge by evaluating AV in the already sampled scenarios and then sample new scenarios based on the retained knowledge. Outer loop stops when all generated samples cover the whole space. While to maximize the coverage of the space in each outer loop, we set an inner loop which receives newly generated samples in outer loop and outputs the updated positions of these samples. We assume that points in a small sphere-like subspace can be covered (or represented) by the point in the center of this sphere. Therefore, we can apply a multi-rounds heuristic strategy to move and pack these spheres in space to find the best covering solution. The simulation results show that faster and more accurate evaluation of AVs can be achieved with more critical scenarios.",28,3,2024
Joint torques prediction of a robotic arm using neural networks,"Accurate dynamic models are crucial for many robotic applications. Traditional approaches to deriving these models are based on the application of Lagrangian or Newtonian mechanics. Although these methods provide a good insight into the physical behaviour of the system, they rely on the exact knowledge of parameters such as inertia, friction and joint flexibility. In addition, the system is often affected by uncertain and nonlinear effects, such as saturation and dead zones, which can be difficult to model. A popular alternative is the application of Machine Learning (ML) techniques - e.g., Neural Networks (NNs) - in the context of a ""black-box"" methodology. This paper reports on our experience with this approach for a real-life 6 degrees of freedom (DoF) manipulator. Specifically, we considered several NN architectures: single NN, multiple NNs, and cascade NN. We compared the performance of the system by using different policies for selecting the NN hyperparameters. Our experiments reveal that the best accuracy and performance are obtained by a cascade NN, in which we encode our prior physical knowledge about the dependencies between joints, complemented by an appropriate optimisation of the hyperparameters.",28,3,2024
Analysis of the Efficacy of the Use of Inertial Measurement and Global Positioning System Data to Reverse Engineer Automotive CAN Bus Steering Signals,"Autonomous vehicle control is growing in availability for new vehicles and there is a potential need to retrofit older vehicles with this capability. Additionally, automotive cybersecurity has become a significant concern in recent years due to documented attacks on vehicles. As a result, researchers have been exploring reverse engineering techniques to automate vehicle control and improve vehicle security and threat analysis. In prior work, a vehicle's accelerator and brake pedal controller area network (CAN) channels were identified using reverse engineering techniques without prior knowledge of the vehicle. However, the correlation results for deceleration were lower than those for acceleration, which may be able to be improved by incorporating data from an additional telemetry device. In this paper, a method that uses IMU and GPS data to reverse-engineer a vehicle's steering wheel position CAN channels, without prior knowledge of the vehicle, is presented. Using GPS data is shown to greatly improve correlation values for deceleration, particularly for the brake pedal CAN channels. This work demonstrates the efficacy of using these data sources for automotive CAN reverse engineering. This has potential uses in automotive vehicle control and for improving vehicle security and threat analysis.",27,3,2024
Large Language Models for Human-Robot Interaction: Opportunities and Risks,"The tremendous development in large language models (LLM) has led to a new wave of innovations and applications and yielded research results that were initially forecast to take longer. In this work, we tap into these recent developments and present a meta-study about the potential of large language models if deployed in social robots. We place particular emphasis on the applications of social robots: education, healthcare, and entertainment. Before being deployed in social robots, we also study how these language models could be safely trained to ``understand'' societal norms and issues, such as trust, bias, ethics, cognition, and teamwork. We hope this study provides a resourceful guide to other robotics researchers interested in incorporating language models in their robots.",26,3,2024
"Technical, organizational and oral history regarding the soil samples measurements for Cs-137 because of the Chernobyl accident fallout","Data are given, commentary is supplied and explanations are provided with regard to the technical, the organizational and, of course, the human history connected to the time of research, which resulted to the paper entitled ""Soil sampling and Cs-137 analysis of the Chernobyl fallout in Greece"", written by late Professor S.E. Simopoulos. This paper has been provided in Greek translation within an issued honorary volume (ISBN 978-960-254-714-4). Reasonably, the narration starts with the review of the political, the financial and the social situation of Greece around 1986. Subsequently, an analysis is given on the then available means, the persons involved, the methods used, the lessons learned and any other connection with the oral history of the NTUA's Nuclear Engineering Laboratory and other relevant Greek Laboratories. For this history, written proof is now scarce and the persons available to pass it on are growing less and less. N.P. Petropoulos, now Laboratory member and then student of Professor S.E. Simopoulos was in charge of preparation of this text.",24,3,2024
Proactive Route Planning for Electric Vehicles,"Due to the limited driving range, inadequate charging facilities, and time-consuming recharging, the process of finding an optimal charging route for electric vehicles (EVs) differs from that of other vehicle types. The time and location of EV charging during a trip impact not only the individual EV's travel time but also the travel time of other EVs, due to the queuing that may arise at the charging station(s). This issue is at large seen as a significant constraint for uplifting EV sales in many countries. In this study, we present a novel Electric Vehicle Route Planning problem, which involves finding the fastest route with recharging for an EV routing request. We model the problem as a new graph problem and present that the problem is NP-hard. We propose a novel two-phase algorithm to traverse the graph to find the best possible charging route for each EV. We also introduce the notion of `influence factor' to propose heuristics to find the best possible route for an EV with the minimum travel time that avoids using charging stations and time to recharge at those stations which can lead to better travel time for other EVs. The results show that our method can decrease total travel time of the EVs by 50\% in comparison with the state-of-the-art on a real dataset, where the benefit of our approach is more significant as the number of EVs on the road increases.",15,3,2024
Scenarios Engineering driven Autonomous Transportation in Open-Pit Mines,"One critical bottleneck that impedes the development and deployment of autonomous transportation in open-pit mines is guaranteed robustness and trustworthiness in prohibitively extreme scenarios. In this research, a novel scenarios engineering (SE) methodology for the autonomous mining truck is proposed for open-pit mines. SE increases the trustworthiness and robustness of autonomous trucks from four key components: Scenario Feature Extractor, Intelligence & Index (I&I), Calibration & Certification (C&C), and Verification & Validation (V&V). Scenario feature extractor is a comprehensive pipeline approach that captures complex interactions and latent dependencies in complex mining scenarios. I&I effectively enhances the quality of the training dataset, thereby establishing a solid foundation for autonomous transportation in mining areas. C&C is grounded in the intrinsic regulation, capabilities, and contributions of the intelligent systems employed in autonomous transportation to align with traffic participants in the real world and ensure their performance through certification. V&V process ensures that the autonomous transportation system can be correctly implemented, while validation focuses on evaluating the ability of the well-trained model to operate efficiently in the complex and dynamic conditions of the open-pit mines. This methodology addresses the unique challenges of autonomous transportation in open-pit mining, promoting productivity, safety, and performance in mining operations.",15,3,2024
Anti-Jamming Path Planning Using GCN for Multi-UAV,"This paper addresses the increasing significance of UAVs (Unmanned Aerial Vehicles) and the emergence of UAV swarms for collaborative operations in various domains. However, the effectiveness of UAV swarms can be severely compromised by jamming technology, necessitating robust antijamming strategies. While existing methods such as frequency hopping and physical path planning have been explored, there remains a gap in research on path planning for UAV swarms when the jammer's location is unknown. To address this, a novel approach, where UAV swarms leverage collective intelligence to predict jamming areas, evade them, and efficiently reach target destinations, is proposed. This approach utilizes Graph Convolutional Networks (GCN) to predict the location and intensity of jamming areas based on information gathered from each UAV. A multi-agent control algorithm is then employed to disperse the UAV swarm, avoid jamming, and regroup upon reaching the target. Through simulations, the effectiveness of the proposed method is demonstrated, showcasing accurate prediction of jamming areas and successful evasion through obstacle avoidance algorithms, ultimately achieving the mission objective. Proposed method offers robustness, scalability, and computational efficiency, making it applicable across various scenarios where UAV swarms operate in potentially hostile environments.",13,3,2024
"Understanding Social Perception, Interactions, and Safety Aspects of Sidewalk Delivery Robots Using Sentiment Analysis","This article presents a comprehensive sentiment analysis (SA) of comments on YouTube videos related to Sidewalk Delivery Robots (SDRs). We manually annotated the collected YouTube comments with three sentiment labels: negative (0), positive (1), and neutral (2). We then constructed models for text sentiment classification and tested the models' performance on both binary and ternary classification tasks in terms of accuracy, precision, recall, and F1 score. Our results indicate that, in binary classification tasks, the Support Vector Machine (SVM) model using Term Frequency-Inverse Document Frequency (TF-IDF) and N-gram get the highest accuracy. In ternary classification tasks, the model using Bidirectional Encoder Representations from Transformers (BERT), Long Short-Term Memory Networks (LSTM) and Gated Recurrent Unit (GRU) significantly outperforms other machine learning models, achieving an accuracy, precision, recall, and F1 score of 0.78. Additionally, we employ the Latent Dirichlet Allocation model to generate 10 topics from the comments to explore the public's underlying views on SDRs. Drawing from these findings, we propose targeted recommendations for shaping future policies concerning SDRs. This work provides valuable insights for stakeholders in the SDR sector regarding social perception, interaction, and safety.",9,3,2024
Optimal Planning for Timed Partial Order Specifications,"This paper addresses the challenge of planning a sequence of tasks to be performed by multiple robots while minimizing the overall completion time subject to timing and precedence constraints. Our approach uses the Timed Partial Orders (TPO) model to specify these constraints. We translate this problem into a Traveling Salesman Problem (TSP) variant with timing and precedent constraints, and we solve it as a Mixed Integer Linear Programming (MILP) problem. Our contributions include a general planning framework for TPO specifications, a MILP formulation accommodating time windows and precedent constraints, its extension to multi-robot scenarios, and a method to quantify plan robustness. We demonstrate our framework on several case studies, including an aircraft turnaround task involving three Jackal robots, highlighting the approach's potential applicability to important real-world problems. Our benchmark results show that our MILP method outperforms state-of-the-art open-source TSP solvers OR-Tools.",8,3,2024
Technical Report on BaumEvA Evolutionary Optimization Python-Library Testing,"This report presents the test results Python library BaumEvA, which implements evolutionary algorithms for optimizing various types of problems, including computer vision tasks accompanied by the search for optimal model architectures. Testing was carried out to evaluate the effectiveness and reliability of the pro-posed methods, as well as to determine their applicability in various fields. Dur-ing testing, various test functions and parameters of evolutionary algorithms were used, which made it possible to evaluate their performance in a wide range of conditions. Test results showed that the library provides effective and reliable methods for solving optimization problems. However, some limitations were identified related to computational resources and execution time of algorithms on problems with large dimensions. The report includes a detailed description of the tests performed, the results obtained and conclusions about the applicability of the genetic algorithm in various tasks. Recommendations for choosing algorithm pa-rameters and using the library to achieve the best results are also provided. The report may be useful to developers involved in the optimization of complex com-puting systems, as well as to researchers studying the possibilities of using evo-lutionary algorithms in various fields of science and technology.",6,3,2024
"The active visual sensing methods for robotic welding: review, tutorial and prospect","The visual sensing system is one of the most important parts of the welding robots to realize intelligent and autonomous welding. The active visual sensing methods have been widely adopted in robotic welding because of their higher accuracies compared to the passive visual sensing methods. In this paper, we give a comprehensive review of the active visual sensing methods for robotic welding. According to their uses, we divide the state-of-the-art active visual sensing methods into four categories: seam tracking, weld bead defect detection, 3D weld pool geometry measurement and welding path planning. Firstly, we review the principles of these active visual sensing methods. Then, we give a tutorial of the 3D calibration methods for the active visual sensing systems used in intelligent welding robots to fill the gaps in the related fields. At last, we compare the reviewed active visual sensing methods and give the prospects based on their advantages and disadvantages.",6,3,2024
Science Fiction Media Representations of Exoplanets: Portrayals of Changing Astronomical Discoveries,"Interest in science fiction's (SF's) potential science communication use is hindered by concerns about SF misrepresenting science. This study addresses these concerns by asking how SF media reflects scientific findings in exoplanet science. A database of SF exoplanets was analysed using a Bayesian network to find interconnected interactions between planetary characterisation features and literary data. Results reveal SF exoplanets designed after the discovery of real exoplanets are less Earth-like, providing statistical evidence that SF incorporates rapidly-evolving science. Understanding SF's portrayal of science is crucial for its potential use in science communication.",4,3,2024
A theory of best choice selection through objective arguments grounded in Linear Response Theory concepts,"In this paper, we propose how to use objective arguments grounded in statistical mechanics concepts in order to obtain a single number, obtained after aggregation, which would allow to rank ""agents"", ""opinions"", ..., all defined in a very broad sense. We aim toward any process which should a priori demand or lead to some consensus in order to attain the presumably best choice among many possibilities. In order to precise the framework, we discuss previous attempts, recalling trivial ""means of scores"", - weighted or not, Condorcet paradox, TOPSIS, etc. We demonstrate through geometrical arguments on a toy example, with 4 criteria, that the pre-selected order of criteria in previous attempts makes a difference on the final result. However, it might be unjustified. Thus, we base our ""best choice theory"" on the linear response theory in statistical mechanics: we indicate that one should be calculating correlations functions between all possible choice evaluations, thereby avoiding an arbitrarily ordered set of criteria. We justify the point through an example with 6 possible criteria. Applications in many fields are suggested. Beside, two toy models serving as practical examples and illustrative arguments are given in an Appendix.",30,3,2024
A guideline for the methodology chapter in computer science dissertations,"Rather than simply offering suggestions, this guideline for the methodology chapter in computer science dissertations provides thorough insights on how to develop a strong research methodology within the area of computer science. The method is structured into several parts starting with an overview of research strategies which include experiments, surveys, interviews and case studies. The guide highlights the significance of defining a research philosophy and reasoning by talking about paradigms such as positivism, constructivism and pragmatism. Besides, it reveals the importance of types of research including deductive and inductive methodologies; basic versus applied research approaches. Moreover, this guideline discusses data collection and analysis intricacies that divide data into quantitative and qualitative typologies. It explains different ways in which data can be collected from observation to experimentation, interviews or surveys. It also mentions ethical considerations in research emphasizing ethical behavior like following academic principles. In general, this guideline is an essential tool for undertaking computer science dissertations that help researchers structure their work while maintaining ethical standards in their study design.",29,3,2024
Tuning the response of bubble-based metamaterials with short transient pulses,"Bubble-based metamaterials have been extensively studied both theoretically and experimentally thanks to their simple geometry and their ability to manipulate acoustic waves. The latter is partly dependent on the structural characteristics of the metamaterial and partly dependent on the incident acoustic wave. Initially, the selection of specific structural characteristics is explained by presenting the Fourier transformations of the reflected waves for different arrangements of a bubbly meta-screen subject to Gaussian excitation. Next, our numerical study focuses on the changes induced to the response of a bubbly meta-screen, subject to different excitation pulses. For complex-frequency excitation the bubbles delay to return to their equilibrium position for a couple of moments, hence the energy is stored in the system during those moments. This research provides a new strategy to actively control the response of a bubbly meta-screen and seeks to inspire future studies towards further optimization of the incident pulse based on the functionalities in need.",28,3,2024
Getting a Handle on Unmanaged Memory,"The inability to relocate objects in unmanaged languages brings with it a menagerie of problems. Perhaps the most impactful is memory fragmentation, which has long plagued applications such as databases and web servers. These issues either fester or require Herculean programmer effort to address on a per-application basis because, in general, heap objects cannot be moved in unmanaged languages. In contrast, managed languages like C# cleanly address fragmentation through the use of compacting garbage collection techniques built upon heap object movement. In this work, we bridge this gap between unmanaged and managed languages through the use of handles, a level of indirection allowing heap object movement. Handles open the door to seamlessly employ runtime features from managed languages in existing, unmodified code written in unmanaged languages. We describe a new compiler and runtime system, ALASKA, that acts as a drop-in replacement for malloc. Without any programmer effort, the ALASKA compiler transforms pointer-based code to utilize handles, with optimizations to reduce performance impact. A codesigned runtime system manages this level of indirection and exploits heap object movement via an extensible service interface. We investigate the overheads of ALASKA on large benchmarks and applications spanning multiple domains. To show the power and extensibility of handles, we use ALASKA to eliminate fragmentation on the heap through compaction, reducing memory usage by up to 40% in Redis.",26,3,2024
"Comment on ""Recovering noise-free quantum observables""","Zero-noise extrapolation (ZNE) stands as the most widespread quantum error mitigation technique in order to aim the recovery of noise-free expectation values of observables of interest by means of Noisy Intermediate-Scale Quantum (NISQ) machines. Recently, Otten and Gray proposed a multidimensional generalization of poynomial ZNE for systems where there is not a tunable global noise source [Phys. Rev. A \textbf{99,} 012338 (2019)]. Specifically, the authors refer to multiqubit systems where each of the qubits experiences several noise processes with different rates, i.e. a non-identically distributed noise model. While effective, the proposed method presents an unbearable experiment repetition overhead, making it impractical, at least from the perspective of quantum computing. In this comment, we show that the traditional extrapolation techniques can be applied for such non-identically distributed noise setting consisted of many different noise sources, implying that the measurement overhead is reduced considerably. For doing so, we clarify what it is meant by a tunable global noise source in the context of ZNE, concept that we consider important to be clarified for a correct understanding about how and why these methods work.",26,3,2024
Spatio-temporal load shifting for truly clean computing,"Companies with datacenters are procuring significant amounts of renewable energy to reduce their carbon footprint. There is increasing interest in achieving 24/7 Carbon-Free Energy (CFE) matching in electricity usage, aiming to eliminate all carbon footprints associated with electricity consumption on an hourly basis. However, the variability of renewable energy resources poses significant challenges for achieving this goal. We explore the impact of shifting computing jobs and associated power loads both in time and between datacenter locations. We develop an optimization model to simulate a network of geographically distributed datacenters managed by a company leveraging spatio-temporal load flexibility to achieve 24/7 CFE matching. We isolate three signals relevant for informed use of load flexiblity: varying average quality of renewable energy resources, low correlation between wind power generation over long distances due to different weather conditions, and lags in solar radiation peak due to Earth's rotation. We illustrate that the location of datacenters and the time of year affect which signal drives an effective load-shaping strategy. The energy procurement and load-shifting decisions based on informed use of these signals facilitate the resource-efficiency and cost-effectiveness of clean computing -- the costs of 24/7 CFE are reduced by 1.29$\pm$0.07 EUR/MWh for every additional percentage of flexible load. We provide practical guidelines on how companies with datacenters can leverage spatio-temporal load flexibility for truly clean computing. Our results and the open-source optimization model can also be useful for a broader variety of companies with flexible loads and an interest in eliminating their carbon footprint.",26,3,2024
Experimental and Numerical Validation of Tape-Based Metasurfaces in Guiding High-Frequency Surface Waves for Efficient Power Transfer,"We present an effective method for transmitting electromagnetic waves as surface waves with a tape-based metasurface design. This design incorporates silver square patches periodically patterned on an adhesive tape substrate. Specifically, our study proposes a strategy to enhance the efficiency of power transfer in high-frequency bands by guiding signals as surface waves rather than free-space waves. Both the numerical and experimental results validate the markedly enhanced efficiency in power transfer of high-frequency signals compared to that achieved with conventional methods, such as wireless power transfer and microstrips. Importantly, our metasurface design can be readily manufactured and tailored for various environments. Thus, our study contributes to designing power-efficient next-generation communication systems such as 6G and 7G, which leverage high-frequency signals in the millimeter-wave and THz bands.",15,3,2024
Directed High-Energy Infrared Laser Beams for Photovoltaic Generation of Electric Power at Remote Locations,"Transferring energy without transferring mass is a powerful paradigm to address the challenges faced when the access to, or the deployment of, the infrastructure for energy conversion is locally impossible or impractical. Laser beaming holds the promise of effectively implementing this paradigm. With this perspective, this work evaluates the optical-to-electrical power conversion that is created when a collimated laser beam illuminates a silicon photovoltaic solar cell that is located kilometers away from the laser. The laser is a CW high-energy Yb-doped fiber laser emitting at a center wavelength of 1075 nm with ~1 m2 of effective beam area. For 20 kW illumination of a solar panel having 0.6 m2 of area, optical simulations and thermal simulations indicate electrical output power of 3000 Watts at a panel temperature of 550 K. Our investigations show that thermo-radiative cells are rather inefficient. In contrast, an optimized approach to harvest laser energy is achieved by using a hybrid module consisting of a photovoltaic cell and a thermo-electric generator. Finally, practical considerations related to infrared power beaming are discussed and its potential applications are outlined.",8,3,2024
One-way Valley-locked waveguide with large channel achieved by all-dielectric Photonic Crystals,"Nonreciprocity, which denotes the asymmetric or even unidirectional transmission of light, constitutes the cornerstone of modern photonic circuits. In the realm of photonic devices, it has been widely utilized in isolators, circulators and so on. Recent topology in artificial materials, an unprecedented degree of freedom, has been proposed to solve the effect of impurities on nonreciprocal transmission. However, in view of the bulk-edge correspondence, the spatial width of the transmission channel with uniform field distribution is quite narrow and needs further exploration. In this paper, we proposed a one-way valley-locked waveguide with a large channel in an all-dielectric photonic crystal. Quite different from the topological edge modes, the unidirectional property of our waveguide comes from the bulk modes with valley-lock, which can fully utilize the whole dimension of the structure with an efficiency of 100%. Additionally, the electrical field is uniformly distributed across the entire channel, which opens a new avenue for low-loss nonreciprocity devices.",8,3,2024
Tight Non-Radiating Bends of 3D-Printed Dielectric Image Lines Based on Electromagnetic Bandgap Mirrors,"This paper reports on a novel compact, low-loss bending technique for additively manufactured dielectric image lines between 140 GHz and 220 GHz. Conventional bending approaches require either large curvature radii or technologically challenging permittivity variations to prevent radiation loss at line discontinuities, e.g. caused by narrow bends. In contrast, this work uses extremely compact, easy-to-manufacture electromagnetic bandgap (EBG) cells to solve the afore mentioned challenge for the first time. These offer excellent reflection properties and thus enable broadband and low-loss (IL < 1 dB) guidance of electromagnetic waves by means of total reflection. Without increasing the complexity of the process, both the high-pass behaviour and the enormous space requirement of conventional dielectric bends are completely avoided. In addition, the use of EBGs improves the mutual isolation of dielectric image lines of up to 30 dB. Therefore, a promising solution for the realization of narrow, 3D-printed, low-loss signal distribution networks in the sub-THz domain is offered.",7,3,2024
Técnicas Quantum-Inspired en Tensor Networks para Contextos Industriales,"In this paper we present a study of the applicability and feasibility of quantum-inspired algorithms and techniques in tensor networks for industrial environments and contexts, with a compilation of the available literature and an analysis of the use cases that may be affected by such methods. In addition, we explore the limitations of such techniques in order to determine their potential scalability.",8,3,2024
"Deepfake Labels Restore Reality, Especially for Those Who Dislike the Speaker","Deepfake videos create dangerous possibilities for public misinformation. In this experiment (N=204), we investigated whether labeling videos as containing actual or deepfake statements from US President Biden helps participants later differentiate between true and fake information. People accurately recalled 93.8% of deepfake videos and 84.2% of actual videos, suggesting that labeling videos can help combat misinformation. Individuals who identify as Republican and had lower favorability ratings of Biden performed better in distinguishing between actual and deepfake videos, a result explained by the elaboration likelihood model (ELM), which predicts that people who distrust a message source will more critically evaluate the message.",29,3,2024
Spontaneous disentanglement and thermalisation,"The problem of quantum measurement can be partially resolved by incorporating a process of spontaneous disentanglement into quantum dynamics. We propose a modified master equation, which contains a nonlinear term giving rise to both spontaneous disentanglement and thermalisation. We find that the added nonlinear term enables limit cycle steady states, which are prohibited in standard quantum mechanics. This finding suggests that an experimental observation of such a limit cycle steady state can provide an important evidence supporting the spontaneous disentanglement hypothesis.",20,3,2024
ML2SC: Deploying Machine Learning Models as Smart Contracts on the Blockchain,"With the growing concern of AI safety, there is a need to trust the computations done by machine learning (ML) models. Blockchain technology, known for recording data and running computations transparently and in a tamper-proof manner, can offer this trust. One significant challenge in deploying ML Classifiers on-chain is that while ML models are typically written in Python using an ML library such as Pytorch, smart contracts deployed on EVM-compatible blockchains are written in Solidity. We introduce Machine Learning to Smart Contract (ML2SC), a PyTorch to Solidity translator that can automatically translate multi-layer perceptron (MLP) models written in Pytorch to Solidity smart contract versions. ML2SC uses a fixed-point math library to approximate floating-point computation. After deploying the generated smart contract, we can train our models off-chain using PyTorch and then further transfer the acquired weights and biases to the smart contract using a function call. Finally, the model inference can also be done with a function call providing the input. We mathematically model the gas costs associated with deploying, updating model parameters, and running inference on these models on-chain, showing that the gas costs increase linearly in various parameters associated with an MLP. We present empirical results matching our modeling. We also evaluate the classification accuracy showing that the outputs obtained by our transparent on-chain implementation are identical to the original off-chain implementation with Pytorch.",28,3,2024
Dynamic Vulnerability Criticality Calculator for Industrial Control Systems,"The convergence of information and communication technologies has introduced new and advanced capabilities to Industrial Control Systems. However, concurrently, it has heightened their vulnerability to cyber attacks. Consequently, the imperative for new security methods has emerged as a critical need for these organizations to effectively identify and mitigate potential threats. This paper introduces an innovative approach by proposing a dynamic vulnerability criticality calculator. Our methodology encompasses the analysis of environmental topology and the effectiveness of deployed security mechanisms, coupled with the utilization of the Common Vulnerability Scoring System framework to adjust detected vulnerabilities based on the specific environment. Moreover, it evaluates the quantity of vulnerabilities and their interdependencies within each asset. Additionally, our approach integrates these factors into a comprehensive Fuzzy Cognitive Map model, incorporating attack paths to holistically assess the overall vulnerability score. To validate the efficacy of our proposed method, we present a relative case study alongside several modified scenarios, demonstrating its effectiveness in practical applications.",20,3,2024
Expectation Entropy as a Password Strength Metric,"The classical combinatorics-based password strength formula provides a result in tens of bits, whereas the NIST Entropy Estimation Suite give a result between 0 and 1 for Min-entropy. In this work, we present a newly developed metric -- Expectation entropy that can be applied to estimate the strength of any random or random-like password. Expectation entropy provides the strength of a password on the same scale as an entropy estimation tool. Having an 'Expectation entropy' of a certain value, for example, 0.4 means that an attacker has to exhaustively search at least 40\% of the total number of guesses to find the password.",18,3,2024
A Disease Labeler for Chinese Chest X-Ray Report Generation,"In the field of medical image analysis, the scarcity of Chinese chest X-ray report datasets has hindered the development of technology for generating Chinese chest X-ray reports. On one hand, the construction of a Chinese chest X-ray report dataset is limited by the time-consuming and costly process of accurate expert disease annotation. On the other hand, a single natural language generation metric is commonly used to evaluate the similarity between generated and ground-truth reports, while the clinical accuracy and effectiveness of the generated reports rely on an accurate disease labeler (classifier). To address the issues, this study proposes a disease labeler tailored for the generation of Chinese chest X-ray reports. This labeler leverages a dual BERT architecture to handle diagnostic reports and clinical information separately and constructs a hierarchical label learning algorithm based on the affiliation between diseases and body parts to enhance text classification performance. Utilizing this disease labeler, a Chinese chest X-ray report dataset comprising 51,262 report samples was established. Finally, experiments and analyses were conducted on a subset of expert-annotated Chinese chest X-ray reports, validating the effectiveness of the proposed disease labeler.",18,3,2024
EdgeLeakage: Membership Information Leakage in Distributed Edge Intelligence Systems,"In contemporary edge computing systems, decentralized edge nodes aggregate unprocessed data and facilitate data analytics to uphold low transmission latency and real-time data processing capabilities. Recently, these edge nodes have evolved to facilitate the implementation of distributed machine learning models, utilizing their computational resources to enable intelligent decision-making, thereby giving rise to an emerging domain referred to as edge intelligence. However, within the realm of edge intelligence, susceptibility to numerous security and privacy threats against machine learning models becomes evident. This paper addresses the issue of membership inference leakage in distributed edge intelligence systems. Specifically, our focus is on an autonomous scenario wherein edge nodes collaboratively generate a global model. The utilization of membership inference attacks serves to elucidate the potential data leakage in this particular context. Furthermore, we delve into the examination of several defense mechanisms aimed at mitigating the aforementioned data leakage problem. Experimental results affirm that our approach is effective in detecting data leakage within edge intelligence systems, and the implementation of our defense methods proves instrumental in alleviating this security threat. Consequently, our findings contribute to safeguarding data privacy in the context of edge intelligence systems.",8,3,2024
Membership Information Leakage in Federated Contrastive Learning,"Federated Contrastive Learning (FCL) represents a burgeoning approach for learning from decentralized unlabeled data while upholding data privacy. In FCL, participant clients collaborate in learning a global encoder using unlabeled data, which can serve as a versatile feature extractor for diverse downstream tasks. Nonetheless, FCL is susceptible to privacy risks, such as membership information leakage, stemming from its distributed nature, an aspect often overlooked in current solutions. This study delves into the feasibility of executing a membership inference attack on FCL and proposes a robust attack methodology. The attacker's objective is to determine if the data signifies training member data by accessing the model's inference output. Specifically, we concentrate on attackers situated within a client framework, lacking the capability to manipulate server-side aggregation methods or discern the training status of other clients. We introduce two membership inference attacks tailored for FCL: the \textit{passive membership inference attack} and the \textit{active membership inference attack}, contingent on the attacker's involvement in local model training. Experimental findings across diverse datasets validate the effectiveness of our attacks and underscore the inherent privacy risks associated with the federated contrastive learning paradigm.",6,3,2024
Smart Grids Secured By Dynamic Watermarking: How Secure?,"Unconditional security for smart grids is defined. Cryptanalyses of the watermarked security of smart grids indicate that watermarking cannot guarantee unconditional security unless the communication within the grid system is unconditionally secure. The successful attack against the dynamically watermarked smart grid remains valid even with the presence of internal noise from the grid. An open question arises: if unconditionally authenticated secure communications within the grid, together with tamper resistance of the critical elements, are satisfactory conditions to provide unconditional security for the grid operation.",5,3,2024
Pauli-Villars and the ultraviolet completion of Einstein gravity,"Through use of the Pauli-Villars regulator procedure we construct a second- plus fourth-order-derivative theory of gravity that serves as an ultraviolet completion of standard second-order-derivative quantum Einstein gravity that is ghost-free, unitary and power counting renormalizable.",28,3,2024
Pilot Study to Discover Candidate Biomarkers for Autism based on Perception and Production of Facial Expressions,"Purpose: Facial expression production and perception in autism spectrum disorder (ASD) suggest potential presence of behavioral biomarkers that may stratify individuals on the spectrum into prognostic or treatment subgroups. Construct validity and group discriminability have been recommended as criteria for identification of candidate stratification biomarkers.Methods: In an online pilot study of 11 children and young adults diagnosed with ASD and 11 age- and gender-matched neurotypical (NT) individuals, participants recognize and mimic static and dynamic facial expressions of 3D avatars. Webcam-based eye-tracking (ET) and facial video tracking (VT), including activation and asymmetry of action units (AUs) from the Facial Action Coding System (FACS) are collected. We assess validity of constructs for each dependent variable (DV) based on the expected response in the NT group. Then, the Boruta statistical method identifies DVs that are significant to group discriminability (ASD or NT).Results: We identify one candidate ET biomarker (percentage gaze duration to the face while mimicking static 'disgust' expression) and 14 additional DVs of interest for future study, including 4 ET DVs, 5 DVs related to VT AU activation, and 4 DVs related to AU asymmetry in VT. Based on a power analysis, we provide sample size recommendations for future studies.Conclusion: This pilot study provides a framework for ASD stratification biomarker discovery based on perception and production of facial expressions.",27,3,2024
On a vectorized basic linear algebra package for prototyping codes in MATLAB,"When writing high-performance code for numerical computation in a scripting language like MATLAB, it is crucial to have the operations in a large for-loop vectorized. If not, the code becomes too slow to use, even for a moderately large problem. However, in the process of vectorizing, the code often loses its original structure and becomes less readable. This is particularly true in the case of a finite element implementation, even though finite element methods are inherently structured. A basic remedy to this is the separation of the vectorization part from the mathematics part of the code, which is easily achieved through building the code on top of the basic linear algebra subprograms that are already vectorized codes, an idea that has been used in a series of papers over the last fifteen years, developing codes that are fast and still structured and readable. We discuss the vectorized basic linear algebra package and introduce a formalism using multi-linear algebra to explain and define formally the functions in the package, as well as MATLAB pagetime functions. We provide examples from computations of varying complexity, including the computation of normal vectors, volumes, and finite element methods. Benchmarking shows that we also get fast computations. Using the library, we can write codes that closely follow our mathematical thinking, making writing, following, reusing, and extending the code easier.",15,3,2024
Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation,"We introduce Metric3D v2, a geometric foundation model for zero-shot metric depth and surface normal estimation from a single image, which is crucial for metric 3D recovery. While depth and normal are geometrically related and highly complimentary, they present distinct challenges. SoTA monocular depth methods achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods have limited zero-shot performance due to the lack of large-scale labeled data. To tackle these issues, we propose solutions for both metric depth estimation and surface normal estimation. For metric depth estimation, we show that the key to a zero-shot single-view model lies in resolving the metric ambiguity from various camera models and large-scale data training. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problem and can be effortlessly plugged into existing monocular models. For surface normal estimation, we propose a joint depth-normal optimization module to distill diverse data knowledge from metric depth, enabling normal estimators to learn beyond normal labels. Equipped with these modules, our depth-normal models can be stably trained with over 16 million of images from thousands of camera models with different-type annotations, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our project page is atthis https URL.",22,3,2024
Label-Efficient Sleep Staging Using Transformers Pre-trained with Position Prediction,"Sleep staging is a clinically important task for diagnosing various sleep disorders, but remains challenging to deploy at scale because it because it is both labor-intensive and time-consuming. Supervised deep learning-based approaches can automate sleep staging but at the expense of large labeled datasets, which can be unfeasible to procure for various settings, e.g., uncommon sleep disorders. While self-supervised learning (SSL) can mitigate this need, recent studies on SSL for sleep staging have shown performance gains saturate after training with labeled data from only tens of subjects, hence are unable to match peak performance attained with larger datasets. We hypothesize that the rapid saturation stems from applying a sub-optimal pretraining scheme that pretrains only a portion of the architecture, i.e., the feature encoder, but not the temporal encoder; therefore, we propose adopting an architecture that seamlessly couples the feature and temporal encoding and a suitable pretraining scheme that pretrains the entire model. On a sample sleep staging dataset, we find that the proposed scheme offers performance gains that do not saturate with amount of labeled training data (e.g., 3-5\% improvement in balanced sleep staging accuracy across low- to high-labeled data settings), reducing the amount of labeled training data needed for high performance (e.g., by 800 subjects). Based on our findings, we recommend adopting this SSL paradigm for subsequent work on SSL for sleep staging.",29,3,2024
DCAE-SR: Design of a Denoising Convolutional Autoencoder for reconstructing Electrocardiograms signals at Super Resolution,"Electrocardiogram (ECG) signals play a pivotal role in cardiovascular diagnostics, providing essential information on the electrical activity of the heart. However, the inherent noise and limited resolution in ECG recordings can hinder accurate interpretation and diagnosis. In this paper, we propose a novel model for ECG super resolution (SR) that uses a DNAE to enhance temporal and frequency information inside ECG signals. Our approach addresses the limitations of traditional ECG signal processing techniques. Our model takes in input 5-second length ECG windows sampled at 50 Hz (very low resolution) and it is able to reconstruct a denoised super-resolution signal with an x10 upsampling rate (sampled at 500 Hz). We trained the proposed DCAE-SR on public available myocardial infraction ECG signals. Our method demonstrates superior performance in reconstructing high-resolution ECG signals from very low-resolution signals with a sampling rate of 50 Hz. We compared our results with the current deep-learning literature approaches for ECG super-resolution and some non-deep learning reproducible methods that can perform both super-resolution and denoising. We obtained current state-of-the-art performances in super-resolution of very low resolution ECG signals frequently corrupted by ECG artifacts. We were able to obtain a signal-to-noise ratio of 12.20 dB (outperforms previous 4.68 dB), mean squared error of 0.0044 (outperforms previous 0.0154) and root mean squared error of 4.86% (outperforms previous 12.40%). In conclusion, our DCAE-SR model offers a robust (to artefact presence), versatile and explainable solution to enhance the quality of ECG signals. This advancement holds promise in advancing the field of cardiovascular diagnostics, paving the way for improved patient care and high-quality clinical decisions",29,3,2024
Two Decades of Scientific Misconduct in India: Retraction Reasons and Journal Quality among Inter-country and Intra-country Institutional Collaboration,"Research stands as a pivotal factor in propelling the progress of any nation forward. However, if tainted by misconduct, it poses a significant threat to the nation's development. This study aims to scrutinize various cases of deliberate scientific misconduct by Indian researchers. A comprehensive analysis was conducted on 3,244 retracted publications sourced from the Retraction Watch database. The upward trend in retractions is alarming, although the decreasing duration of retractions indicates proactive measures by journals against misconduct. Approximately 60% of retractions stem from private institutions, with fake peer reviews identified as the primary cause of misconduct. This trend could be attributed to incentivizing publication quantity over quality in private institutions, potentially fostering unfair publishing practices. Retractions due to data integrity issues are predominantly observed in public and medical institutions, while retractions due to plagiarism occur in conference proceedings and non-Scopus-indexed journals. Examining retractions resulting from institutional collaborations reveals that 80% originate from within the country, with the remaining 20% being international collaborations. Among inter-country collaborations, one-third of retractions come from the top two journal quartiles, whereas, in intra-country collaborations, half of the retractions stem from Q1 and Q2 journals. Clinical studies retracted from intra-country collaborations are mostly from Q3 and Q4 journals, whereas in inter-country collaborations, they primarily come from Q1 journals. Regarding top journals by the number of retractions in intra-country collaborations, they belong to the Q2 and Q4 categories, whereas in inter-country collaborations, they are in Q1.",29,3,2024
ADAPT^2: Adapting Pre-Trained Sensing Models to End-Users via Self-Supervision Replay,"Self-supervised learning has emerged as a method for utilizing massive unlabeled data for pre-training models, providing an effective feature extractor for various mobile sensing applications. However, when deployed to end-users, these models encounter significant domain shifts attributed to user diversity. We investigate the performance degradation that occurs when self-supervised models are fine-tuned in heterogeneous domains. To address the issue, we propose ADAPT^2, a few-shot domain adaptation framework for personalizing self-supervised models. ADAPT2 proposes self-supervised meta-learning for initial model pre-training, followed by a user-side model adaptation by replaying the self-supervision with user-specific data. This allows models to adjust their pre-trained representations to the user with only a few samples. Evaluation with four benchmarks demonstrates that ADAPT^2 outperforms existing baselines by an average F1-score of 8.8%p. Our on-device computational overhead analysis on a commodity off-the-shelf (COTS) smartphone shows that ADAPT2 completes adaptation within an unobtrusive latency (in three minutes) with only a 9.54% memory consumption, demonstrating the computational efficiency of the proposed method.",29,3,2024
Facilitating Human Feedback for GenAI Prompt Optimization,"This study investigates the optimization of Generative AI (GenAI) systems through human feedback, focusing on how varying feedback mechanisms influence the quality of GenAI outputs. We devised a Human-AI training loop where 32 students, divided into two groups, evaluated AI-generated responses based on a single prompt. One group assessed a single output, while the other compared two outputs. Preliminary results from this small-scale experiment suggest that comparative feedback might encourage more nuanced evaluations, highlighting the potential for improved human-AI collaboration in prompt optimization. Future research with larger samples is recommended to validate these findings and further explore effective feedback strategies for GenAI systems.",29,3,2024
State Space Paradox of Computational Research in Creativity,"This paper explores the paradoxical nature of computational creativity, focusing on the inherent limitations of closed digital systems in emulating the open-ended, dynamic process of human creativity. Through a comprehensive analysis, we delve into the concept of the State Space Paradox (SSP) in computational research on creativity, which arises from the attempt to model or replicate creative behaviors within the bounded state spaces of digital systems. Utilizing a combination of procedural and representational paradigms, we examine various computational models and their capabilities to assist or emulate the creative process. Our investigation encompasses rule-based systems, genetic algorithms, case-based reasoning, shape grammars, and data mining, among others, to understand how these methods contribute to or fall short of achieving genuine creativity. The discussion extends to the implications of SSP on the future of creativity-related computer systems, emphasizing the cultural and contextual fluidity of creativity itself and the challenges of producing truly creative outcomes within the constraints of pre-defined algorithmic structures. We argue that while digital systems can provoke sudden mental insights (SMIs) in human observers and potentially support the creative process, their capacity to autonomously break out of their pre-programmed state spaces and achieve originality akin to human creativity remains fundamentally constrained. The paper concludes with reflections on the future directions for research in computational creativity, suggesting that recognizing and embracing the limitations and potentials of digital systems could lead to more nuanced and effective tools for creative assistance.",28,3,2024
Robust Phase Retrieval by Alternating Minimization,"We consider a least absolute deviation (LAD) approach to the robust phase retrieval problem that aims to recover a signal from its absolute measurements corrupted with sparse noise. To solve the resulting non-convex optimization problem, we propose a robust alternating minimization (Robust-AM) derived as an unconstrained Gauss-Newton method. To solve the inner optimization arising in each step of Robust-AM, we adopt two computationally efficient methods for linear programs. We provide a non-asymptotic convergence analysis of these practical algorithms for Robust-AM under the standard Gaussian measurement assumption. These algorithms, when suitably initialized, are guaranteed to converge linearly to the ground truth at an order-optimal sample complexity with high probability while the support of sparse noise is arbitrarily fixed and the sparsity level is no larger than $1/4$. Additionally, through comprehensive numerical experiments on synthetic and image datasets, we show that Robust-AM outperforms existing methods for robust phase retrieval offering comparable theoretical performance",28,3,2024
Development of a Gamification Model for Personalized E-learning,"This study designed a personality-based gamification model for E-learning systems. It also implemented the model and evaluated the performance of the gamification model implemented. These were with a view to developing a model for gamifying personalization of e-learning systems. Personalization requirements for motivational tendencies based on the Myers-Briggs Type Indicator (MBTI) and gamification elements were elicited for e-learning from existing literature and from education experts using interview and questionnaire. The gamification model for personalized e-learning was designed by mapping motivational tendencies to corresponding gamification elements using set theory and rendered using Unified modelling language (UML) tools. The model was implemented using Hypertext Markup Language for the front end, Hypertext Preprocessor (PHP) for the backend and Structured Query Language (SQL) for database on WordPress. The model was evaluated using appeal, emotion, user-centricity as well as satisfaction as engagement criteria, and clarity, error correction as well as feedback for educational usability. The results collected from the implemented system database and questionnaires administered to learners showed an average appeal rating of 4.3, an emotion rating of 4.5, a user-centricity rating of 4.4, and a satisfaction rating of 4.4 in terms of engagement on a 5.0 scale. The results also showed that clarity, error correction and feedback received an average rating of 3.9, 4.7, and 4.8 respectively on a 5.0 scale concerning educational usability. In addition, when comparing educational usability (4.5) to engagement (4.4), educational usability received slightly higher ratings. The study concluded that the gamification model for personalized e-learning was suitable for increasing learner motivation and engagement within the personalized e-learning environment.",28,3,2024
Bioregionalization analyses with the bioregion R-package,"Bioregionalization consists in the identification of spatial units with similar species composition and is a classical approach in the fields of biogeography and macroecology. The recent emergence of global databases, improvements in computational power, and the development of clustering algorithms coming from the network theory have led to several major updates of the bioregionalizations of many taxa. A typical bioregionalization workflow involves five different steps: formatting the input data, computing a (dis)similarity matrix, selecting a clustering algorithm, evaluating the resulting bioregionalization, and mapping and interpreting the bioregions. For most of these steps, there are many options available in the methods and R packages. Here, we present bioregion, a package that includes all the steps of a bioregionalization workflow under a single architecture, with an exhaustive list of the clustering algorithms used in biogeography and macroecology. These algorithms include (non-)hierarchical algorithms as well as community detection algorithms coming from the network theory. Some key methods from the literature, such as Infomap or OSLOM, that were not available in the R language are included in bioregion. By allowing different methods coming from different fields to communicate easily, bioregion will allow a reproducible and complete comparison of the different bioregionalization methods, which is still missing in the literature.",28,3,2024
On the implementation in Abaqus of the global--local iterative coupling and acceleration techniques,"This paper presents results and convergence study of the Global--Local Iterative Coupling through the implementation in the commercial software Abaqus making use of the co-simulation engine. A hierarchical modeling and simulation approach is often required to alleviate modeling burdens. Particular focus has been devoted here on convergence acceleration and performance optimization. Two applications in statics with nonlinear material behavior and geometrically nonlinear formulation are considered here: first a holed curved plate under traction with elastic--plastic material, then a pre-stressed bolted joint connecting two plates between each other and subjected to traction load. Three different convergence acceleration techniques are compared in terms of convergence performance and accuracy. An inexact solver strategy is proposed to improve computing time performance. The results show promising results for the coupling technology and constitute a step forward in the availability of non-intrusive multi-scale modeling capabilities for complex structures and assemblies.",27,3,2024
"Unraveling Retraction Dynamics in COVID-19 Research: Patterns, Reasons, and Implications","Amid the COVID-19 pandemic, while the world sought solutions, some scholars exploited the situation for personal gains through deceptive studies and manipulated data. This paper presents the extent of 400 retracted COVID-19 papers listed by the Retraction Watch database until February 2024. The primary purpose of the research was to analyze journal quality and retraction trends. For all stakeholders involved, such as editors, relevant researchers, and policymakers, evaluating the journal's quality is crucial information since it could help them effectively stop such incidents and their negative effects in the future. The present research results imply that one-fourth of publications were retracted within the first month of their publication, followed by an additional 6\% within six months of publication. One-third of the retractions originated from Q1 journals, with another significant portion coming from Q2 (29.8). A notable percentage of the retracted papers (23.2\%) lacked publishing impact, signifying their publication as conference papers or in journals not indexed by Scopus. An examination of the retraction reasons reveals that one-fourth of retractions were due to numerous causes, mostly in Q2 journals, and another quarter were due to data problems, with the majority happening in Q1 publications. Elsevier retracted 31 of the papers, with the majority published in Q1, followed by Springer (11.5), predominantly in Q2. Retracted papers were mainly associated with the USA, China, and India. In the USA, retractions were primarily from Q1 journals followed by no-impact publications; in China, it was Q1 followed by Q2, and in India, it was Q2 followed by no-impact publications. The study also examined author contributions, revealing that 69.3 were male contributors, with females (30.7) mainly holding middle author positions.",26,3,2024
Maximum Discrepancy Generative Regularization and Non-Negative Matrix Factorization for Single Channel Source Separation,"The idea of adversarial learning of regularization functionals has recently been introduced in the wider context of inverse problems. The intuition behind this method is the realization that it is not only necessary to learn the basic features that make up a class of signals one wants to represent, but also, or even more so, which features to avoid in the representation. In this paper, we will apply this approach to the training of generative models, leading to what we call Maximum Discrepancy Generative Regularization. In particular, we apply this to problem of source separation by means of Non-negative Matrix Factorization (NMF) and present a new method for the adversarial training of NMF bases. We show in numerical experiments, both for image and audio separation, that this leads to a clear improvement of the reconstructed signals, in particular in the case where little or no strong supervision data is available.",26,3,2024
Complexity of Popularity and Dynamics of Within-Game Achievements in Computer Games,"Tasks of different nature and difficulty levels are a part of people's lives. In this context, there is a scientific interest in the relationship between the difficulty of the task and the persistence need to accomplish it. Despite the generality of this problem, some tasks can be simulated in the form of games. In this way, we employ data from a large online platform, called Steam, to analyze games and the performance of their players. More specifically, we investigated persistence in completing tasks based on the proportion of players who accomplished game achievements. Overall, we present five major findings. First, the probability distribution for the number of achievements is log-normal distribution. Second, the distribution of game players also follows a log-normal. Third, most games require neither a very high degree of persistence nor a very low one. Fourth, players also prefer games that demand a certain intermediate persistence. Fifth, the proportion of players as a function of the number of achievements declines approximately exponentially. As both the log-normal and the exponential functions are memoryless, they are mathematical forms that describe random effects arising from the nature of the system. Therefore our first two findings describe random processes of fragmenting achievements and players while the last three provide a quantitative measure of the human preference in the pursuit of challenging, achievable, and justifiable tasks.",26,3,2024
Multimodal Physical Fitness Monitoring (PFM) Framework Based on TimeMAE-PFM in Wearable Scenarios,"Physical function monitoring (PFM) plays a crucial role in healthcare especially for the elderly. Traditional assessment methods such as the Short Physical Performance Battery (SPPB) have failed to capture the full dynamic characteristics of physical function. Wearable sensors such as smart wristbands offer a promising solution to this issue. However, challenges exist, such as the computational complexity of machine learning methods and inadequate information capture. This paper proposes a multi-modal PFM framework based on an improved TimeMAE, which compresses time-series data into a low-dimensional latent space and integrates a self-enhanced attention module. This framework achieves effective monitoring of physical health, providing a solution for real-time and personalized assessment. The method is validated using the NHATS dataset, and the results demonstrate an accuracy of 70.6% and an AUC of 82.20%, surpassing other state-of-the-art time-series classification models.",25,3,2024
Interactive Manipulation and Visualization of 3D Brain MRI for Surgical Training,"In modern medical diagnostics, magnetic resonance imaging (MRI) is an important technique that provides detailed insights into anatomical structures. In this paper, we present a comprehensive methodology focusing on streamlining the segmentation, reconstruction, and visualization process of 3D MRI data. Segmentation involves the extraction of anatomical regions with the help of state-of-the-art deep learning algorithms. Then, 3D reconstruction converts segmented data from the previous step into multiple 3D representations. Finally, the visualization stage provides efficient and interactive presentations of both 2D and 3D MRI data. Integrating these three steps, the proposed system is able to augment the interpretability of the anatomical information from MRI scans according to our interviews with doctors. Even though this system was originally designed and implemented as part of human brain haptic feedback simulation for surgeon training, it can also provide experienced medical practitioners with an effective tool for clinical data analysis, surgical planning and other purposes",24,3,2024
Multi-objective Optimization for Multi-UAV-assisted Mobile Edge Computing,"Recent developments in unmanned aerial vehicles (UAVs) and mobile edge computing (MEC) have provided users with flexible and resilient computing services. However, meeting the computing-intensive and latency-sensitive demands of users poses a significant challenge due to the limited resources of UAVs. To address this challenge, we present a multi-objective optimization approach for multi-UAV-assisted MEC systems. First, we formulate a multi-objective optimization problem \textcolor{b2}{aiming} at minimizing the total task completion delay, reducing the total UAV energy consumption, and maximizing the total amount of offloaded tasks by jointly optimizing task offloading, computation resource allocation, and UAV trajectory control. Since the problem is a mixed-integer non-linear programming (MINLP) and NP-hard problem which is challenging, we propose a joint task offloading, computation resource allocation, and UAV trajectory control (JTORATC) approach to solve the problem. \textcolor{b3}{However, since the decision variables of task offloading, computation resource allocation, and UAV trajectory control are coupled with each other, the original problem is split into three sub-problems, i.e., task offloading, computation resource allocation, and UAV trajectory control, which are solved individually to obtain the corresponding decisions.} \textcolor{b2}{Moreover, the sub-problem of task offloading is solved by using distributed splitting and threshold rounding methods, the sub-problem of computation resource allocation is solved by adopting the Karush-Kuhn-Tucker (KKT) method, and the sub-problem of UAV trajectory control is solved by employing the successive convex approximation (SCA) method.} Simulation results show that the proposed JTORATC has superior performance compared to the other benchmark methods.",23,3,2024
A new modified highly accurate Laplace-Fourier method for linear neutral delay differential equations,"In this article, a new modified Laplace-Fourier method is developed in order to obtain the solutions of linear neutral delay differential equations. The proposed method provides a more accurate solution than the one provided by the pure Laplace method and the original Laplace-Fourier method. We develop and show the crucial modifications of the Laplace-Fourier method. As with the original Laplace-Fourier method, the new modified method combines the Laplace transform method with Fourier series theory. All of the beneficial features from the original Laplace-Fourier method are retained. The modified solution still includes a component that accounts for the terms in the tail of the infinite series, allowing one to obtain more accurate solutions. The Laplace-Fourier method requires us to approximate the formula for the residues with an asymptotic expansion. This is essential to enable us to use the Fourier series results that enable us to account for the tail. The improvement is achieved by deriving a new asymptotic expansion which minimizes the error between the actual residues and those which are obtained from this asymptotic expansion. With both the pure Laplace and improved Laplace-Fourier methods increasing the number of terms in the truncated series obviously increases the accuracy. However, with the pure Laplace, this improvement is small. As we shall show, with the improved Laplace-Fourier method the improvement is significantly larger. We show that the convergence rate of the new modified Laplace-Fourier solution has a remarkable order of convergence $O(N^{-3})$. The validity of the modified technique is corroborated by means of illustrative examples. Comparisons of the solutions of the new modified method with those generated by the pure Laplace method and the original/unmodified Laplace-Fourier approach are presented.",23,3,2024
A point cloud processing method of mmWave radar over automotive scenario,"This paper introduces in detail the effective method of comprehensive target judgment by using radar RA map and point cloud map. Different output of radar can effectively judge the road boundary of target and the relative coordinates of target, avoid the error of output caused by excessive processing information, and greatly improve the processing efficiency of DBSCAN of the measured target.",23,3,2024
EEGDiR: Electroencephalogram denoising network for temporal information storage and global modeling through Retentive Network,"Electroencephalogram (EEG) signals play a pivotal role in clinical medicine, brain research, and neurological disease studies. However, susceptibility to various physiological and environmental artifacts introduces noise in recorded EEG data, impeding accurate analysis of underlying brain activity. Denoising techniques are crucial to mitigate this challenge. Recent advancements in deep learningbased approaches exhibit substantial potential for enhancing the signal-to-noise ratio of EEG data compared to traditional methods. In the realm of large-scale language models (LLMs), the Retentive Network (Retnet) infrastructure, prevalent for some models, demonstrates robust feature extraction and global modeling capabilities. Recognizing the temporal similarities between EEG signals and natural language, we introduce the Retnet from natural language processing to EEG denoising. This integration presents a novel approach to EEG denoising, opening avenues for a profound understanding of brain activities and accurate diagnosis of neurological diseases. Nonetheless, direct application of Retnet to EEG denoising is unfeasible due to the one-dimensional nature of EEG signals, while natural language processing deals with two-dimensional data. To facilitate Retnet application to EEG denoising, we propose the signal embedding method, transforming one-dimensional EEG signals into two dimensions for use as network inputs. Experimental results validate the substantial improvement in denoising effectiveness achieved by the proposed method.",20,3,2024
Hybrid weakly over-penalised symmetric interior penalty method on anisotropic meshes,"In this study, we investigate a hybrid-type anisotropic weakly over-penalised symmetric interior penalty method for the Poisson equation on convex domains. Compared with the well-known hybrid discontinuous Galerkin methods, our approach is simple and easy to implement. Our primary contributions are the proposal of a new scheme and the demonstration of a proof for the consistency term, which allows us to estimate the anisotropic consistency error. The key idea of the proof is to apply the relation between the Raviart--Thomas finite element space and a discontinuous space. In numerical experiments, we compare the calculation results for standard and anisotropic mesh partitions.",19,3,2024
A Semi-automatic Cranial Implant Design Tool Based on Rigid ICP Template Alignment and Voxel Space Reconstruction,"In traumatic medical emergencies, the patients heavily depend on cranioplasty - the craft of neurocranial repair using cranial implants. Despite the improvements made in recent years, the design of a patient-specific implant (PSI) is among the most complex, expensive, and least automated tasks in cranioplasty. Further research in this area is needed. Therefore, we created a prototype application with a graphical user interface (UI) specifically tailored for semi-automatic implant generation, where the users only need to perform high-level actions. A general outline of the proposed implant generation process involves setting an area of interest, aligning the templates, and then creating the implant in voxel space. Furthermore, we show that the alignment can be improved significantly, by only considering clipped geometry in the vicinity of the defect border. The software prototype will be open-sourced atthis https URL",19,3,2024
Computationally efficient orthogonalization for pairwise comparisons method,Orthogonalization is one of few mathematical methods conforming to mathematical standards for approximation. Finding a consistent PC matrix of a given an inconsistent PC matrix is the main goal of a pairwise comparisons method. We introduce an orthogonalization for pairwise comparisons matrix based on a generalized Frobenius inner matrix product. The proposed theory is supported by numerous examples and visualizations.,18,3,2024
Cell agglomeration strategy for cut cells in eXtended discontinuous Galerkin methods,"In this work, a cell agglomeration strategy for the cut cells arising in the extended discontinuous Galerkin (XDG) method is presented. Cut cells are a fundamental aspect of unfitted mesh approaches where complex geometries or interfaces separating sub-domains are embedded into Cartesian background grids to facilitate the mesh generation process. In such methods, arbitrary small cells occur due to the intersections of background cells with embedded geometries and lead to discretization difficulties due to their diminutive sizes. Furthermore, temporal evolutions of these geometries may lead to topological changes across different time steps. Both of these issues, i.e., small-cut cells and topological changes, can be addressed with a cell agglomeration technique. In this work, a comprehensive strategy for the typical issues associated with cell agglomeration in three-dimensional and multiprocessor simulations is provided. The proposed strategy is implemented into the open-source software package BoSSS and tested with 2- and 3-dimensional simulations of immersed boundary flows.",14,3,2024
Global 4D Ionospheric STEC Prediction based on DeepONet for GNSS Rays,"The ionosphere is a vitally dynamic charged particle region in the Earth's upper atmosphere, playing a crucial role in applications such as radio communication and satellite navigation. The Slant Total Electron Contents (STEC) is an important parameter for characterizing wave propagation, representing the integrated electron density along the ray of radio signals passing through the ionosphere. The accurate prediction of STEC is essential for mitigating the ionospheric impact particularly on Global Navigation Satellite Systems (GNSS). In this work, we propose a high-precision STEC prediction model named DeepONet-STEC, which learns nonlinear operators to predict the 4D temporal-spatial integrated parameter for specified ground station - satellite ray path globally. As a demonstration, we validate the performance of the model based on GNSS observation data for global and US-CORS regimes under ionospheric quiet and storm conditions. The DeepONet-STEC model results show that the three-day 72 hour prediction in quiet periods could achieve high accuracy using observation data by the Precise Point Positioning (PPP) with temporal resolution 30s. Under active solar magnetic storm periods, the DeepONet-STEC also demonstrated its robustness and superiority than traditional deep learning methods. This work presents a neural operator regression architecture for predicting the 4D temporal-spatial ionospheric parameter for satellite navigation system performance, which may be further extended for various space applications and beyond.",12,3,2024
Integrated Control of Robotic Arm through EMG and Speech: Decision-Driven Multimodal Data Fusion,"Interactions with electronic devices are changing in our daily lives. The day-to-day development brings curiosity to recent technology and challenges its use. The gadgets are becoming cumbersome, and their usage frustrates a segment of society. In specific scenarios, the user cannot use the modalities because of the challenges that bring in, e.g., the usage of touch screen devices by elderly people. The idea of multimodality provides easy access to devices of daily use through various modalities. In this paper, we suggest a solution that allows the operation of a microcontroller-based device using voice and speech. The model implemented will learn from the user's behavior and decide based on prior knowledge.",11,3,2024
UCINet0: A Machine Learning based Receiver for 5G NR PUCCH Format 0,"Accurate decoding of Uplink Control Information (UCI) on the Physical Uplink Control Channel (PUCCH) is essential for enabling 5G wireless links. This paper explores an AI/ML-based receiver design for PUCCH Format 0. Format 0 signaling encodes the UCI content within the phase of a known base waveform and even supports multiplexing of up to 12 users within the same time-frequency resources. Our first-of-a-kind neural network classifier, which we term UCINet0, is capable of predicting when no user is transmitting on the PUCCH, as well as decoding the UCI content of any number of multiplexed users, up to 12. Inference results with both simulated and hardware-captured field datasets show that the UCINet0 model outperforms conventional DFT-based decoders across all SNR ranges.",10,3,2024
Highly sensitive and efficient 1550 nm photodetector for room temperature operation,"Effective quantum communication requires room temperature (RT) operating single photon sensor with high photo detection efficiency (PDE) at 1550 nm wavelength. The leading class of devices in this segment is avalanche photo detectors operating particularly in the Geiger mode. Often the requirements for RT operation and for the high PDE are in conflict, resulting in a compromised solution. We have developed a device which employs a two-dimensional (2D) semiconductor material on a co-optimized dielectric photonic crystal substrate to simultaneously decrease the dark current by orders of magnitude and increase the PDE. The device is predicted to achieve RT operation with a PDE >99%. Harnessing the high carrier mobility of 2D materials, the device has ~ps jitter time and can be integrated into a large 2D array camera.",20,3,2024
Towards Large-Scale Training of Pathology Foundation Models,"Driven by the recent advances in deep learning methods and, in particular, by the development of modern self-supervised learning algorithms, increased interest and efforts have been devoted to build foundation models (FMs) for medical images. In this work, we present our scalable training pipeline for large pathology imaging data, and a comprehensive analysis of various hyperparameter choices and training techniques for building pathology FMs. We release and make publicly available the first batch of our pathology FMs (this https URL) trained on open-access TCGA whole slide images, a commonly used collection of pathology images. The experimental evaluation shows that our models reach state-of-the-art performance on various patch-level downstream tasks, ranging from breast cancer subtyping to colorectal nuclear segmentation. Finally, to unify the evaluation approaches used in the field and to simplify future comparisons of different FMs, we present an open-source framework (this https URL) designed for the consistent evaluation of pathology FMs across various downstream tasks.",24,3,2024
Automatic Classification of Subjective Time Perception Using Multi-modal Physiological Data of Air Traffic Controllers,"One indicator of well-being can be the person's subjective time perception. In our project ChronoPilot, we aim to develop a device that modulates human subjective time perception. In this study, we present a method to automatically assess the subjective time perception of air traffic controllers, a group often faced with demanding conditions, using their physiological data and eleven state-of-the-art machine learning classifiers. The physiological data consist of photoplethysmogram, electrodermal activity, and temperature data. We find that the support vector classifier works best with an accuracy of 79 % and electrodermal activity provides the most descriptive biomarker. These findings are an important step towards closing the feedback loop of our ChronoPilot-device to automatically modulate the user's subjective time perception. This technological advancement may promise improvements in task management, stress reduction, and overall productivity in high-stakes professions.",28,3,2024
Real-time Lane-wise Traffic Monitoring in Optimal ROIs,"In the US, thousands of Pan, Tilt, and Zoom (PTZ) traffic cameras monitor highway conditions. There is a great interest in using these highway cameras to gather valuable road traffic data to support traffic analysis and decision-making for highway safety and efficient traffic management. However, there are too many cameras for a few human traffic operators to effectively monitor, so a fully automated solution is desired. This paper introduces a novel system that learns the locations of highway lanes and traffic directions from these camera feeds automatically. It collects real-time, lane-specific traffic data continuously, even adjusting for changes in camera angle or zoom. This facilitates efficient traffic analysis, decision-making, and improved highway safety.",29,3,2024
LACS: Learning-Augmented Algorithms for Carbon-Aware Resource Scaling with Uncertain Demand,"Motivated by an imperative to reduce the carbon emissions of cloud data centers, this paper studies the online carbon-aware resource scaling problem with unknown job lengths (OCSU) and applies it to carbon-aware resource scaling for executing computing workloads. The task is to dynamically scale resources (e.g., the number of servers) assigned to a job of unknown length such that it is completed before a deadline, with the objective of reducing the carbon emissions of executing the workload. The total carbon emissions of executing a job originate from the emissions of running the job and excess carbon emitted while switching between different scales (e.g., due to checkpoint and resume). Prior work on carbon-aware resource scaling has assumed accurate job length information, while other approaches have ignored switching losses and require carbon intensity forecasts. These assumptions prohibit the practical deployment of prior work for online carbon-aware execution of scalable computing workload. We propose LACS, a theoretically robust learning-augmented algorithm that solves OCSU. To achieve improved practical average-case performance, LACS integrates machine-learned predictions of job length. To achieve solid theoretical performance, LACS extends the recent theoretical advances on online conversion with switching costs to handle a scenario where the job length is unknown. Our experimental evaluations demonstrate that, on average, the carbon footprint of LACS lies within 1.2% of the online baseline that assumes perfect job length information and within 16% of the offline baseline that, in addition to the job length, also requires accurate carbon intensity forecasts. Furthermore, LACS achieves a 32% reduction in carbon footprint compared to the deadline-aware carbon-agnostic execution of the job.",29,3,2024
Domain Adaptation in Intent Classification Systems: A Review,"Dialogue agents, which perform specific tasks, are part of the long-term goal of NLP researchers to build intelligent agents that communicate with humans in natural language. Such systems should adapt easily from one domain to another to assist users in completing tasks. Researchers have developed a broad range of techniques, objectives, and datasets for intent classification to achieve such systems. Despite the progress in developing intent classification systems (ICS), a systematic review of the progress from a technical perspective is yet to be conducted. In effect, important implementation details of intent classification remain restricted and unclear, making it hard for natural language processing (NLP) researchers to develop new methods. To fill this gap, we review contemporary works in intent classification. Specifically, we conduct a thorough technical review of the datasets, domains, tasks, and methods needed to train the intent classification part of dialogue systems. Our structured analysis describes why intent classification is difficult and studies the limitations to domain adaptation while presenting opportunities for future work.",26,3,2024
Investigation of phonons and magnons in [Ni80Fe20/Au/Co/Au]N multilayers,"The interaction between phonons and magnons is a widely developing topic, especially in the field of acoustic spintronics. To discuss this interaction, it is necessary to observe two different waves (acoustic and spin waves) with the same frequency and wavelength. In the Ni80Fe20/Au/Co/Au system deposited on a silicon substrate, we observe the interaction between spin waves and surface acoustic waves using Brillouin light scattering spectroscopy. As a result, we can selectively control (activate or deactivate) the magnetoelastic interaction between the fundamental spin wave mode and surface acoustic waves by adjusting the magnetostrictive layer thickness in the multilayer. We demonstrate that by adjusting the number of layers in a multilayer structure, we can precisely control the dispersion of surface acoustic waves, with minimal impact on the fundamental spin wave mode",25,3,2024
The Development of a Microcontroller based Smoked Fish Machine,"The development of a microcontroller-based smoked fish machine aims to combine and automate the boiling, smoking, and drying. The machine consists of an Arduino microcontroller, heater, spark gap igniter, stepper motor with motor driver, temperature sensor, exhaust fan, DC converter, relay module, power supply, and controller box. The main mechanism of the smoked fish machine is a stepper motor that automates the process. The PID is used to optimize the performance of controlling the machine's temperature. The machine used galvanized steel and food-grade material to support the heating of the fish, which helps minimize contamination and burns. The typical time of operation of the entire process using the proposed machine is up to 60 minutes.",20,3,2024
FlowMind: Automatic Workflow Generation with LLMs,"The rapidly evolving field of Robotic Process Automation (RPA) has made significant strides in automating repetitive processes, yet its effectiveness diminishes in scenarios requiring spontaneous or unpredictable tasks demanded by users. This paper introduces a novel approach, FlowMind, leveraging the capabilities of Large Language Models (LLMs) such as Generative Pretrained Transformer (GPT), to address this limitation and create an automatic workflow generation system. In FlowMind, we propose a generic prompt recipe for a lecture that helps ground LLM reasoning with reliable Application Programming Interfaces (APIs). With this, FlowMind not only mitigates the common issue of hallucinations in LLMs, but also eliminates direct interaction between LLMs and proprietary data or code, thus ensuring the integrity and confidentiality of information - a cornerstone in financial services. FlowMind further simplifies user interaction by presenting high-level descriptions of auto-generated workflows, enabling users to inspect and provide feedback effectively. We also introduce NCEN-QA, a new dataset in finance for benchmarking question-answering tasks from N-CEN reports on funds. We used NCEN-QA to evaluate the performance of workflows generated by FlowMind against baseline and ablation variants of FlowMind. We demonstrate the success of FlowMind, the importance of each component in the proposed lecture recipe, and the effectiveness of user interaction and feedback in FlowMind.",17,3,2024
DG-RePlAce: A Dataflow-Driven GPU-Accelerated Analytical Global Placement Framework for Machine Learning Accelerators,"Global placement is a fundamental step in VLSI physical design. The wide use of 2D processing element (PE) arrays in machine learning accelerators poses new challenges of scalability and Quality of Results (QoR) for state-of-the-art academic global placers. In this work, we develop DG-RePlAce, a new and fast GPU-accelerated global placement framework built on top of the OpenROAD infrastructure, which exploits the inherent dataflow and datapath structures of machine learning accelerators. Experimental results with a variety of machine learning accelerators using a commercial 12nm enablement show that, compared with RePlAce (DREAMPlace), our approach achieves an average reduction in routed wirelength by 10% (7%) and total negative slack (TNS) by 31% (34%), with faster global placement and on-par total runtimes relative to DREAMPlace. Empirical studies on the TILOS MacroPlacement Benchmarks further demonstrate that post-route improvements over RePlAce and DREAMPlace may reach beyond the motivating application to machine learning accelerators.",16,3,2024
A Big Data Analytics System for Predicting Suicidal Ideation in Real-Time Based on Social Media Streaming Data,"Online social media platforms have recently become integral to our society and daily routines. Every day, users worldwide spend a couple of hours on such platforms, expressing their sentiments and emotional state and contacting each other. Analyzing such huge amounts of data from these platforms can provide a clear insight into public sentiments and help detect their mental status. The early identification of these health condition risks may assist in preventing or reducing the number of suicide ideation and potentially saving people's lives. The traditional techniques have become ineffective in processing such streams and large-scale datasets. Therefore, the paper proposed a new methodology based on a big data architecture to predict suicidal ideation from social media content. The proposed approach provides a practical analysis of social media data in two phases: batch processing and real-time streaming prediction. The batch dataset was collected from the Reddit forum and used for model building and training, while streaming big data was extracted using Twitter streaming API and used for real-time prediction. After the raw data was preprocessed, the extracted features were fed to multiple Apache Spark ML classifiers: NB, LR, LinearSVC, DT, RF, and MLP. We conducted various experiments using various feature-extraction techniques with different testing scenarios. The experimental results of the batch processing phase showed that the features extracted of (Unigram + Bigram) + CV-IDF with MLP classifier provided high performance for classifying suicidal ideation, with an accuracy of 93.47%, and then applied for real-time streaming prediction phase.",19,3,2024
Reclassification of thermal equilibrium phase transitions in thermodynamic limit systems,"For relaxor-ferroelectrics and relaxor-ferromagnets, Ehrenfest classification of phase transitions based on the discontinuity of entropy or specific heat etc. with temperature (T) gives no transition that contradicts the measured order parameter, i.e. spontaneous polarization and magnetization, at low temperatures, while Landau classification based on the minimum derivative of the order parameter to T raises the question about the relationships between the phase transition and the specific heat peak above and near the transition temperature. Here, based on the free energy of the thermodynamic limit systems with T when the external field tends to 0, thermal equilibrium phase transitions are reclassified into three categories, i.e. 1) First-order phase transition, 2) Second-order phase transition, and 3) Diffuse phase transition, naturally giving the the relation of the phase transition to the specific heat peak. Deep analyses indicate that the classification of this paper is based on both the long-range and short-range correlation orders of the systems, while the Ehrenfest's or Landau's independently on the short-range or long-range order, respectively.",17,3,2024
Melting and polymorphic transitions in liquid,"Review of the author's data, partly still unpublished, on studying of liquid tellurium and cesium are given. No proofs indicating phase transitions in liquids were found. New developments in studying the liquid-liquid phase transition are briefly described. Some relevant ethical problems are exposed in the bibliography section.",5,3,2024
Advancing Speech Translation: A Corpus of Mandarin-English Conversational Telephone Speech,"This paper introduces a set of English translations for a 123-hour subset of the CallHome Mandarin Chinese data and the HKUST Mandarin Telephone Speech data for the task of speech translation. Paired source-language speech and target-language text is essential for training end-to-end speech translation systems and can provide substantial performance improvements for cascaded systems as well, relative to training on more widely available text data sets. We demonstrate that fine-tuning a general-purpose translation model to our Mandarin-English conversational telephone speech training set improves target-domain BLEU by more than 8 points, highlighting the importance of matched training data.",25,3,2024
Fractional-Diffraction-Optics Cauchy Problem: Resolvent-Function Solution of the Matrix Integral Equation,"The fractional diffraction optics theory has been elaborated using the Green function technique. The optics-fractional equation describing the diffraction X-ray scattering by imperfect crystals has been derived as the fractional matrix integral Fredholm--Volterra equation of the second kind. In the paper, to solve the Cauchy problems, the Liouville--Neumann-type series formalism has been used to build up the matrix Resolvent-function solution. In the case when the imperfect crystal-lattice elastic displacement field is the linear function $f({\bf R}) = a x+b$, $a, b = const,$ the explicit solution of the diffraction-optics Cauchy problem has been obtained and analyzed for arbitrary fractional-order-parameter $\alpha$, $\alpha\in (0, 1].$",20,3,2024
Phishing Website Detection Using a Combined Model of ANN and LSTM,"In this digital era, our lives highly depend on the internet and worldwide technology. Wide usage of technology and platforms of communication makes our lives better and easier. But on the other side it carries out some security issues and cruel activities, phishing is one activity of these cruel activities. It is a type of cybercrime, which has the purpose of stealing the personal information of the computer user, and enterprises, which carry out fake websites that are the copy of the original websites. The attackers used personal information like account IDs, passwords, and usernames for the purpose of some fraudulent activities against the user of the computer. To overcome this problem researchers focused on the machine learning and deep learning approaches. In our study, we are going to use machine learning and deep learning models to identify the fake web pages on the secondary dataset.",24,3,2024
Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations,"There is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) o get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be ine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.",23,3,2024
Motivated exposition of combinatorial Nullstellensatz,"In this expository note we show how combinatorial Nullstellensatz by N. Alon naturally appears in solutions of elementary problems. Simple ideas gradually and naturally appear in such solutions, thus bringing a reader to generalizations. The note is accessible to mathematicians not specialized in the area, and to students familiar with polynomials.",23,3,2024
Compressed Federated Reinforcement Learning with a Generative Model,"Reinforcement learning has recently gained unprecedented popularity, yet it still grapples with sample inefficiency. Addressing this challenge, federated reinforcement learning (FedRL) has emerged, wherein agents collaboratively learn a single policy by aggregating local estimations. However, this aggregation step incurs significant communication costs. In this paper, we propose CompFedRL, a communication-efficient FedRL approach incorporating both \textit{periodic aggregation} and (direct/error-feedback) compression mechanisms. Specifically, we consider compressed federated $Q$-learning with a generative model setup, where a central server learns an optimal $Q$-function by periodically aggregating compressed $Q$-estimates from local agents. For the first time, we characterize the impact of these two mechanisms (which have remained elusive) by providing a finite-time analysis of our algorithm, demonstrating strong convergence behaviors when utilizing either direct or error-feedback compression. Our bounds indicate improved solution accuracy concerning the number of agents and other federated hyperparameters while simultaneously reducing communication costs. To corroborate our theory, we also conduct in-depth numerical experiments to verify our findings, considering Top-$K$ and Sparsified-$K$ sparsification operators.",26,3,2024
Flow-Acoustics: Theory and Benchmarking,"The urgent need for transitioning to green energy solutions, particularly in the context of house heating and urban redensification, has brought the issue of fan noise aeroacoustics investigations to the forefront. As societies worldwide strive to mitigate climate change and reduce carbon emissions, adopting sustainable heating technologies such as air heat pumps has gained significant traction. In Germany, renowned for its commitment to environmental sustainability, the ""TA Lärm"" regulations, derived from the ""Bundes-Immissionsschutzgesetz,"" impose stringent limits on noise levels both inside and outside buildings across various applications. These regulations delineate permissible noise levels during daytime (6 AM to 10 PM) and nighttime (10 PM to 6 AM), with particular emphasis on protecting residential areas with low noise limits. Moreover, the noise limits prescribed for indoor environments are even more stringent. Given the necessity of maintaining acoustic comfort and quality of life, compliance with these regulations necessitates meticulous attention to noise generation sources, especially those associated with heating and ventilation systems. Consequently, understanding and mitigating fan noise through aeroacoustic investigations is essential to ensure the successful adoption and integration of green energy solutions in residential and urban settings. In the following, an experimental benchmark for a low-pressure rise axial fan (FAN-01) is presented, and several prediction methods of the sound pressure and sound power are evaluated.",26,3,2024
Parallel Implementations Assessment of a Spatial-Spectral Classifier for Hyperspectral Clinical Applications,"Hyperspectral (HS) imaging presents itself as a non-contact, non-ionizing and non-invasive technique, proven to be suitable for medical diagnosis. However, the volume of information contained in these images makes difficult providing the surgeon with information about the boundaries in real-time. To that end, High-Performance-Computing (HPC) platforms become necessary. This paper presents a comparison between the performances provided by five different HPC platforms while processing a spatial-spectral approach to classify HS images, assessing their main benefits and drawbacks. To provide a complete study, two different medical applications, with two different requirements, have been analyzed. The first application consists of HS images taken from neurosurgical operations; the second one presents HS images taken from dermatological interventions. While the main constraint for neurosurgical applications is the processing time, in other environments, as the dermatological one, other requirements can be considered. In that sense, energy efficiency is becoming a major challenge, since this kind of applications are usually developed as hand-held devices, thus depending on the battery capacity. These requirements have been considered to choose the target platforms: on the one hand, three of the most powerful Graphic Processing Units (GPUs) available in the market; and, on the other hand, a low-power GPU and a manycore architecture, both specifically thought for being used in battery-dependent environments.",28,3,2024
"Optimizing Performance on Trinity Utilizing Machine Learning, Proxy Applications and Scheduling Priorities","The sheer number of nodes continues to increase in todays supercomputers, the first half of Trinity alone contains more than 9400 compute nodes. Since the speed of todays clusters are limited by the slowest nodes, it more important than ever to identify slow nodes, improve their performance if it can be done, and assure minimal usage of slower nodes during performance critical runs. This is an ongoing maintenance task that occurs on a regular basis and, therefore, it is important to minimize the impact upon its users by assessing and addressing slow performing nodes and mitigating their consequences while minimizing down time. These issues can be solved, in large part, through a systematic application of fast running hardware assessment tests, the application of Machine Learning, and making use of performance data to increase efficiency of large clusters. Proxy applications utilizing both MPI and OpenMP were developed to produce data as a substitute for long runtime applications to evaluate node performance. Machine learning is applied to identify underperforming nodes, and policies are being discussed to both minimize the impact of underperforming nodes and increase the efficiency of the system. In this paper, I will describe the process used to produce quickly performing proxy tests, consider various methods to isolate the outliers, and produce ordered lists for use in scheduling to accomplish this task.",16,3,2024
InfoCon: Concept Discovery with Generative and Discriminative Informativeness,"We focus on the self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. We propose that the decision to conceptualize a physical procedure should not depend on how we name it (semantics) but rather on the significance of the informativeness in its representation regarding the low-level physical state and state changes. We model manipulation concepts (discrete symbols) as generative and discriminative goals and derive metrics that can autonomously link them to meaningful sub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ a trainable codebook containing encodings (concepts) capable of synthesizing the end-state of a sub-trajectory given the current state (generative informativeness). Moreover, the encoding corresponding to a particular sub-trajectory should differentiate the state within and outside it and confidently predict the subsequent action based on the gradient of its discriminative score (discriminative informativeness). These metrics, which do not rely on human annotation, can be seamlessly integrated into a VQ-VAE framework, enabling the partitioning of demonstrations into semantically consistent sub-trajectories, fulfilling the purpose of discovering manipulation concepts and the corresponding sub-goal (key) states. We evaluate the effectiveness of the learned concepts by training policies that utilize them as guidance, demonstrating superior performance compared to other baselines. Additionally, our discovered manipulation concepts compare favorably to human-annotated ones while saving much manual effort.",14,3,2024
Thermal Crosstalk Modelling and Compensation Methods for Programmable Photonic Integrated Circuits,"Photonic integrated circuits play an important role in the field of optical computing, promising faster and more energy-efficient operations compared to their digital counterparts. This advantage stems from the inherent suitability of optical signals to carry out matrix multiplication. However, even deterministic phenomena such as thermal crosstalk make precise programming of photonic chips a challenging task. Here, we train and experimentally evaluate three models incorporating varying degrees of physics intuition to predict the effect of thermal crosstalk in different locations of an integrated programmable photonic mesh. We quantify the effect of thermal crosstalk by the resonance wavelength shift in the power spectrum of a microring resonator implemented in the chip, achieving modelling errors <0.5 pm. We experimentally validate the models through compensation of the crosstalk-induced wavelength shift. Finally, we evaluate the generalization capabilities of one of the models by employing it to predict and compensate for the effect of thermal crosstalk for parts of the chip it was not trained on, revealing root-mean-square-errors of <2.0 pm.",19,3,2024
Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data,"Ambient diffusion is a recently proposed framework for training diffusion models using corrupted data. Both Ambient Diffusion and alternative SURE-based approaches for learning diffusion models from corrupted data resort to approximations which deteriorate performance. We present the first framework for training diffusion models that provably sample from the uncorrupted distribution given only noisy training data, solving an open problem in this space. Our key technical contribution is a method that uses a double application of Tweedie's formula and a consistency loss function that allows us to extend sampling at noise levels below the observed data noise. We also provide further evidence that diffusion models memorize from their training sets by identifying extremely corrupted images that are almost perfectly reconstructed, raising copyright and privacy concerns. Our method for training using corrupted samples can be used to mitigate this problem. We demonstrate this by fine-tuning Stable Diffusion XL to generate samples from a distribution using only noisy samples. Our framework reduces the amount of memorization of the fine-tuning dataset, while maintaining competitive performance.",20,3,2024
Calculation of DC Stark Resonances for the Ammonia Molecule,"A model potential previously developed for the ammonia molecule is treated in a single-center partial-wave approximation in analogy with a self-consistent field method developed by Moccia. The latter was used in a number of collision studies. The model potential is used to calculate dc Stark resonance parameters, i.e., resonance positions and shifts within a single-center partial wave expansion, using the exterior complex scaling method for the radial coordinate. Three molecular valence orbitals are investigated for fields along the three Cartesian coordinates, i.e., along the molecular axis and in two perpendicular directions. The work extends previous work on the planar-geometry water molecule for which non-monotonic shifts were observed. We find such non-monotonic shifts for fields along the molecular axis. For perpendicular fields we report the splitting of the 1e orbitals into a fast- and a slow-ionizing orbital.",28,3,2024
Dynamic renormalization of scalar active field theories,"We study Active Model B+, a scalar field theory extending the paradigmatic Model B for equilibrium coexistence through including terms that do not arise from an underlying free energy functional and thus break detailed balance. In the first part of the manuscript, we provide a pedagogical and self-contained introduction to one-loop dynamic renormalization. We then address the technical challenge of complex vertex functions through developing a symbolic computer algebra code that allows us to obtain the graphical corrections of model parameters. We argue that the additional terms of Active Model B+ imply the generation of, potentially relevant, higher-order terms; strongly restricting the parameter regime in which we can apply a perturbative renormalization scheme. Moreover, we elucidate the role of the cubic coefficient, which, in contrast to passive Model B, is incessantly generated by the new terms. Analyzing its behavior with and without field shift near the Wilson-Fisher fixed point, we find that additional fixed points in the one-loop flow equations are likely artifacts. Additionally, we characterize the renormalization flow of perturbatively accessible field theories derived from Active Model B+.",11,3,2024
Targeted aspect-based emotion analysis to detect opportunities and precaution in financial Twitter messages,"Microblogging platforms, of which Twitter is a representative example, are valuable information sources for market screening and financial models. In them, users voluntarily provide relevant information, including educated knowledge on investments, reacting to the state of the stock markets in real-time and, often, influencing this state. We are interested in the user forecasts in financial, social media messages expressing opportunities and precautions about assets. We propose a novel Targeted Aspect-Based Emotion Analysis (TABEA) system that can individually discern the financial emotions (positive and negative forecasts) on the different stock market assets in the same tweet (instead of making an overall guess about that whole tweet). It is based on Natural Language Processing (NLP) techniques and Machine Learning streaming algorithms. The system comprises a constituency parsing module for parsing the tweets and splitting them into simpler declarative clauses; an offline data processing module to engineer textual, numerical and categorical features and analyse and select them based on their relevance; and a stream classification module to continuously process tweets on-the-fly. Experimental results on a labelled data set endorse our solution. It achieves over 90% precision for the target emotions, financial opportunity, and precaution on Twitter. To the best of our knowledge, no prior work in the literature has addressed this problem despite its practical interest in decision-making, and we are not aware of any previous NLP nor online Machine Learning approaches to TABEA.",30,3,2024
Identifying Banking Transaction Descriptions via Support Vector Machine Short-Text Classification Based on a Specialized Labelled Corpus,"Short texts are omnipresent in real-time news, social network commentaries, etc. Traditional text representation methods have been successfully applied to self-contained documents of medium size. However, information in short texts is often insufficient, due, for example, to the use of mnemonics, which makes them hard to classify. Therefore, the particularities of specific domains must be exploited. In this article we describe a novel system that combines Natural Language Processing techniques with Machine Learning algorithms to classify banking transaction descriptions for personal finance management, a problem that was not previously considered in the literature. We trained and tested that system on a labelled dataset with real customer transactions that will be available to other researchers on request. Motivated by existing solutions in spam detection, we also propose a short text similarity detector to reduce training set size based on the Jaccard distance. Experimental results with a two-stage classifier combining this detector with a SVM indicate a high accuracy in comparison with alternative approaches, taking into account complexity and computing time. Finally, we present a use case with a personal finance application, CoinScrap, which is available at Google Play and App Store.",29,3,2024
Assessing wind energy's potential for Kentucky,"Recently, there has been a push by countries to diversify their energy mix considering various factors. In this regard, there have been several studies conducted to assess the potential for using sources such as wind and solar to generate supplemental energy to the already present energy generation setup. In this regard, this study explores the potential of wind for the Commonwealth of Kentucky. To perform this study, wind data was sourced for eight locations across Kentucky from the publicly accessible wind speed information present at Weatherunderground for the years 2020-2021. An analysis was performed concerning the seasonal, monthly and hourly variation in the wind speed so as to identify the expected times of sufficient wind energy generation. Moreover, a comparison of the collected data was performed with data from a home-based weather station and a deployed wind turbine as well to validate the variation pattern of the publicly sourced data. Finally, in order to investigate the variation patterns of wind and solar energy sources, a comparative analysis was also performed using data from a solar power generation plant in Kentucky. It was observed that a seasonal and monthly complemetarity was observed between the wind and solar. However, when considering daily patterns, the wind was found to follow solar generation with an offset. While further research is required, this analysis indicates that it is possible to deploy wind energy power generation projects in the Commonwealth of Kentucky. The seasonal complementary behavior of wind and solar can be used along with battery storage in conjunction with natural gas to provide a diversified electricity generation portfolio.",29,3,2024
FewUser: Few-Shot Social User Geolocation via Contrastive Learning,"To address the challenges of scarcity in geotagged data for social user geolocation, we propose FewUser, a novel framework for Few-shot social User geolocation. We incorporate a contrastive learning strategy between users and locations to improve geolocation performance with no or limited training data. FewUser features a user representation module that harnesses a pre-trained language model (PLM) and a user encoder to process and fuse diverse social media inputs effectively. To bridge the gap between PLM's knowledge and geographical data, we introduce a geographical prompting module with hard, soft, and semi-soft prompts, to enhance the encoding of location information. Contrastive learning is implemented through a contrastive loss and a matching loss, complemented by a hard negative mining strategy to refine the learning process. We construct two datasets TwiU and FliU, containing richer metadata than existing benchmarks, to evaluate FewUser and the extensive experiments demonstrate that FewUser significantly outperforms state-of-the-art methods in both zero-shot and various few-shot settings, achieving absolute improvements of 26.95\% and \textbf{41.62\%} on TwiU and FliU, respectively, with only one training sample per class. We further conduct a comprehensive analysis to investigate the impact of user representation on geolocation performance and the effectiveness of FewUser's components, offering valuable insights for future research in this area.",28,3,2024
The Comparison of Translationese in Machine Translation and Human Transation in terms of Translation Relations,"This study explores the distinctions between neural machine translation (NMT) and human translation (HT) through the lens of translation relations. It benchmarks HT to assess the translation techniques produced by an NMT system and aims to address three key research questions: the differences in overall translation relations between NMT and HT, how each utilizes non-literal translation techniques, and the variations in factors influencing their use of specific non-literal techniques. The research employs two parallel corpora, each spanning nine genres with the same source texts with one translated by NMT and the other by humans. Translation relations in these corpora are manually annotated on aligned pairs, enabling a comparative analysis that draws on linguistic insights, including semantic and syntactic nuances such as hypernyms and alterations in part-of-speech tagging. The results indicate that NMT relies on literal translation significantly more than HT across genres. While NMT performs comparably to HT in employing syntactic non-literal translation techniques, it falls behind in semantic-level performance.",27,3,2024
How Does Message Passing Improve Collaborative Filtering?,"Collaborative filtering (CF) has exhibited prominent results for recommender systems and been broadly utilized for real-world applications. A branch of research enhances CF methods by message passing used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF. They assume that message passing helps CF methods in a manner akin to its benefits for graph-based learning tasks in general. However, even though message passing empirically improves CF, whether or not this assumption is correct still needs verification. To address this gap, we formally investigate why message passing helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate. With our curated ablation studies and theoretical analyses, we discover that (1) message passing improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) message passing usually helps low-degree nodes more than high-degree nodes. Utilizing these novel findings, we present Test-time Aggregation for CF, namely TAG-CF, a test-time augmentation framework that only conducts message passing once at inference time. The key novelty of TAG-CF is that it effectively utilizes graph knowledge while circumventing most of notorious computational overheads of message passing. Besides, TAG-CF is extremely versatile can be used as a plug-and-play module to enhance representations trained by different CF supervision signals. Evaluated on six datasets, TAG-CF consistently improves the recommendation performance of CF methods without graph by up to 39.2% on cold users and 31.7% on all users, with little to no extra computational overheads.",27,3,2024
Field method approach in the factorization of nonlinear second order differential equations,"In this paper, the general solution of second-order nonlinear differential equations of Lienard type is obtained within the nonlinear factorization method of Rosu and Cornejo-Perez by the so-called field method approach. This method is based on writing the factorization conditions in the dynamical systems form and requires to assume that the intermediate function Phi that occurs in the factorization be dependent not only on the dependent variable of the nonlinear equation, but also on the independent variable. The method is applied to several cases of Lienard type equations which are written in a commutative factorization form and their general solutions are obtained by solving Bernoulli differential equations.",27,3,2024
Coupling Phase Field Crystal and Field Dislocation Mechanics for a consistent description of dislocation structure and elasticity,"This work addresses differences in predicted elastic fields created by dislocations either by the Phase Field Crystal (PFC) model, or by static Field Dislocation Mechanics (FDM). The PFC order parameter describes the topological content of the lattice, but it fails to correctly capture the elastic distortion. In contrast, static FDM correctly captures the latter but requires input about defect cores. The case of a dislocation dipole in two dimensional, isotropic, elastic medium is studied, and a weak coupling is introduced between the two models. The PFC model produces compact and stable dislocation cores, free of any singularity, i.e., diffuse. The PFC predicted dislocation density field (a measure of the topological defect content) is used as the source (input) for the static FDM problem. This coupling allows a critical analysis of the relative role played by configurational (from PFC) and elastic (from static FDM) fields in the theory, and of the consequences of the lack of elastic relaxation in the diffusive evolution of the PFC order parameter.",25,3,2024
Advancing Extrapolative Predictions of Material Properties through Learning to Learn,"Recent advancements in machine learning have showcased its potential to significantly accelerate the discovery of new materials. Central to this progress is the development of rapidly computable property predictors, enabling the identification of novel materials with desired properties from vast material spaces. However, the limited availability of data resources poses a significant challenge in data-driven materials research, particularly hindering the exploration of innovative materials beyond the boundaries of existing data. While machine learning predictors are inherently interpolative, establishing a general methodology to create an extrapolative predictor remains a fundamental challenge, limiting the search for innovative materials beyond existing data boundaries. In this study, we leverage an attention-based architecture of neural networks and meta-learning algorithms to acquire extrapolative generalization capability. The meta-learners, experienced repeatedly with arbitrarily generated extrapolative tasks, can acquire outstanding generalization capability in unexplored material spaces. Through the tasks of predicting the physical properties of polymeric materials and hybrid organic--inorganic perovskites, we highlight the potential of such extrapolatively trained models, particularly with their ability to rapidly adapt to unseen material domains in transfer learning scenarios.",25,3,2024
Linear Cross-document Event Coreference Resolution with X-AMR,"Event Coreference Resolution (ECR) as a pairwise mention classification task is expensive both for automated systems and manual annotations. The task's quadratic difficulty is exacerbated when using Large Language Models (LLMs), making prompt engineering for ECR prohibitively costly. In this work, we propose a graphical representation of events, X-AMR, anchored around individual mentions using a \textbf{cross}-document version of \textbf{A}bstract \textbf{M}eaning \textbf{R}epresentation. We then linearize the ECR with a novel multi-hop coreference algorithm over the event graphs. The event graphs simplify ECR, making it a) LLM cost-effective, b) compositional and interpretable, and c) easily annotated. For a fair assessment, we first enrich an existing ECR benchmark dataset with these event graphs using an annotator-friendly tool we introduce. Then, we employ GPT-4, the newest LLM by OpenAI, for these annotations. Finally, using the ECR algorithm, we assess GPT-4 against humans and analyze its limitations. Through this research, we aim to advance the state-of-the-art for efficient ECR and shed light on the potential shortcomings of current LLMs at this task. Code and annotations: \url{this https URL}",25,3,2024
Transformer-based Joint Modelling for Automatic Essay Scoring and Off-Topic Detection,"Automated Essay Scoring (AES) systems are widely popular in the market as they constitute a cost-effective and time-effective option for grading systems. Nevertheless, many studies have demonstrated that the AES system fails to assign lower grades to irrelevant responses. Thus, detecting the off-topic response in automated essay scoring is crucial in practical tasks where candidates write unrelated text responses to the given task in the question. In this paper, we are proposing an unsupervised technique that jointly scores essays and detects off-topic essays. The proposed Automated Open Essay Scoring (AOES) model uses a novel topic regularization module (TRM), which can be attached on top of a transformer model, and is trained using a proposed hybrid loss function. After training, the AOES model is further used to calculate the Mahalanobis distance score for off-topic essay detection. Our proposed method outperforms the baseline we created and earlier conventional methods on two essay-scoring datasets in off-topic detection as well as on-topic scoring. Experimental evaluation results on different adversarial strategies also show how the suggested method is robust for detecting possible human-level perturbations.",24,3,2024
Optimal path for Biomedical Text Summarization Using Pointer GPT,"Biomedical text summarization is a critical tool that enables clinicians to effectively ascertain patient status. Traditionally, text summarization has been accomplished with transformer models, which are capable of compressing long documents into brief summaries. However, transformer models are known to be among the most challenging natural language processing (NLP) tasks. Specifically, GPT models have a tendency to generate factual errors, lack context, and oversimplify words. To address these limitations, we replaced the attention mechanism in the GPT model with a pointer network. This modification was designed to preserve the core values of the original text during the summarization process. The effectiveness of the Pointer-GPT model was evaluated using the ROUGE score. The results demonstrated that Pointer-GPT outperformed the original GPT model. These findings suggest that pointer networks can be a valuable addition to EMR systems and can provide clinicians with more accurate and informative summaries of patient medical records. This research has the potential to usher in a new paradigm in EMR systems and to revolutionize the way that clinicians interact with patient medical records.",22,3,2024
Machine-Learning Based Selection and Synthesis of Candidate Metal-Insulator Transition Metal Oxides,"The discovery of materials that exhibit a metal-insulator transition (MIT) is key to the development of multiple types of novel efficient microelectronic and optoelectronic devices. However, identifying MIT materials is challenging due to a combination of high computational cost of electronic structure calculations needed to understand their mechanism, the mechanisms' complexity, and the labor-intensive experimental validation process. To that end, we use a machine learning classification model to rapidly screen a high-throughput crystal structure database to identify candidate compounds exhibiting thermally-driven MITs. We focus on three candidate oxides, Ca$_2$Fe$_3$O$_8$, CaCo$_2$O$_4$, and CaMn$_2$O$_4$, and identify their MIT mechanism using high-fidelity density functional theory calculations. Then, we provide a probabilistic estimate of which synthesis reactions may lead to their realization. Our approach couples physics-informed machine learning, density functional theory calculations, and machine learning-suggested synthesis to reduce the time to discovery and synthesis of new technologically relevant materials.",21,3,2024
Algorithm for AGC index management against crowded radio environment,"This paper describes a receiver that uses an innovative method to predict, according to history of receiver operating metrics (packet lost/well received), the optimum automatic gain control (AGC) index or most appropriate variable gain range to be used for next packet reception, anticipating an interferer appearing during the payload reception. This allows the receiver to have higher immunity to interferers even if they occur during the gain frozen payload reception period whilst still ensuring an optimum sensitivity level. As a result, the method allows setting the receiver gain to get an optimum trade-off between reception sensitivity and random interferer immunity.",19,3,2024
Analysis of growth of silicon thin films on textured and non-textured surface,"Hydrogenated amorphous silicon alloy films are generally deposited by radio frequency plasma enhanced chemical vapor deposition (RF PECVD) technique on various types of substrates. Generally it is assumed that film quality remains unchanged when deposited on textured or non-textured substrates. Here we analyzed the difference in growth of thin film silicon layers when deposited in a textured and a non-textured surface. In this investigation characteristics of two solar cells were compared, where one cell was prepared on a textured surface ( Cell-A) while the other prepared on a non-textured surface (Cell-B). Defect analysis of the devices were carried out by simulation and device modeling. It shows that the intrinsic film deposited on a textured surface was more defective ($2.4\times 10^{17}$ cm$^{-3}$) than that deposited on a flat surface ($3.2\times 10^{16}$ cm$^{-3}$). Although the primary differences in these two cells were thickness of the active layer and nature of surface texturing, the simulation results show that thin film deposited on a textured surface may acquire an increased defect density than that deposited on a flat surface. Lower effective flux density of $SiH_{3}$ precursors on the textured surface can be one of the reasons for higher defect density in the film deposited on textured surface. An Improved light coupling can be achieved by using a thinner doped window layer. By changing the thickness from 15 nm to 3 nm, the short circuit current density increased from 16.4 mA/cm$^{2}$ to 20.96 mA/cm$^{2}$ and efficiency increased from $9.4\%$ to $12.32\%$.",14,3,2024
QSPR Analysis with Curvilinear Regression Modeling and Temperature-based Topological Indices,"Establishing quantitative correlations between various molecular properties and chemical structures is of great technological importance for environmental and medical aspects. These approaches are referred to as Quantitative Structure-Property Relationships (QSPR), which relate the physicochemical or thermodynamic properties of compounds to their structures. The main goal of QSPR studies is to find a mathematical relationship between the property of interest and several molecular descriptors derived from the structure of the molecule. Topological indices are the molecular descriptors that characterize the formation of chemical compounds and predict certain physicochemical properties. In this study, the QSPR models are designed using certain temperature-based topological indices such as the sum connectivity temperature index, product connectivity temperature index, F-temperature index, and symmetric division temperature index to predict the thermodynamic properties, such as enthalpies of formation ($\Delta H^{0}_{f}$ \hspace{1mm} liquid), enthalpies of combustion ($\Delta H^{0}_{C}$ \hspace{1mm} liquid), and enthalpies of vaporization ($\Delta H^{0}_{vap}$ \hspace{1mm} gas) of monocarboxylic acids ($C_2H_{4}O_{2}$ - $C_{20}H_{40}O_{2}$). The relationship analysis between thermodynamic properties and topological indices is done using linear, quadratic, and cubic equations of a curvilinear regression model. These regression models are then compared.",13,3,2024
Software-defined optical networking applications enabled by programmable integrated photonics,"Data center networks are experiencing unprecedented exponential growth, mostly driven by the continuous computing demands in machine learning and artificial intelligence algorithms. Within this realm, optical networking offers numerous advantages, including low latency, energy efficiency, and bandwidth transparency, positioning it as a compelling alternative to its electronic counterparts. In this work, we showcase a range of software-defined optical networking applications deployed on a general-purpose programmable integrated photonic processor. Leveraging graph-based theory, we experimentally demonstrate dynamic optical interconnects, circuit switching, and multicasting on the same photonic platform, yielding remarkable results in terms of crosstalk and reconfiguration speed. Our approach harnesses the benefits of reconfigurability and reliability, paving the way for a new generation of high-performance optical devices tailored for data center and computing clusters.",4,3,2024
A new SU(2/1) supergroup with determinant 1 explains many mysteries of the weak interactions,"Taken as a classification paradigm completing the standard model, a new compact form of the SU(2/1) supergroup explains many mysterious properties of the weak interactions: the maximal breaking of parity, the fractional charges of the quarks, the cancellation of the quantum field theory anomalies, and ties together the existence of the right neutrinos and of the heavier Fermions. This compact supergroup is constructed by exponentiating the matrices representing the leptons and the quarks which form a semi-direct sum of Kac modules of the real superalgebra su(2/1,R) such that the overall trace of the $U(1)$ weak-hypercharge $Y$ vanishes. Remarkably, all the elements of this supergroup have Berezinian 1 and determinant 1. In practice, $Tr(Y)=0$ simply means that the electric charge of the hydrogen atom is zero.",25,3,2024
Going Forward-Forward in Distributed Deep Learning,"This paper introduces a new approach in distributed deep learning, utilizing Geoffrey Hinton's Forward-Forward (FF) algorithm to enhance the training of neural networks in distributed computing environments. Unlike traditional methods that rely on forward and backward passes, the FF algorithm employs a dual forward pass strategy, significantly diverging from the conventional backpropagation process. This novel method aligns more closely with the human brain's processing mechanisms, potentially offering a more efficient and biologically plausible approach to neural network training. Our research explores the implementation of the FF algorithm in distributed settings, focusing on its capability to facilitate parallel training of neural network layers. This parallelism aims to reduce training times and resource consumption, thereby addressing some of the inherent challenges in current distributed deep learning systems. By analyzing the effectiveness of the FF algorithm in distributed computing, we aim to demonstrate its potential as a transformative tool in distributed deep learning systems, offering improvements in training efficiency. The integration of the FF algorithm into distributed deep learning represents a significant step forward in the field, potentially revolutionizing the way neural networks are trained in distributed environments.",30,3,2024
Fractal Calculus to Derive Fractal Frenet Equations for Fractal Curves,"This paper introduces the concept of Fractal Frenet equations, a set of differential equations used to describe the behavior of vectors along fractal curves. The study explores the analogue of arc length for fractal curves, providing a measure to quantify their length. It also discusses fundamental mathematical constructs, such as the analogue of the unit tangent vector, which indicates the curve's direction at different points, and the analogue of curvature vector or fractal curvature vector, which characterizes its curvature at various locations. The concept of torsion, describing the twisting and turning of fractal curves in three-dimensional space, is also explored. Specific examples, like the fractal helix and the fractal snowflake, illustrate the application and significance of the Fractal Frenet equations.",30,3,2024
On the covariant coefficients of geodesic sprays on Finsler manifolds,"For a Finsler metric $F$, we introduce the notion of $F$-covariant coefficients $H_i$ of the geodesic spray of $F$ (Def. 3.1). We study some geometric consequences concerning the objects $H_i$. If the $F$-covariant coefficients $H_i$ are written in the form $H_i={\dot{\partial}}_iH$, for some smooth function $H$ on ${\mathcal T\hspace{-1pt}M}$, positively 3-homogeneous in y, then $H$ is called spray scalar or simply $S$-scalar. We prove that if the $S$-scalar exists, then it is of the form $H=\frac{1}{12}\,y^i\partial_iF^2$ and this expression is unique up to a function of position only. We prove also that on a Finsler maifold $(M,F)$, the $S$-scalar $H$ exists if and only if $(M,F)$ is dually flat. Generally, the $n^3$ functions $H^h_{ij}$ resulting from the $F$-covariant coefficients do not form a linear connection. We find out that in the case of projectively flat metrics, the $n^3$ functions $H^h_{ij}$ are coefficients of a linear connection. We introduce two new special Finsler spaces, namely, the $H$-Berwald and the $H$-Landsberg spaces and show that every $H$-Berwald metric is $H$-Landsbergian but the converse is not necessarily true. Also, we study the $F$-covariant coefficients $H_i$ of projectivly flat and dually flat spherically symmetric Finsler metrics and provide a solution of the ""$H$-unicorn"" Landsberg problem. Finally, we give some examples of $H$-Berwald and $H$-Landsberg metrics and an example of $H$-Landsberg metric which is not $H$-Berwaldian.",18,3,2024
An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting,"As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available atthis https URL.",25,3,2024
AD-NEv++ : The multi-architecture neuroevolution-based multivariate anomaly detection framework,"Anomaly detection tools and methods enable key analytical capabilities in modern cyberphysical and sensor-based systems. Despite the fast-paced development in deep learning architectures for anomaly detection, model optimization for a given dataset is a cumbersome and time-consuming process. Neuroevolution could be an effective and efficient solution to this problem, as a fully automated search method for learning optimal neural networks, supporting both gradient and non-gradient fine tuning. However, existing frameworks incorporating neuroevolution lack of support for new layers and architectures and are typically limited to convolutional and LSTM layers. In this paper we propose AD-NEv++, a three-stage neuroevolution-based method that synergically combines subspace evolution, model evolution, and fine-tuning. Our method overcomes the limitations of existing approaches by optimizing the mutation operator in the neuroevolution process, while supporting a wide spectrum of neural layers, including attention, dense, and graph convolutional layers. Our extensive experimental evaluation was conducted with widely adopted multivariate anomaly detection benchmark datasets, and showed that the models generated by AD-NEv++ outperform well-known deep learning architectures and neuroevolution-based approaches for anomaly detection. Moreover, results show that AD-NEv++ can improve and outperform the state-of-the-art GNN (Graph Neural Networks) model architecture in all anomaly detection benchmarks.",25,3,2024
EduAgent: Generative Student Agents in Learning,"Student simulation in online education is important to address dynamic learning behaviors of students with diverse backgrounds. Existing simulation models based on deep learning usually need massive training data, lacking prior knowledge in educational contexts. Large language models (LLMs) may contain such prior knowledge since they are pre-trained from a large corpus. However, because student behaviors are dynamic and multifaceted with individual differences, directly prompting LLMs is not robust nor accurate enough to capture fine-grained interactions among diverse student personas, learning behaviors, and learning outcomes. This work tackles this problem by presenting a newly annotated fine-grained large-scale dataset and proposing EduAgent, a novel generative agent framework incorporating cognitive prior knowledge (i.e., theoretical findings revealed in cognitive science) to guide LLMs to first reason correlations among various behaviors and then make simulations. Our two experiments show that EduAgent could not only mimic and predict learning behaviors of real students but also generate realistic learning behaviors of virtual students without real data.",23,3,2024
Live and Learn: Continual Action Clustering with Incremental Views,"Multi-view action clustering leverages the complementary information from different camera views to enhance the clustering performance. Although existing approaches have achieved significant progress, they assume all camera views are available in advance, which is impractical when the camera view is incremental over time. Besides, learning the invariant information among multiple camera views is still a challenging issue, especially in continual learning scenario. Aiming at these problems, we propose a novel continual action clustering (CAC) method, which is capable of learning action categories in a continual learning manner. To be specific, we first devise a category memory library, which captures and stores the learned categories from historical views. Then, as a new camera view arrives, we only need to maintain a consensus partition matrix, which can be updated by leveraging the incoming new camera view rather than keeping all of them. Finally, a three-step alternate optimization is proposed, in which the category memory library and consensus partition matrix are optimized. The empirical experimental results on 6 realistic multi-view action collections demonstrate the excellent clustering performance and time/space efficiency of the CAC compared with 15 state-of-the-art baselines.",23,3,2024
Content Knowledge Identification with Multi-Agent Large Language Models (LLMs),"Teachers' mathematical content knowledge (CK) is of vital importance and need in teacher professional development (PD) programs. Computer-aided asynchronous PD systems are the most recent proposed PD techniques, which aim to help teachers improve their PD equally with fewer concerns about costs and limitations of time or location. However, current automatic CK identification methods, which serve as one of the core techniques of asynchronous PD systems, face challenges such as diversity of user responses, scarcity of high-quality annotated data, and low interpretability of the predictions. To tackle these challenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to assess the user responses' coverage of identified CK learning goals without human annotations. By taking advantage of multi-agent LLMs in strong generalization ability and human-like discussions, our proposed LLMAgent-CK presents promising CK identifying performance on a real-world mathematical CK dataset MaCKT. Moreover, our case studies further demonstrate the working of the multi-agent framework.",22,3,2024
Damage identification of offshore jacket platforms in a digital twin framework considering optimal sensor placement,"A new digital twin (DT) framework with optimal sensor placement (OSP) is proposed to accurately calculate the modal responses and identify the damage ratios of the offshore jacket platforms. The proposed damage identification framework consists of two models (namely one OSP model and one damage identification model). The OSP model adopts the multi-objective Lichtenberg algorithm (MOLA) to perform the sensor number/location optimization to make a good balance between the sensor cost and the modal calculation accuracy. In the damage identification model, the Markov Chain Monte Carlo (MCMC)-Bayesian method is developed to calculate the structural damage ratios based on the modal information obtained from the sensory measurements, where the uncertainties of the structural parameters are quantified. The proposed method is validated using an offshore jacket platform, and the analysis results demonstrate efficient identification of the structural damage location and severity.",26,3,2024
Curvature and Weitzenbock formula for spectral triples,"Using the Levi-Civita connection on the noncommutative differential one-forms of a spectral triple $(\B,\H,\D)$, we define the full Riemann curvature tensor, the Ricci curvature tensor and scalar curvature. We give a definition of Dirac spectral triples and derive a general Weitzenbock formula for them. We apply these tools to $\theta$-deformations of compact Riemannian manifolds. We show that the Riemann and Ricci tensors transform naturally under $\theta$-deformation, whereas the connection Laplacian, Clifford representation of the curvature and the scalar curvature are all invariant under deformation.",21,3,2024
"Triple Component Matrix Factorization: Untangling Global, Local, and Noisy Components","In this work, we study the problem of common and unique feature extraction from noisy data. When we have N observation matrices from N different and associated sources corrupted by sparse and potentially gross noise, can we recover the common and unique components from these noisy observations? This is a challenging task as the number of parameters to estimate is approximately thrice the number of observations. Despite the difficulty, we propose an intuitive alternating minimization algorithm called triple component matrix factorization (TCMF) to recover the three components exactly. TCMF is distinguished from existing works in literature thanks to two salient features. First, TCMF is a principled method to separate the three components given noisy observations provably. Second, the bulk of the computation in TCMF can be distributed. On the technical side, we formulate the problem as a constrained nonconvex nonsmooth optimization problem. Despite the intricate nature of the problem, we provide a Taylor series characterization of its solution by solving the corresponding Karush-Kuhn-Tucker conditions. Using this characterization, we can show that the alternating minimization algorithm makes significant progress at each iteration and converges into the ground truth at a linear rate. Numerical experiments in video segmentation and anomaly detection highlight the superior feature extraction abilities of TCMF.",21,3,2024
An efficient domain-independent approach for supervised keyphrase extraction and ranking,"We present a supervised learning approach for automatic extraction of keyphrases from single documents. Our solution uses simple to compute statistical and positional features of candidate phrases and does not rely on any external knowledge base or on pre-trained language models or word embeddings. The ranking component of our proposed solution is a fairly lightweight ensemble model. Evaluation on benchmark datasets shows that our approach achieves significantly higher accuracy than several state-of-the-art baseline models, including all deep learning-based unsupervised models compared with, and is competitive with some supervised deep learning-based models too. Despite the supervised nature of our solution, the fact that does not rely on any corpus of ""golden"" keywords or any external knowledge corpus means that our solution bears the advantages of unsupervised solutions to a fair extent.",24,3,2024
Deep learning-based auto-segmentation of paraganglioma for growth monitoring,"Volume measurement of a paraganglioma (a rare neuroendocrine tumor that typically forms along major blood vessels and nerve pathways in the head and neck region) is crucial for monitoring and modeling tumor growth in the long term. However, in clinical practice, using available tools to do these measurements is time-consuming and suffers from tumor-shape assumptions and observer-to-observer variation. Growth modeling could play a significant role in solving a decades-old dilemma (stemming from uncertainty regarding how the tumor will develop over time). By giving paraganglioma patients treatment, severe symptoms can be prevented. However, treating patients who do not actually need it, comes at the cost of unnecessary possible side effects and complications. Improved measurement techniques could enable growth model studies with a large amount of tumor volume data, possibly giving valuable insights into how these tumors develop over time. Therefore, we propose an automated tumor volume measurement method based on a deep learning segmentation model using no-new-UNnet (nnUNet). We assess the performance of the model based on visual inspection by a senior otorhinolaryngologist and several quantitative metrics by comparing model outputs with manual delineations, including a comparison with variation in manual delineation by multiple observers. Our findings indicate that the automatic method performs (at least) equal to manual delineation. Finally, using the created model, and a linking procedure that we propose to track the tumor over time, we show how additional volume measurements affect the fit of known growth functions.",19,3,2024
Visualization for physics analysis improvement and applications in BESIII,"Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.",19,3,2024
Reinforcement Learning with Generalizable Gaussian Splatting,"An excellent representation is crucial for reinforcement learning (RL) performance, especially in vision-based reinforcement learning tasks. The quality of the environment representation directly influences the achievement of the learning task. Previous vision-based RL typically uses explicit or implicit ways to represent environments, such as images, points, voxels, and neural radiance fields. However, these representations contain several drawbacks. They cannot either describe complex local geometries or generalize well to unseen scenes, or require precise foreground masks. Moreover, these implicit neural representations are akin to a ``black box"", significantly hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit scene representation and differentiable rendering nature, is considered a revolutionary change for reconstruction and representation methods. In this paper, we propose a novel Generalizable Gaussian Splatting framework to be the representation of RL tasks, called GSRL. Through validation in the RoboMimic environment, our method achieves better results than other baselines in multiple tasks, improving the performance by 10%, 44%, and 15% compared with baselines on the hardest task. This work is the first attempt to leverage generalizable 3DGS as a representation for RL.",18,3,2024
Usability and Performance Analysis of Embedded Development Environment for On-device Learning,"This research empirically examines embedded development tools viable for on-device TinyML implementation. The research evaluates various development tools with various abstraction levels on resource-constrained IoT devices, from basic hardware manipulation to deployment of minimalistic ML training. The analysis encompasses memory usage, energy consumption, and performance metrics during model training and inference and usability of the different solutions. Arduino Framework offers ease of implementation but with increased energy consumption compared to the native option, while RIOT OS exhibits efficient energy consumption despite higher memory utilization with equivalent ease of use. The absence of certain critical functionalities like DVFS directly integrated into the OS highlights limitations for fine hardware control.",18,3,2024
ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference,"This paper presents ExeGPT, a distributed system designed for constraint-aware LLM inference. ExeGPT finds and runs with an optimal execution schedule to maximize inference throughput while satisfying a given latency constraint. By leveraging the distribution of input and output sequences, it effectively allocates resources and determines optimal execution configurations, including batch sizes and partial tensor parallelism. We also introduce two scheduling strategies based on Round-Robin Allocation and Workload-Aware Allocation policies, suitable for different NLP workloads. We evaluate ExeGPT on six LLM instances of T5, OPT, and GPT-3 and five NLP tasks, each with four distinct latency constraints. Compared to FasterTransformer, ExeGPT achieves up to 15.2x improvements in throughput and 6x improvements in latency. Overall, ExeGPT achieves an average throughput gain of 2.9x across twenty evaluation scenarios. Moreover, when adapting to changing sequence distributions, the cost of adjusting the schedule in ExeGPT is reasonably modest. ExeGPT proves to be an effective solution for optimizing and executing LLM inference for diverse NLP workload and serving conditions.",15,3,2024
Towards Faster Training of Diffusion Models: An Inspiration of A Consistency Phenomenon,"Diffusion models (DMs) are a powerful generative framework that have attracted significant attention in recent years. However, the high computational cost of training DMs limits their practical applications. In this paper, we start with a consistency phenomenon of DMs: we observe that DMs with different initializations or even different architectures can produce very similar outputs given the same noise inputs, which is rare in other generative models. We attribute this phenomenon to two factors: (1) the learning difficulty of DMs is lower when the noise-prediction diffusion model approaches the upper bound of the timestep (the input becomes pure noise), where the structural information of the output is usually generated; and (2) the loss landscape of DMs is highly smooth, which implies that the model tends to converge to similar local minima and exhibit similar behavior patterns. This finding not only reveals the stability of DMs, but also inspires us to devise two strategies to accelerate the training of DMs. First, we propose a curriculum learning based timestep schedule, which leverages the noise rate as an explicit indicator of the learning difficulty and gradually reduces the training frequency of easier timesteps, thus improving the training efficiency. Second, we propose a momentum decay strategy, which reduces the momentum coefficient during the optimization process, as the large momentum may hinder the convergence speed and cause oscillations due to the smoothness of the loss landscape. We demonstrate the effectiveness of our proposed strategies on various models and show that they can significantly reduce the training time and improve the quality of the generated images.",14,3,2024
HomoGenius: a Foundation Model of Homogenization for Rapid Prediction of Effective Mechanical Properties using Neural Operators,"Homogenization is an essential tool for studying multiscale physical phenomena. However, traditional numerical homogenization, heavily reliant on finite element analysis, requires extensive computation costs, particularly in handling complex geometries, materials, and high-resolution problems. To address these limitations, we propose a numerical homogenization model based on operator learning: HomoGenius. The proposed model can quickly provide homogenization results for arbitrary geometries, materials, and resolutions, increasing the efficiency by a factor of 80 compared to traditional numerical homogenization methods. We validate effectiveness of our model in predicting the effective elastic modulus on periodic materials (TPMS: Triply Periodic Minimal Surface), including complex geometries, various Poisson's ratios and elastic modulus, and different resolutions for training and testing. The results show that our model possesses high precision, super efficiency, and learning capability.",18,3,2024
SiGNN: A Spike-induced Graph Neural Network for Dynamic Graph Representation Learning,"In the domain of dynamic graph representation learning (DGRL), the efficient and comprehensive capture of temporal evolution within real-world networks is crucial. Spiking Neural Networks (SNNs), known as their temporal dynamics and low-power characteristic, offer an efficient solution for temporal processing in DGRL task. However, owing to the spike-based information encoding mechanism of SNNs, existing DGRL methods employed SNNs face limitations in their representational capacity. Given this issue, we propose a novel framework named Spike-induced Graph Neural Network (SiGNN) for learning enhanced spatialtemporal representations on dynamic graphs. In detail, a harmonious integration of SNNs and GNNs is achieved through an innovative Temporal Activation (TA) mechanism. Benefiting from the TA mechanism, SiGNN not only effectively exploits the temporal dynamics of SNNs but also adeptly circumvents the representational constraints imposed by the binary nature of spikes. Furthermore, leveraging the inherent adaptability of SNNs, we explore an in-depth analysis of the evolutionary patterns within dynamic graphs across multiple time granularities. This approach facilitates the acquisition of a multiscale temporal node representation.Extensive experiments on various real-world dynamic graph datasets demonstrate the superior performance of SiGNN in the node classification task.",11,3,2024
InfiCoder-Eval: Systematically Evaluating the Question-Answering Capabilities of Code Large Language Models,"Large Language Models for understanding and generating code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiCoder-Eval, a large-scale freeform question-answering (QA) benchmark for code, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. To evaluate the response correctness, InfiCoder-Eval supports four types of model-free metrics and domain experts carefully choose and concretize the criterion for each question. We conduct a systematic evaluation for more than 80 code LLMs on InfiCoder-Eval, leading to a series of insightful findings. Furthermore, our detailed analyses showcase possible directions for further improvement of code LLMs. InfiCoder-Eval is fully open source atthis https URLand continuously maintaining and expanding to foster more scientific and systematic practices for evaluating code LLMs.",11,3,2024
Distributed Record Linkage in Healthcare Data with Apache Spark,"Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.",9,3,2024
Unveiling the Impact of Macroeconomic Policies: A Double Machine Learning Approach to Analyzing Interest Rate Effects on Financial Markets,"This study examines the effects of macroeconomic policies on financial markets using a novel approach that combines Machine Learning (ML) techniques and causal inference. It focuses on the effect of interest rate changes made by the US Federal Reserve System (FRS) on the returns of fixed income and equity funds between January 1986 and December 2021. The analysis makes a distinction between actively and passively managed funds, hypothesizing that the latter are less susceptible to changes in interest rates. The study contrasts gradient boosting and linear regression models using the Double Machine Learning (DML) framework, which supports a variety of statistical learning techniques. Results indicate that gradient boosting is a useful tool for predicting fund returns; for example, a 1% increase in interest rates causes an actively managed fund's return to decrease by -11.97%. This understanding of the relationship between interest rates and fund performance provides opportunities for additional research and insightful, data-driven advice for fund managers and investors",31,3,2024
Detection of financial opportunities in micro-blogging data with a stacked classification system,"Micro-blogging sources such as the Twitter social network provide valuable real-time data for market prediction models. Investors' opinions in this network follow the fluctuations of the stock markets and often include educated speculations on market opportunities that may have impact on the actions of other investors. In view of this, we propose a novel system to detect positive predictions in tweets, a type of financial emotions which we term ""opportunities"" that are akin to ""anticipation"" in Plutchik's theory. Specifically, we seek a high detection precision to present a financial operator a substantial amount of such tweets while differentiating them from the rest of financial emotions in our system. We achieve it with a three-layer stacked Machine Learning classification system with sophisticated features that result from applying Natural Language Processing techniques to extract valuable linguistic information. Experimental results on a dataset that has been manually annotated with financial emotion and ticker occurrence tags demonstrate that our system yields satisfactory and competitive performance in financial opportunity detection, with precision values up to 83%. This promising outcome endorses the usability of our system to support investors' decision making.",29,3,2024
Stock Recommendations for Individual Investors: A Temporal Graph Network Approach with Diversification-Enhancing Contrastive Learning,"In complex financial markets, recommender systems can play a crucial role in empowering individuals to make informed decisions. Existing studies predominantly focus on price prediction, but even the most sophisticated models cannot accurately predict stock prices. Also, many studies show that most individual investors do not follow established investment theories because they have their own preferences. Hence, the tricky point in stock recommendation is that recommendations should give good investment performance but also should not ignore individual preferences. To develop effective stock recommender systems, it is essential to consider three key aspects: 1) individual preferences, 2) portfolio diversification, and 3) temporal aspect of both stock features and individual preferences. In response, we develop the portfolio temporal graph network recommender PfoTGNRec, which can handle time-varying collaborative signals and incorporates diversification-enhancing contrastive learning. As a result, our model demonstrated superior performance compared to various baselines, including cutting-edge dynamic embedding models and existing stock recommendation models, in a sense that our model exhibited good investment performance while maintaining competitive in capturing individual preferences. The source code and data are available at https://anonymous.4open.science/r/IJCAI2024-12F4.",27,3,2024
Improving Retrieval for RAG based Question Answering Models on Financial Documents,"The effectiveness of Large Language Models (LLMs) in generating accurate responses relies heavily on the quality of input provided, particularly when employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by sourcing the most relevant text chunk(s) to base queries upon. Despite the significant advancements in LLMs' response quality in recent years, users may still encounter inaccuracies or irrelevant answers; these issues often stem from suboptimal text chunk retrieval by RAG rather than the inherent capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine the RAG process. This paper explores the existing constraints of RAG pipelines and introduces methodologies for enhancing text retrieval. It delves into strategies such as sophisticated chunking techniques, query expansion, the incorporation of metadata annotations, the application of re-ranking algorithms, and the fine-tuning of embedding algorithms. Implementing these approaches can substantially improve the retrieval quality, thereby elevating the overall performance and reliability of LLMs in processing and responding to queries.",23,3,2024
Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers,"Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q\&A datasets like SQUAD, even surpassing fine-tuning performance.",22,3,2024
Miniaturized time-correlated single-photon counting module for time-of-flight non-line-of-sight imaging applications,"Single-photon time-of-flight (TOF) non-line-of-sight (NLOS) imaging enables the high-resolution reconstruction of objects outside the field of view. The compactness of TOF NLOS imaging systems, entailing the miniaturization of key components within such systems is crucial for practical applications. Here, we present a miniaturized four-channel time-correlated single-photon counting module dedicated to TOF NLOS imaging applications. The module achieves excellent performance with a 10 ps bin size and 27.4 ps minimum root-mean-square time resolution. We present the results of TOF NLOS imaging experiment using an InGaAs/InP single-photon detector and the time-correlated single-photon counting module, and show that a 6.3 cm lateral resolution and 2.3 cm depth resolution can be achieved under the conditions of 5 m imaging distance and 1 ms pixel dwell time.",9,3,2024
On Generalized Bihyperbolic Third-order Jacobsthal Polynomials,"In this paper, a new generalization of third-order Jacobsthal bihyperbolic polynomials is introduced. Some of the properties of presented polynomials are given. A Vadja formula for the generalized bihyperbolic third-order Jacobsthal polynomials is obtained. This result implies the Catalan, Cassini and d'Ocagne identities. Moreover, generating function and matrix generators for these polynomials are presented.",30,3,2024
Soil respiration signals in response to sustainable soil management practices enhance soil organic carbon stocks,"Development of a spatial-temporal and data-driven model of soil respiration at the global scale based on soil temperature, yearly soil moisture, and soil organic carbon (C) estimates. Prediction of soil respiration on an annual basis (1991-2018) with relatively high accuracy (NSE 0.69, CCC 0.82). Lower soil respiration trends, higher soil respiration magnitudes, and higher soil organic C stocks across areas experiencing the presence of sustainable soil management practices.",28,3,2024
Beta Distribution of Long Memory Sequences,"Three long memory models, ARFIMA, Timmer and Konig 1995, and a circular convolution model based on Wold's representation theorem are examined. Each model is shown to produce sequences with nonstationary generalized beta marginal distributions. It is demonstrated that the variance divided by the squared range of the sequence is stationary, and is a function of the shape parameter of the resulting symmetric beta distribution. Using the Wold model, a simple matrix distribution transformation is given that maps the normal components of the long memory model onto the beta distribution.",26,3,2024
A Python Framework for Neutrosophic Sets and Mappings,"In this paper we present an open source framework developed in Python and consisting of three distinct classes designed to manipulate in a simple and intuitive way both symbolic representations of neutrosophic sets over universes of various types as well as mappings between them. The capabilities offered by this framework extend and generalize previous attempts to provide software solutions to the manipulation of neutrosophic sets such as those proposed by Salama et al., Saranya et al., El-Ghareeb, Topal et al. and Sleem. The code is described in detail and many examples and use cases are also provided.",24,3,2024
An Online Algorithm for Solving Feedback Optimal Control Problems with Partial Observations,"This paper presents a novel methodology to tackle feedback optimal control problems in scenarios where the exact state of the controlled process is unknown. It integrates data assimilation techniques and optimal control solvers to manage partial observation of the state process, a common occurrence in practical scenarios. Traditional stochastic optimal control methods assume full state observation, which is often not feasible in real-world applications. Our approach underscores the significance of utilizing observational data to inform control policy design. Specifically, we introduce a kernel learning backward stochastic differential equation (SDE) filter to enhance data assimilation efficiency and propose a sample-wise stochastic optimization method within the stochastic maximum principle framework. Numerical experiments validate the efficacy and accuracy of our algorithm, showcasing its high efficiency in solving feedback optimal control problems with partial observation.",22,3,2024
Recursive index for assessing value added of individual scientific publications,"An aggregated recursive K-index is proposed as a new scientometric indicator of added value and scientific research output of individual publications. This index can be used instead of or in addition to the H-index (J.E. Hirsch. An index to quantify an individual's scientific research output,arXiv:physics/0508025). In particular, it is proposed to switch from a pure strategy for assessing the quality and effectiveness of R&D using the H-index (Hirsch index) to a mixed strategy (in the context of publication activity as a combination of cooperative and noncooperative games) using the K-index on subnational and H-index on international or differentiated levels. In the context of a hybrid strategy of the scientist's payoff functions. This transition is correct and in demand for a number of national scientific systems with limited financial, material, infrastructural and linguistic (in terms of the English language) potential. Scientific systems with highly developed indigenous (autochthonous) characteristics are also needed in some scientific areas.",29,3,2024
"Comment on ""Non-reciprocal topological solitons in active metamaterials""","In the recent work ""Non-reciprocal topological solitons in active metamaterials"" (seearXiv:2312.03544v1), for an analytical understanding of the system under consideration, the authors derive an ordinary differential equation for the sine-Gordon (anti)soliton velocity, with the perturbation theory in the adiabatic approximation, via the inverse scattering transform formalism, see Eq. (3) in their work. Here we note that the latter equation for the (anti)soliton velocity also follows from an energy balance approach.",29,3,2024
Pattern formation from Gauge/Gravity Duality,"In the framework of the AdS/CFT correspondence, we report a complex scalar field dynamics in a $2+1$ dimensional black hole background can provide a scheme to study the pattern formation process in $1+1$ dimensional reaction-diffusion systems. The patterns include plane wave, defect turbulence, phase turbulence, spatio-temporal intermittency where defect chaos coexists with stable plane wave, and coherent structures. A phase diagram is obtained by studying the linear instability of the plane wave solutions to determine the onset of the holographic version of the Benjamin-Feir instability near a supercritical Hopf bifurcation.",29,3,2024
Photon Transitions in Arbitrary Time-Varying Metamaterials,We present a general theory for calculating photon transitions in arbitrarily time-varying metamaterials. This theory circumvents the difficulties of conventional approaches in solving such a general problem by exploiting the eigenstates of time-dependent number operators. We demonstrate here the temporal evolution of these operators and the related transition probabilities for the cases of logistic and linear permittivity profiles. The theory is potentially extensible to arbitrary space-time modulations and may hence lead to multiple novel quantum effects and applications.,25,3,2024
Selecting Query-bag as Pseudo Relevance Feedback for Information-seeking Conversations,"Information-seeking dialogue systems are widely used in e-commerce systems, with answers that must be tailored to fit the specific settings of the online system. Given the user query, the information-seeking dialogue systems first retrieve a subset of response candidates, then further select the best response from the candidate set through re-ranking. Current methods mainly retrieve response candidates based solely on the current query, however, incorporating similar questions could introduce more diverse content, potentially refining the representation and improving the matching process. Hence, in this paper, we proposed a Query-bag based Pseudo Relevance Feedback framework (QB-PRF), which constructs a query-bag with related queries to serve as pseudo signals to guide information-seeking conversations. Concretely, we first propose a Query-bag Selection module (QBS), which utilizes contrastive learning to train the selection of synonymous queries in an unsupervised manner by leveraging the representations learned from pre-trained VAE. Secondly, we come up with a Query-bag Fusion module (QBF) that fuses synonymous queries to enhance the semantic representation of the original query through multidimensional attention computation. We verify the effectiveness of the QB-PRF framework on two competitive pretrained backbone models, including BERT and GPT-2. Experimental results on two benchmark datasets show that our framework achieves superior performance over strong baselines.",22,3,2024
Towards Effective Next POI Prediction: Spatial and Semantic Augmentation with Remote Sensing Data,"The next point-of-interest (POI) prediction is a significant task in location-based services, yet its complexity arises from the consolidation of spatial and semantic intent. This fusion is subject to the influences of historical preferences, prevailing location, and environmental factors, thereby posing significant challenges. In addition, the uneven POI distribution further complicates the next POI prediction procedure. To address these challenges, we enrich input features and propose an effective deep-learning method within a two-step prediction framework. Our method first incorporates remote sensing data, capturing pivotal environmental context to enhance input features regarding both location and semantics. Subsequently, we employ a region quad-tree structure to integrate urban remote sensing, road network, and POI distribution spaces, aiming to devise a more coherent graph representation method for urban spatial. Leveraging this method, we construct the QR-P graph for the user's historical trajectories to encapsulate historical travel knowledge, thereby augmenting input features with comprehensive spatial and semantic insights. We devise distinct embedding modules to encode these features and employ an attention mechanism to fuse diverse encodings. In the two-step prediction procedure, we initially identify potential spatial zones by predicting user-preferred tiles, followed by pinpointing specific POIs of a designated type within the projected tiles. Empirical findings from four real-world location-based social network datasets underscore the remarkable superiority of our proposed approach over competitive baseline methods.",22,3,2024
Accelerating Recommender Model Training by Dynamically Skipping Stale Embeddings,"Training recommendation models pose significant challenges regarding resource utilization and performance. Prior research has proposed an approach that categorizes embeddings into popular and non-popular classes to reduce the training time for recommendation models. We observe that, even among the popular embeddings, certain embeddings undergo rapid training and exhibit minimal subsequent variation, resulting in saturation. Consequently, updates to these embeddings lack any contribution to model quality. This paper presents Slipstream, a software framework that identifies stale embeddings on the fly and skips their updates to enhance performance. This capability enables Slipstream to achieve substantial speedup, optimize CPU-GPU bandwidth usage, and eliminate unnecessary memory access. SlipStream showcases training time reductions of 2x, 2.4x, 1.2x, and 1.175x across real-world datasets and configurations, compared to Baseline XDL, Intel-optimized DRLM, FAE, and Hotline, respectively.",22,3,2024
Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists,"We investigate algorithmic collective action in transformer-based recommender systems. Our use case is a collective of fans aiming to promote the visibility of an artist by strategically placing one of their songs in the existing playlists they control. The success of the collective is measured by the increase in test-time recommendations of the targeted song. We introduce two easily implementable strategies towards this goal and test their efficacy on a publicly available recommender system model released by a major music streaming platform. Our findings reveal that even small collectives (controlling less than 0.01% of the training data) can achieve up 25x amplification of recommendations by strategically choosing the position at which to insert the song. We then focus on investigating the externalities of the strategy. We find that the performance loss for the platform is negligible, and the recommendations of other songs are largely preserved, minimally impairing the user experience of participants. Moreover, the costs are evenly distributed among other artists. Taken together, our findings demonstrate how collective action strategies can be effective while not necessarily being adversarial, raising new questions around incentives, social dynamics, and equilibria in recommender systems.",19,3,2024
The Use of Generative Search Engines for Knowledge Work and Complex Tasks,"Until recently, search engines were the predominant method for people to access online information. The recent emergence of large language models (LLMs) has given machines new capabilities such as the ability to generate new digital artifacts like text, images, code etc., resulting in a new tool, a generative search engine, which combines the capabilities of LLMs with a traditional search engine. Through the empirical analysis of Bing Copilot (Bing Chat), one of the first publicly available generative search engines, we analyze the types and complexity of tasks that people use Bing Copilot for compared to Bing Search. Findings indicate that people use the generative search engine for more knowledge work tasks that are higher in cognitive complexity than were commonly done with a traditional search engine.",19,3,2024
WGMR Self-Injection Locking Method Based on Enhanced Optical Feedback with Auxiliary Prism,"The optical feedback intensity is an important parameter for realizing narrow linewidth lasers in Whispering-gallery-mode resonator (WGMR) self-injection locking technology. We proposed an approach that enhances the intensity of intracavity feedback in crystalline WGMR by using only a single coated auxiliary prism. Compared to the Rayleigh scattering, the feedback intensity of the enhanced scheme increased by more than a hundred times. Furthermore, we demonstrated that, with the enhanced approach, the instantaneous linewidth of the laser was suppressed to 7 Hz level, the locking range was expanded up to 8 GHz, and the relative intensity noise (RIN) was reduced to -152 dBc/Hz@10MHz. The feedback enhanced design is compact, easy-to-operated and can be integrated with the WGMR. It provides a miniaturized solution for controlling optical feedback intensity in WGMR self-injection locking technology.",19,3,2024
Accelerating Matrix Factorization by Dynamic Pruning for Fast Recommendation,"Matrix factorization (MF) is a widely used collaborative filtering (CF) algorithm for recommendation systems (RSs), due to its high prediction accuracy, great flexibility and high efficiency in big data processing. However, with the dramatically increased number of users/items in current RSs, the computational complexity for training a MF model largely increases. Many existing works have accelerated MF, by either putting in additional computational resources or utilizing parallel systems, introducing a large cost. In this paper, we propose algorithmic methods to accelerate MF, without inducing any additional computational resources. In specific, we observe fine-grained structured sparsity in the decomposed feature matrices when considering a certain threshold. The fine-grained structured sparsity causes a large amount of unnecessary operations during both matrix multiplication and latent factor update, increasing the computational time of the MF training process. Based on the observation, we firstly propose to rearrange the feature matrices based on joint sparsity, which potentially makes a latent vector with a smaller index more dense than that with a larger index. The feature matrix rearrangement is given to limit the error caused by the later performed pruning process. We then propose to prune the insignificant latent factors by an early stopping process during both matrix multiplication and latent factor update. The pruning process is dynamically performed according to the sparsity of the latent factors for different users/items, to accelerate the process. The experiments show that our method can achieve 1.2-1.65 speedups, with up to 20.08% error increase, compared with the conventional MF training process. We also prove the proposed methods are applicable considering different hyperparameters including optimizer, optimization strategy and initialization method.",18,3,2024
The Future of MEV,"This paper analyzes the Execution Tickets proposal on Ethereum Research, unveiling its potential to revolutionize the Ethereum blockchain's economic model. At the core of this proposal lies a novel ticketing mechanism poised to redefine how the Ethereum protocol distributes the value associated with proposing execution payloads. This innovative approach enables the Ethereum protocol to directly broker Maximal Extractable Value (MEV), traditionally an external revenue stream for validators. The implementation of Execution Tickets goes beyond optimizing validator compensation; it also introduces a new Ethereum native asset with a market capitalization expected to correlate closely with the present value of all value associated with future block production. The analysis demonstrates that the Execution Ticket system can facilitate a more equitable distribution of value within the Ethereum ecosystem, and pave the way for a more secure and economically robust blockchain network.",5,3,2024
CONCERT: Covariate-Elaborated Robust Local Information Transfer with Conditional Spike-and-Slab Prior,"The popularity of transfer learning stems from the fact that it can borrow information from useful auxiliary datasets. Existing statistical transfer learning methods usually adopt a global similarity measure between the source data and the target data, which may lead to inefficiency when only local information is shared. In this paper, we propose a novel Bayesian transfer learning method named ""CONCERT"" to allow robust local information transfer for high-dimensional data analysis. A novel conditional spike-and-slab prior is introduced in the joint distribution of target and source parameters for information transfer. By incorporating covariate-specific priors, we can characterize the local similarities and make the sources work collaboratively to help improve the performance on the target. Distinguished from existing work, CONCERT is a one-step procedure, which achieves variable selection and information transfer simultaneously. Variable selection consistency is established for our CONCERT. To make our algorithm scalable, we adopt the variational Bayes framework to facilitate implementation. Extensive experiments and a genetic data analysis demonstrate the validity and the advantage of CONCERT over existing cutting-edge transfer learning methods. We also extend our CONCERT to the logistical models with numerical studies showing its superiority over other methods.",30,3,2024
Muon Track Reconstruction in the Scintillator Phase of SNO+,"The large depth of the SNO+ experiment (2070 m, 6010 m.w.e.) means that only a few muons per day pass through the detector. However, their high energy causes muon induced backgrounds which can affect multiple physics analyses. Reconstructing the muon track would allow for improved rejection for these induced backgrounds. Currently there is no muon tracker for the scintillator phase of SNO+. This poster presents a novel method of muon track reconstruction by using the high photon sampling from muons and the assumption that each PMT first registers a hit from a photon that takes the fastest possible path from the muon entry point to the PMT.",28,3,2024
Pulse Shape Discrimination in JSNS$^2$,"JSNS$^2$ (J-PARC Sterile Neutrino Search at J-PARC Spallation Neutron Source) is an experiment that is searching for sterile neutrinos via the observation of $\bar{\nu}_{\mu} \rightarrow \bar{\nu}_e$ appearance oscillations using neutrinos with muon decay-at-rest. For this search, rejecting cosmic-ray-induced neutron events by Pulse Shape Discrimination (PSD) is essential because the JSNS$^2$ detector is located above ground, on the third floor of the building. We have achieved 95$\%$ rejection of neutron events while keeping 90$\%$ of signal, electron-like events using a data driven likelihood method.",28,3,2024
Machine learning augmented diagnostic testing to identify sources of variability in test performance,"Diagnostic tests which can detect pre-clinical or sub-clinical infection, are one of the most powerful tools in our armoury of weapons to control infectious diseases. Considerable effort has been therefore paid to improving diagnostic testing for human, plant and animal diseases, including strategies for targeting the use of diagnostic tests towards individuals who are more likely to be infected. Here, we follow other recent proposals to further refine this concept, by using machine learning to assess the situational risk under which a diagnostic test is applied to augment its interpretation . We develop this to predict the occurrence of breakdowns of cattle herds due to bovine tuberculosis, exploiting the availability of exceptionally detailed testing records. We show that, without compromising test specificity, test sensitivity can be improved so that the proportion of infected herds detected by the skin test, improves by over 16 percentage points. While many risk factors are associated with increased risk of becoming infected, of note are several factors which suggest that, in some herds there is a higher risk of infection going undetected, including effects that are correlated to the veterinary practice conducting the test, and number of livestock moved off the herd.",28,3,2024
Status of the production of GEM chambers for the CMS experiment at Large Hadron Collider,"The High Luminosity LHC phase includes an upgrade to the muon stations for the CMS Experiment. CMS trigger and muon identification performance will be crucial, and it is, therefore, necessary to install new GEM stations to extend acceptance in the high-{\eta} region. An explanation of the quality control test and an update on the status of production will be provided.",28,3,2024
Deciphering Accretion-Driven Starquakes in Recycled Millisecond Pulsars using Gravitational Waves,"Recycled millisecond pulsars are susceptible to starquakes as they are continuously accreting matter from their binary companion. A starquake happens when the rotational frequency of the star crosses its breaking frequency. In this study, we perform a model analysis of an accreting neutron star suffering a starquake. We analyze two models: a spherical star with accreting mountains and a deformed star with accreting mountains. We find that as the star crosses the breaking frequency and suffers a starquake there is a sudden change in the continuous gravitational wave signal arriving from them. It is interesting to note that the amplitude of the gravitational wave signals increases suddenly for the spherical star. In contrast, for the deformed star, the amplitude of the continuous gravitational wave signal decreases suddenly. This sudden change in the continuous gravitational wave signal in recycled millisecond pulsars can be a unique signature for such pulsars undergoing a starquake.",27,3,2024
Target normal single-spin asymmetry in inclusive electron-nucleon scattering in the 1/Nc expansion,"The target normal single-spin asymmetry in electron nucleon scattering is studied in the framework of the 1/Nc expansion of QCD, which allows for a rigorous description in the energy range that includes the Delta resonance and below the second baryon resonance region. The asymmetry is driven by the absorptive part of the two-photon exchange component of the scattering amplitude, being therefore the most unambiguous two-photon exchange effect. Such amplitude is shown to be described up to the next to leading order in the 1/Nc expansion only in terms of the charge and magnetic form factors of the nucleons, consequence of the approximate $SU(4)$ spin flavor symmetry valid in the large Nc limit for baryons. A discussion is provided of the 1/Nc expansion framework along with the results for the asymmetries in elastic, inelastic, and inclusive electron-nucleon scattering.",26,3,2024
RL for Consistency Models: Faster Reward Guided Text-to-Image Generation,"Reinforcement learning (RL) has improved guided image generation with diffusion models by directly optimizing rewards that capture image quality, aesthetics, and instruction following capabilities. However, the resulting generative policies inherit the same iterative sampling process of diffusion models that causes slow generation. To overcome this limitation, consistency models proposed learning a new class of generative models that directly map noise to data, resulting in a model that can generate an image in as few as one sampling iteration. In this work, to optimize text-to-image generative models for task specific rewards and enable fast training and inference, we propose a framework for fine-tuning consistency models via RL. Our framework, called Reinforcement Learning for Consistency Model (RLCM), frames the iterative inference process of a consistency model as an RL procedure. RLCM improves upon RL fine-tuned diffusion models on text-to-image generation capabilities and trades computation during inference time for sample quality. Experimentally, we show that RLCM can adapt text-to-image consistency models to objectives that are challenging to express with prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Comparing to RL finetuned diffusion models, RLCM trains significantly faster, improves the quality of the generation measured under the reward objectives, and speeds up the inference procedure by generating high quality images with as few as two inference steps. Our code is available atthis https URL",25,3,2024
On self-similar solutions of a multi-phase Stefan problem in the half-line,"We study self-similar solutions of a multi-phase Stefan problem for a heat equation on the half-line $x>0$ with a constant initial data and with Dirichlet or Neumann boundary conditions. In the case of Dirichlet boundary condition we prove that a nonlinear algebraic system for determination of the free boundaries is gradient one and the corresponding potential is an explicitly written strictly convex and coercive function. Therefore, there exists a unique minimum point of the potential, coordinates of this point determine free boundaries and provide the desired solution. In the case of Neumann boundary condition we demonstrate that the problem may have solutions with different numbers (called types) of phase transitions. For each fixed type $n$ the system for determination of the free boundaries is again gradient and the corresponding potential is proved to be strictly convex and coercive, but in some wider non-physical domain. In particular, a solution of type $n$ is unique and can exist only if the minimum point of the potential belongs to the physical domain. We provide an explicit criteria for existence of solutions of any type $n$. Due to the rather complicated structure of the set of solutions, neither existence nor uniqueness of a solution to Stefan-Neumann problem are guaranteed. Bibliography: 5 titles.",16,3,2024
Central Engine and Spectral Energy Distribution Properties of High Redshift Gamma Ray Blazars,"We report on the properties of central engines in the $\gamma$-ray blazars located at high redshifts beyond z~>~0.4, where the extra-galactic background light (EBL) starts affecting their $\gamma$-ray spectra. The physical engine that provides power to the blazars of very high bolometric luminosity is assumed to be a highly collimated jet of matter moving relativistically away from the supermassive black hole (SMBH), located in the central region of the host galaxy, in a direction aligned toward the Earth. Due to their peculiar geometry and special physical conditions, blazars at redshifts beyond z~>~0.4 are bright enough to be detected in the $\gamma$-ray energy band. In this work, we investigate the physical properties of high-$z$ $\gamma$-ray blazars detected by the Large Area Telescope (LAT) on board the \emph{Fermi} satellite. We also study the properties of their emission regions and the central engines and discuss cosmological and astrophysical implications.",11,3,2024
Solving Min-Cost Concave Generalized Dynamic Flows and Approximating Dynamic Optimal Power Flows,"Assuming power travels instantaneously, can be steered by us, and is lost quadratically in each power line, the dynamic optimal power flow problem simplifies to a min-cost dynamic generalized flow with quadratic losses (MCDGFWQL) problem. As this is a special case of the min-cost concave dynamic generalized flow (MCCDGF) problem, we derive both general results for the MCCDGF problem and stronger results for the MCDGFWQL problem.Our main contribution is a fully polynomial-time approximation scheme for a mild restriction of the MCDGFWQL problem. We also implement and benchmark a slight modification of this algorithm, finding it to be efficient in practice, with a slightly superlinear and possibly subquadratic dependence of execution time on the edge count of the input graph.Our secondary contributions are the derivation of a reduction from the MCCDGF problem to a convex program and a corresponding sensitivity analysis of the MCCDGF problem. We also provide a brief comparison of our dynamic optimal power flow simplification and the classic direct current simplification and point out some interesting directions for future research.",9,3,2024
On the transition stage of pulsar pulsed radio emission and its potential association with radio pulsar nulling,"While the precise mechanism of generating pulsed coherent radio emission from pulsars remains elusive, certain gap-invoking models (especially, the inner gap model) offer a comprehensive and plausible explanation for the genesis and termination of such emissions. However, the transition stage between the period of persistent radio emission and the period of radio quiet remains poorly understood, despite observations indicating that a radio pulsar in the pulse nulling state is undergoing the transition stage. In this study, we present a qualitative explanation for the elusive transition stage by modeling pulsar magnetospheres analytically as equivalent RC circuits based on the inner gap model. Our result indicates that, due to lengthy spin-down, older radio pulsars will gradually shift from the state of persistent radio emission to a certain type of pulse nulling state by delayed sparks within their inner gaps.",9,3,2024
An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in Power Distribution Grids,"Detection of cyber attacks in smart power distribution grids with unbalanced configurations poses challenges due to the inherent nonlinear nature of these uncertain and stochastic systems. It originates from the intermittent characteristics of the distributed energy resources (DERs) generation and load variations. Moreover, the unknown behavior of cyber attacks, especially false data injection attacks (FDIAs) in the distribution grids with complex temporal correlations and the limited amount of labeled data increases the vulnerability of the grids and imposes a high risk in the secure and reliable operation of the grids. To address these challenges, this paper proposes an unsupervised adversarial autoencoder (AAE) model to detect FDIAs in unbalanced power distribution grids integrated with DERs, i.e., PV systems and wind generation. The proposed method utilizes long short-term memory (LSTM) in the structure of the autoencoder to capture the temporal dependencies in the time-series measurements and leverages the power of generative adversarial networks (GANs) for better reconstruction of the input data. The advantage of the proposed data-driven model is that it can detect anomalous points for the system operation without reliance on abstract models or mathematical representations. To evaluate the efficacy of the approach, it is tested on IEEE 13-bus and 123-bus systems with historical meteorological data (wind speed, ambient temperature, and solar irradiance) as well as historical real-world load data under three types of data falsification functions. The comparison of the detection results of the proposed model with other unsupervised learning methods verifies its superior performance in detecting cyber attacks in unbalanced power distribution grids.",31,3,2024
Scalar induced gravitational waves in metric teleparallel gravity with the Nieh-Yan term,"We investigate the scalar induced gravitational waves (SIGWs) in metric teleparallel gravity with the Nieh-Yan (NY) term, which results in parity violation during the radiation-dominated era. By solving the equations of motion of linear scalar perturbations from both the metric and tetrad fields, we obtain the corresponding analytic expressions. Then, we calculate the SIGWs in metric teleparallel gravity with the NY term and evaluate the energy density of SIGWs with a monochromatic power spectrum numerically. We find that the spectrum of the energy density of SIGWs in metric teleparallel gravity with the NY term is significantly different from that in general relativity (GR), which makes metric teleparallel gravity distinguishable from GR.",29,3,2024
Enhancing Research Information Systems with Identification of Domain Experts,"Research organisations and their research outputs have been growing considerably in the past decades. This large body of knowledge attracts various stakeholders, e.g., for knowledge sharing, technology transfer, or potential collaborations. However, due to the large amount of complex knowledge created, traditional methods of manually curating catalogues are often out of time, imprecise, and cumbersome. Finding domain experts and knowledge within any larger organisation, scientific and also industrial, has thus become a serious challenge. Hence, exploring an institutions domain knowledge and finding its experts can only be solved by an automated solution. This work presents the scheme of an automated approach for identifying scholarly experts based on their publications and, prospectively, their teaching materials. Based on a search engine, this approach is currently being implemented for two universities, for which some examples are presented. The proposed system will be helpful for finding peer researchers as well as starting points for knowledge exploitation and technology transfer. As the system is designed in a scalable manner, it can easily include additional institutions and hence provide a broader coverage of research facilities in the future.",28,3,2024
Advanced Algorithms for Autonomous Guidance of Solar-powered UAVs,"Unmanned aerial vehicle (UAV) techniques have developed rapidly within the past few decades. Using UAVs provides benefits in numerous applications such as site surveying, communication systems, parcel delivery, target tracking, etc. The high manoeuvrability of the drone and its ability to replace a certain amount of labour cost are the reasons why it can be widely chosen. There will be more applications of UAVs if they can have longer flight time, which is a very challenging hurdle because of the energy constraint of the onboard battery. One promising solution is to equip UAVs with some lightweight solar panels to maximize flight time. Therefore, more research is needed for solar-powered UAVs (SUAVs) in different environments.",28,3,2024
Relaxation for degenerate nonlinear functionals in the onedimensional case,"In this study, we approach the analysis of a degenerate nonlinear functional in one dimension, accommodating a degenerate weight w. Building upon recent findings from [9, 12], our investigation focuses on establishing an explicit relaxation formula for a functional exhibiting p-growth for $1\leq p<+\infty$. For the case $1<p<\infty$, we adopt the approach developed in [12], where some assumptions like doubling or Muckenhoupt conditions are dropped. Moreover, for $p = 1$, we leverage novel concepts introduced in [9]. In both cases, our main tools consist of proving the validity of a weighted Poincaré inequality involving an auxiliary weight.",28,3,2024
Finding all solutions to the KZ equations in characteristic $p$,"The KZ equations are differential equations satisfied by the correlation functions (on the Riemann sphere) of two-dimensional conformal field theories associated with an affine Lie algebra at a fixed level. They form a system of complex partial differential equations with regular singular points satisfied by the $n$-point functions of affine primary fields. In [SV1] the KZ equations were identified with equations for flat sections of suitable Gauss-Manin connections, and solutions of the KZ equations were constructed in the form of multidimensional hypergeometric integrals. In [SV2] the KZ equations were considered modulo a prime number $p$, and polynomial solutions of the KZ equations modulo $p$ were constructed by an elementary procedure as suitable $p$-approximations of the hypergeometric integrals. In this paper we address the problem of whether all solutions of the KZ equations modulo $p$ are generated by the $p$-hypergeometric solutions. We consider the first nontrivial example of the KZ equations and demonstrate that, indeed, in this case, all solutions of the KZ equations modulo $p$ stem from the $p$-hypergeometric solutions.",25,4,2024
Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty,"We propose a post-hoc, computationally lightweight method to quantify predictive uncertainty in semantic image segmentation. Our approach uses conformal prediction to generate statistically valid prediction sets that are guaranteed to include the ground-truth segmentation mask at a predefined confidence level. We introduce a novel visualization technique of conformalized predictions based on heatmaps, and provide metrics to assess their empirical validity. We demonstrate the effectiveness of our approach on well-known benchmark datasets and image segmentation prediction models, and conclude with practical insights.",16,4,2024
Improving Automated Distractor Generation for Math Multiple-choice Questions with Overgenerate-and-rank,"Multiple-choice questions (MCQs) are commonly used across all levels of math education since they can be deployed and graded at a large scale. A critical component of MCQs is the distractors, i.e., incorrect answers crafted to reflect student errors or misconceptions. Automatically generating them in math MCQs, e.g., with large language models, has been challenging. In this work, we propose a novel method to enhance the quality of generated distractors through overgenerate-and-rank, training a ranking model to predict how likely distractors are to be selected by real students. Experimental results on a real-world dataset and human evaluation with math teachers show that our ranking model increases alignment with human-authored distractors, although human-authored ones are still preferred over generated ones.",19,4,2024
Learning Object Semantic Similarity with Self-Supervision,"Humans judge the similarity of two objects not just based on their visual appearance but also based on their semantic relatedness. However, it remains unclear how humans learn about semantic relationships between objects and categories. One important source of semantic knowledge is that semantically related objects frequently co-occur in the same context. For instance, forks and plates are perceived as similar, at least in part, because they are often experienced together in a ``kitchen"" or ``eating'' context. Here, we investigate whether a bio-inspired learning principle exploiting such co-occurrence statistics suffices to learn a semantically structured object representation {\em de novo} from raw visual or combined visual and linguistic input. To this end, we simulate temporal sequences of visual experience by binding together short video clips of real-world scenes showing objects in different contexts. A bio-inspired neural network model aligns close-in-time visual representations while also aligning visual and category label representations to simulate visuo-language alignment. Our results show that our model clusters object representations based on their context, e.g. kitchen or bedroom, in particular in high-level layers of the network, akin to humans. In contrast, lower-level layers tend to better reflect object identity or category. To achieve this, the model exploits two distinct strategies: the visuo-language alignment ensures that different objects of the same category are represented similarly, whereas the temporal alignment leverages that objects from the same context are frequently seen in succession to make their representations more similar. Overall, our work suggests temporal and visuo-language alignment as plausible computational principles for explaining the origins of certain forms of semantic knowledge in humans.",19,4,2024
Ordinal Behavior Classification of Student Online Course Interactions,"The study in interaction patterns between students in on-campus and MOOC-style online courses has been broadly studied for the last 11 years. Yet there remains a gap in the literature comparing the habits of students completing the same course offered in both on-campus and MOOC-style online formats. This study will look at browser-based usage patterns for students in the Georgia Tech CS1301 edx course for both the online course offered to on-campus students and the MOOCstyle course offered to anyone to determine what, if any, patterns exist between the two cohorts.",20,4,2024
Learning-to-learn enables rapid learning with phase-change memory-based in-memory computing,"There is a growing demand for low-power, autonomously learning artificial intelligence (AI) systems that can be applied at the edge and rapidly adapt to the specific situation at deployment site. However, current AI models struggle in such scenarios, often requiring extensive fine-tuning, computational resources, and data. In contrast, humans can effortlessly adjust to new tasks by transferring knowledge from related ones. The concept of learning-to-learn (L2L) mimics this process and enables AI models to rapidly adapt with only little computational effort and data. In-memory computing neuromorphic hardware (NMHW) is inspired by the brain's operating principles and mimics its physical co-location of memory and compute. In this work, we pair L2L with in-memory computing NMHW based on phase-change memory devices to build efficient AI models that can rapidly adapt to new tasks. We demonstrate the versatility of our approach in two scenarios: a convolutional neural network performing image classification and a biologically-inspired spiking neural network generating motor commands for a real robotic arm. Both models rapidly learn with few parameter updates. Deployed on the NMHW, they perform on-par with their software equivalents. Moreover, meta-training of these models can be performed in software with high-precision, alleviating the need for accurate hardware models.",22,4,2024
Distributed Learning for Wi-Fi AP Load Prediction,"The increasing cloudification and softwarization of networks foster the interplay among multiple independently managed deployments. An appealing reason for such an interplay lies in distributed Machine Learning (ML), which allows the creation of robust ML models by leveraging collective intelligence and computational power. In this paper, we study the application of the two cornerstones of distributed learning, namely Federated Learning (FL) and Knowledge Distillation (KD), on the Wi-Fi Access Point (AP) load prediction use case. The analysis conducted in this paper is done on a dataset that contains real measurements from a large Wi-Fi campus network, which we use to train the ML model under study based on different strategies. Performance evaluation includes relevant aspects for the suitability of distributed learning operation in real use cases, including the predictive performance, the associated communication overheads, or the energy consumption. In particular, we prove that distributed learning can improve the predictive accuracy centralized ML solutions by up to 93% while reducing the communication overheads and the energy cost by 80%.",22,4,2024
Integrating LSTM and BERT for Long-Sequence Data Analysis in Intelligent Tutoring Systems,"The field of Knowledge Tracing aims to understand how students learn and master knowledge over time by analyzing their historical behaviour data. To achieve this goal, many researchers have proposed Knowledge Tracing models that use data from Intelligent Tutoring Systems to predict students' subsequent actions. However, with the development of Intelligent Tutoring Systems, large-scale datasets containing long-sequence data began to emerge. Recent deep learning based Knowledge Tracing models face obstacles such as low efficiency, low accuracy, and low interpretability when dealing with large-scale datasets containing long-sequence data. To address these issues and promote the sustainable development of Intelligent Tutoring Systems, we propose a LSTM BERT-based Knowledge Tracing model for long sequence data processing, namely LBKT, which uses a BERT-based architecture with a Rasch model-based embeddings block to deal with different difficulty levels information and an LSTM block to process the sequential characteristic in students' actions. LBKT achieves the best performance on most benchmark datasets on the metrics of ACC and AUC. Additionally, an ablation study is conducted to analyse the impact of each component of LBKT's overall performance. Moreover, we used t-SNE as the visualisation tool to demonstrate the model's embedding strategy. The results indicate that LBKT is faster, more interpretable, and has a lower memory cost than the traditional deep learning based Knowledge Tracing methods.",24,4,2024
Lessons from the Use of Natural Language Inference (NLI) in Requirements Engineering Tasks,"We investigate the use of Natural Language Inference (NLI) in automating requirements engineering tasks. In particular, we focus on three tasks: requirements classification, identification of requirements specification defects, and detection of conflicts in stakeholders' requirements. While previous research has demonstrated significant benefit in using NLI as a universal method for a broad spectrum of natural language processing tasks, these advantages have not been investigated within the context of software requirements engineering. Therefore, we design experiments to evaluate the use of NLI in requirements analysis. We compare the performance of NLI with a spectrum of approaches, including prompt-based models, conventional transfer learning, Large Language Models (LLMs)-powered chatbot models, and probabilistic models. Through experiments conducted under various learning settings including conventional learning and zero-shot, we demonstrate conclusively that our NLI method surpasses classical NLP methods as well as other LLMs-based and chatbot models in the analysis of requirements specifications. Additionally, we share lessons learned characterizing the learning settings that make NLI a suitable approach for automating requirements engineering tasks.",24,4,2024
Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized Adaptive Learning,"In contrast to pedagogies like evidence-based teaching, personalized adaptive learning (PAL) distinguishes itself by closely monitoring the progress of individual students and tailoring the learning path to their unique knowledge and requirements. A crucial technique for effective PAL implementation is knowledge tracing, which models students' evolving knowledge to predict their future performance. Based on these predictions, personalized recommendations for resources and learning paths can be made to meet individual needs. Recent advancements in deep learning have successfully enhanced knowledge tracking through Deep Knowledge Tracing (DKT). This paper introduces generative AI models to further enhance DKT. Generative AI models, rooted in deep learning, are trained to generate synthetic data, addressing data scarcity challenges in various applications across fields such as natural language processing (NLP) and computer vision (CV). This study aims to tackle data shortage issues in student learning records to enhance DKT performance for PAL. Specifically, it employs TabDDPM, a diffusion model, to generate synthetic educational records to augment training data for enhancing DKT. The proposed method's effectiveness is validated through extensive experiments on ASSISTments datasets. The experimental results demonstrate that the AI-generated data by TabDDPM significantly improves DKT performance, particularly in scenarios with small data for training and large data for testing.",25,4,2024
GP-MoLFormer: A Foundation Model For Molecular Generation,"Transformer-based models trained on large and general purpose datasets consisting of molecular strings have recently emerged as a powerful tool for successfully modeling various structure-property relations. Inspired by this success, we extend the paradigm of training chemical language transformers on large-scale chemical datasets to generative tasks in this work. Specifically, we propose GP-MoLFormer, an autoregressive molecular string generator that is trained on more than 1.1B chemical SMILES. GP-MoLFormer uses a 46.8M parameter transformer decoder model with linear attention and rotary positional encodings as the base architecture. We explore the utility of GP-MoLFormer in generating novel, valid, and unique SMILES. Impressively, we find GP-MoLFormer is able to generate a significant fraction of novel, valid, and unique SMILES even when the number of generated molecules is in the 10 billion range and the reference set is over a billion. We also find strong memorization of training data in GP-MoLFormer generations, which has so far remained unexplored for chemical language models. Our analyses reveal that training data memorization and novelty in generations are impacted by the quality of the training data; duplication bias in training data can enhance memorization at the cost of lowering novelty. We evaluate GP-MoLFormer's utility and compare it with that of existing baselines on three different tasks: de novo generation, scaffold-constrained molecular decoration, and unconstrained property-guided optimization. While the first two are handled with no additional training, we propose a parameter-efficient fine-tuning method for the last task, which uses property-ordered molecular pairs as input. We call this new approach pair-tuning. Our results show GP-MoLFormer performs better or comparable with baselines across all three tasks, demonstrating its general utility.",4,4,2024
PoPE: Legendre Orthogonal Polynomials Based Position Encoding for Large Language Models,"There are several improvements proposed over the baseline Absolute Positional Encoding (APE) method used in original transformer. In this study, we aim to investigate the implications of inadequately representing positional encoding in higher dimensions on crucial aspects of the attention mechanism, the model's capacity to learn relative positional information, and the convergence of models, all stemming from the choice of sinusoidal basis functions. Through a combination of theoretical insights and empirical analyses, we elucidate how these challenges extend beyond APEs and may adversely affect the performance of Relative Positional Encoding (RPE) methods, such as Rotatory Positional Encoding (RoPE).Subsequently, we introduce an innovative solution termed Orthogonal Polynomial Based Positional Encoding (PoPE) to address some of the limitations associated with existing methods. The PoPE method encodes positional information by leveraging Orthogonal Legendre polynomials. Legendre polynomials as basis functions offers several desirable properties for positional encoding, including improved correlation structure, non-periodicity, orthogonality, and distinct functional forms among polynomials of varying orders. Our experimental findings demonstrate that transformer models incorporating PoPE outperform baseline transformer models on the $Multi30k$ English-to-German translation task, thus establishing a new performance benchmark. Furthermore, PoPE-based transformers exhibit significantly accelerated convergence rates.Additionally, we will present novel theoretical perspectives on position encoding based on the superior performance of PoPE.",29,4,2024
The Phase Transition of Reissner-Nordström Black Holes,"Under the framework of thermodynamics, the phase transition of the black hole is a general issue in general relativity. In this work, the phase transition of charged black holes is discussed carefully. The metric tensor of thermodynamics is redefined in the charged black hole, based on the Ruppeiner geometry. With the well-defined metric tensor of thermodynamics, the scalar curvature of the charged black hole is obtained. It is indicated that the scalar curvature is diverged and infinite when the mass M or charge Q are set by some values, and it is shown that the charged black hole suffers from a phase transition. At the same time, there is a phase transition from small mass to large mass or from small to high charged state. It is shown that the phase transition of a charged black hole is a common and general process and this work is meaningful for the construction of microscopic states of black holes.",30,4,2024
Torsion-rotational transitions in methanol as a probe of fundamental physical constants -- electron and proton masses,We report on the using of torsion-rotational transitions in the CH3OH and (13)CH3OH molecules to evaluate possible variations of the physical constant mu=m_e/m_p - the electron-to-proton mass ratio - from spectral observations of emission lines detected in the microwave range towards the dense molecular cloud Orion-KL. An estimate of the upper limit on the relative changes in mu is obtained by two independent ways - with (13)CH3OH lines and with the combination of (13)CH3OH and CH3OH lines. The calculated upper limit Delta mu/mu < 1.1*10^{-8} (1 sigma) is in line with the most stringent constraints on the variability of fundamental physical constants established by other astrophysical methods.,25,4,2024
Thermodynamic Topology of Quantum RN Black Holes,"This paper presents a comprehensive exploration of the thermodynamics of black holes, focusing on foundational concepts such as free energy, entropy, and topological numbers, alongside a detailed examination of quantum RN black holes. By extending the discussion to encompass the symmetry groups SO(2), SO(3)/SO(2), and SO(3) within the framework of (f(R)) gravity, the paper offers a nuanced understanding of black hole physics. Key insights include the pivotal role of free energy and entropy in understanding the thermodynamic properties of black holes, the significance of topological numbers in determining thermodynamic stability and phase transitions, and the implications of quantum mechanics and (f(R)) gravity on traditional thermodynamic concepts. This exploration not only enriches our theoretical knowledge of black holes but also sets the stage for future empirical investigations, marking a pivotal contribution to our ongoing quest to decipher the universe's mysteries.",25,4,2024
Is artificial consciousness achievable? Lessons from the human brain,"We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (structural and architectural) and extrinsic (related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it is theoretically possible that AI research can develop partial or potentially alternative forms of consciousness that is qualitatively different from the human, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word consciousness for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify what is common and what differs in AI conscious processing from full human conscious experience.",18,4,2024
Some variation of COBRA in sequential learning setup,"This research paper introduces innovative approaches for multivariate time series forecasting based on different variations of the combined regression strategy. We use specific data preprocessing techniques which makes a radical change in the behaviour of prediction. We compare the performance of the model based on two types of hyper-parameter tuning Bayesian optimisation (BO) and Usual Grid search. Our proposed methodologies outperform all state-of-the-art comparative models. We illustrate the methodologies through eight time series datasets from three categories: cryptocurrency, stock index, and short-term load forecasting.",7,4,2024
GeoViz: A Multi-View Visualization Platform for Spatio-temporal Knowledge Graph,"In this paper, we propose a multi-view visualization technology for spatio-temporal knowledge graph(STKG), which utilizes three distinct perspectives: knowledge tree, knowledge net, and knowledge map, to facilitate a comprehensive analysis of the STKG. The knowledge tree enables the visualization of hierarchical interrelation within the STKG, while the knowledge net elucidates semantic relationships among knowledge entities. Additionally, the knowledge map displays spatial and temporal distributions via spatial maps and time axes, respectively. Our visualization technology addresses the limitations inherent in single-view approaches and the deficiency of interaction in spatio-temporal perspectives evident in existing visualization methods. Moreover, we have encapsulated this technology within an integrated, open-source platform named GeoViz. A demo video of GeoViz can be accessed at https://anonymous.4open.science/r/GeoViz.",29,4,2024
Several Special Solutions of Open WDVV Equations,"The Witten-Dijkgraaf-Verlinde-Verlinde(WDVV) equations appeared in the study of two-dimensional topological field theoies in the early 1990s. An extension of the WDVV equations, called the open WDVV equations, was introduced by A.Horev and J.P.Solomon (arXiv:1210.4034). In this paper, we give some particular solutions to the open WDVV equations.",26,4,2024
Evaluating Large Language Models for Material Selection,"Material selection is a crucial step in conceptual design due to its significant impact on the functionality, aesthetics, manufacturability, and sustainability impact of the final product. This study investigates the use of Large Language Models (LLMs) for material selection in the product design process and compares the performance of LLMs against expert choices for various design scenarios. By collecting a dataset of expert material preferences, the study provides a basis for evaluating how well LLMs can align with expert recommendations through prompt engineering and hyperparameter tuning. The divergence between LLM and expert recommendations is measured across different model configurations, prompt strategies, and temperature settings. This approach allows for a detailed analysis of factors influencing the LLMs' effectiveness in recommending materials. The results from this study highlight two failure modes, and identify parallel prompting as a useful prompt-engineering method when using LLMs for material selection. The findings further suggest that, while LLMs can provide valuable assistance, their recommendations often vary significantly from those of human experts. This discrepancy underscores the need for further research into how LLMs can be better tailored to replicate expert decision-making in material selection. This work contributes to the growing body of knowledge on how LLMs can be integrated into the design process, offering insights into their current limitations and potential for future improvements.",23,4,2024
Mass-energy equivalence and the gravitational redshift: Does energy always have mass?,"One of the most widespread interpretations of the mass-energy equivalence establishes that not only can mass be transformed into energy (e.g., through nuclear fission, fusion, or annihilation) but that every type of energy also has mass (via the mass-energy equivalence formula). Here, we show that this is not always the case. With the help a few thought experiments, we show that, for instance, the electric potential energy of a charged capacitor should not contribute to the capacitor's gravitational rest mass (while still contributing to its linear momentum). That result is in agreement with the fact that light (ultimately, an electromagnetic phenomenon) has momentum but not rest mass.",18,4,2024
Trajectory analysis through entropy characterization over coded representation,"Any continuous curve in a higher dimensional space can be considered a trajectory that can be parameterized by a single variable, usually taken as time. It is well known that a continuous curve can have a fractional dimensionality, which can be estimated using already standard algorithms. However, characterizing a trajectory from an entropic perspective is far less developed. The search for such characterization leads us to use chain coding to discretize the description of a curve. Calculating the entropy density and entropy-related magnitudes from the resulting finite alphabet code becomes straightforward. In such a way, the entropy of a trajectory can be defined and used as an effective tool to assert creativity and pattern formation from a Shannon perspective. Applying the procedure to actual experimental physiological data and modelled trajectories of astronomical dynamics proved the robustness of the entropic characterization in a wealth of trajectories of different origins and the insight that can be gained from its use.",4,4,2024
Enhancing Channel Estimation in Quantized Systems with a Generative Prior,"Channel estimation in quantized systems is challenging, particularly in low-resolution systems. In this work, we propose to leverage a Gaussian mixture model (GMM) as generative prior, capturing the channel distribution of the propagation environment, to enhance a classical estimation technique based on the expectation-maximization (EM) algorithm for one-bit quantization. Thereby, a maximum a posteriori (MAP) estimate of the most responsible mixture component is inferred for a quantized received signal, which is subsequently utilized in the EM algorithm as side information. Numerical results demonstrate the significant performance improvement of our proposed approach over both a simplistic Gaussian prior and current state-of-the-art channel estimators. Furthermore, the proposed estimation framework exhibits adaptability to higher resolution systems and alternative generative priors.",26,4,2024
Expected biases in the distribution of consecutive primes,"In 2016 Lemke Oliver and Soundararajan examined the gaps between the first hundred million primes and observed biases in their distributions modulo 10. Given our work on the evolution of the populations of various gaps across stages of Eratosthenes sieve, the observed biases are totally expected.The biases observed by Lemke Oliver and Soundararajan are a wonderful example for contrasting the computational range with the asymptotic range for the populations of the gaps between primes. The observed biases are the combination of two phenomena: (a) very small gaps, say $2 \le g \le 30$, get off to quick starts and over the first 100 million primes larger gaps are too early in their evolution; and (b) the assignment of small gaps across the residue classes disadvantages some of those classes - until enormous primes, far beyond the computational range.For modulus 10 and a few other bases, we aggregate the gaps by residue class and track the evolution of these teams as Eratosthenes sieve continues. The relative populations across these teams start with biases across the residue classes. These initial biases fade as the sieve continues. The OS enumeration strongly agrees with a uniform sampling at the corresponding stage of the sieve. The biases persist well beyond the computational range, but they are ultimately transient.",29,4,2024
Design Fiction as Breaching Experiment: An Interdisciplinary Methodology for Understanding the Acceptability and Adoption of Future Technologies,"HCI is fundamentally occupied with the problem of the future and understanding the acceptability and adoption challenges that future and emerging technologies face from the viewpoint of their being situated in everyday life. This paper explicates an interdisciplinary approach towards addressing the problem and understanding acceptability and adoption challenges that leverages design fiction as breaching experiment. Design fiction is an arts based approach to exploring the future, breaching experiments a social science method for explicating common sense reasoning and surfacing the taken for granted expectations societys members have and hold about situated action and how it should work. Both approaches have previously been employed in HCI, but this the first time they have been combined to enable HCI researchers to provoke through design the acceptability and adoption challenges that confront future and emerging technologies.",29,4,2024
Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G,"Building future wireless systems that support services like digital twins (DTs) is challenging to achieve through advances to conventional technologies like meta-surfaces. While artificial intelligence (AI)-native networks promise to overcome some limitations of wireless technologies, developments still rely on AI tools like neural networks. Such tools struggle to cope with the non-trivial challenges of the network environment and the growing demands of emerging use cases. In this paper, we revisit the concept of AI-native wireless systems, equipping them with the common sense necessary to transform them into artificial general intelligence (AGI)-native systems. These systems acquire common sense by exploiting different cognitive abilities such as perception, analogy, and reasoning, that enable them to generalize and deal with unforeseen scenarios. Towards developing the components of such a system, we start by showing how the perception module can be built through abstracting real-world elements into generalizable representations. These representations are then used to create a world model, founded on principles of causality and hyper-dimensional (HD) computing, that aligns with intuitive physics and enables analogical reasoning, that define common sense. Then, we explain how methods such as integrated information theory play a role in the proposed intent-driven and objective-driven planning methods that maneuver the AGI-native network to take actions. Next, we discuss how an AGI-native network can enable use cases related to human and autonomous agents: a) analogical reasoning for next-generation DTs, b) synchronized and resilient experiences for cognitive avatars, and c) brain-level metaverse experiences like holographic teleportation. Finally, we conclude with a set of recommendations to build AGI-native systems. Ultimately, we envision this paper as a roadmap for the beyond 6G era.",29,4,2024
sDAC -- Semantic Digital Analog Converter for Semantic Communications,"In this paper, we propose a novel semantic digital analog converter (sDAC) for the compatibility of semantic communications and digital communications. Most of the current semantic communication systems are based on the analog modulations, ignoring their incorporation with digital communication systems, which are more common in practice. In fact, quantization methods in traditional communication systems are not appropriate for use in the era of semantic communication as these methods do not consider the semantic information inside symbols. In this case, any bit flip caused by channel noise can lead to a great performance drop. To address this challenge, sDAC is proposed. It is a simple yet efficient and generative module used to realize digital and analog bi-directional conversion. On the transmitter side, continuous values from the encoder are converted to binary bits and then can be modulated by any existing methods. After transmitting through the noisy channel, these bits get demodulated by paired methods and converted back to continuous values for further semantic decoding. The whole progress does not depend on any specific semantic model, modulation methods, or channel conditions. In the experiment section, the performance of sDAC is tested across different semantic models, semantic tasks, modulation methods, channel conditions and quantization orders. Test results show that the proposed sDAC has great generative properties and channel robustness.",26,4,2024
Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics,"In the last years, artificial intelligence (AI) in clinical decision support systems (CDSS) played a key role in harnessing machine learning and deep learning architectures. Despite their promising capabilities, the lack of transparency and explainability of AI models poses significant challenges, particularly in medical contexts where reliability is a mandatory aspect. Achieving transparency without compromising predictive accuracy remains a key challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the predictive power of CNN-derived features with the interpretability inherent in radiomic features. Rad4XCNN diverges from conventional methods based on saliency map, by associating intelligible meaning to CNN-derived features by means of Radiomics, offering new perspectives on explanation methods beyond visualization maps. Using a breast cancer classification task as a case study, we evaluated Rad4XCNN on ultrasound imaging datasets, including an online dataset and two in-house datasets for internal and external validation. Some key results are: i) CNN-derived features guarantee more robust accuracy when compared against ViT-derived and radiomic features; ii) conventional visualization map methods for explanation present several pitfalls; iii) Rad4XCNN does not sacrifice model accuracy for their explainability; iv) Rad4XCNN provides global explanation insights enabling the physician to analyze the model outputs and findings. In addition, we highlight the importance of integrating interpretability into AI models for enhanced trust and adoption in clinical practice, emphasizing how our method can mitigate some concerns related to explainable AI methods.",26,4,2024
Speech Technology Services for Oral History Research,"Oral history is about oral sources of witnesses and commentors on historical events. Speech technology is an important instrument to process such recordings in order to obtain transcription and further enhancements to structure the oral account In this contribution we address the transcription portal and the webservices associated with speech processing at BAS, speech solutions developed at LINDAT, how to do it yourself with Whisper, remaining challenges, and future developments.",26,4,2024
Efficient Exploration of Image Classifier Failures with Bayesian Optimization and Text-to-Image Models,"Image classifiers should be used with caution in the real world. Performance evaluated on a validation set may not reflect performance in the real world. In particular, classifiers may perform well for conditions that are frequently encountered during training, but poorly for other infrequent conditions. In this study, we hypothesize that recent advances in text-to-image generative models make them valuable for benchmarking computer vision models such as image classifiers: they can generate images conditioned by textual prompts that cause classifier failures, allowing failure conditions to be described with textual attributes. However, their generation cost becomes an issue when a large number of synthetic images need to be generated, which is the case when many different attribute combinations need to be tested. We propose an image classifier benchmarking method as an iterative process that alternates image generation, classifier evaluation, and attribute selection. This method efficiently explores the attributes that ultimately lead to poor behavior detection.",26,4,2024
Comparing varieties generated by certain wreath products of groups,"An exhaustive version of the thesis to a talk presented at the Groups \& Algebras in Bicocca Conference (GABY), University of Milano-Bicocca (Milan, Italy), June 17 to June 21, 2024.",25,4,2024
Adaptive Semantic Token Selection for AI-native Goal-oriented Communications,"In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",25,4,2024
Digital ASIC Design with Ongoing LLMs: Strategies and Prospects,"The escalating complexity of modern digital systems has imposed significant challenges on integrated circuit (IC) design, necessitating tools that can simplify the IC design flow. The advent of Large Language Models (LLMs) has been seen as a promising development, with the potential to automate the generation of Hardware Description Language (HDL) code, thereby streamlining digital IC design. However, the practical application of LLMs in this area faces substantial hurdles. Notably, current LLMs often generate HDL code with small but critical syntax errors and struggle to accurately convey the high-level semantics of circuit designs. These issues significantly undermine the utility of LLMs for IC design, leading to misinterpretations and inefficiencies.In response to these challenges, this paper presents targeted strategies to harness the capabilities of LLMs for digital ASIC design. We outline approaches that improve the reliability and accuracy of HDL code generation by LLMs. As a practical demonstration of these strategies, we detail the development of a simple three-phase Pulse Width Modulation (PWM) generator. This project, part of the ""Efabless AI-Generated Open-Source Chip Design Challenge,"" successfully passed the Design Rule Check (DRC) and was fabricated, showcasing the potential of LLMs to enhance digital ASIC design. This work underscores the feasibility and benefits of integrating LLMs into the IC design process, offering a novel approach to overcoming the complexities of modern digital systems.",25,4,2024
Topology of restricted phase space thermodynamics in Kerr-Sen-Ads black holes,"In this study, we investigate the thermodynamic topology of the Kerr-Sen-Ads black hole in restricted phase space. In the restricted phase space, a new parameter, central charge $C$, and its conjugate parameter $\mu$ are introduced, omitting the well-known $PdV$ term in the first law of black hole thermodynamics. We study the local and global topology of the black hole by considering the black hole solution as topological defects in the free energy landscape. We compute the winding number and the total topological number at the thermodynamic defects. For our analysis, we have considered five ensembles of Kerr-Sen-Ads black holes in restricted phase space: fixed $(Q, J, C)$, fixed $(\phi, J, C)$, fixed $(Q,\Omega, C)$, fixed $(Q, J, \mu)$, and fixed $(\phi,\Omega, C)$, where $Q$ is the electric charge, $J$ is the angular momentum, $C$ is the central charge, $\phi$ is the electric potential conjugate to charge, $\Omega$ is the angular frequency conjugate to $J$, and finally, $\mu$ is the chemical potential. In the fixed $(Q, J, C)$, fixed $(\phi, J, C)$, and fixed $(Q, J, \mu)$ ensembles, we find a topological charge of $+1$. In the fixed $(Q,\Omega, C)$ and fixed $(\phi, \Omega, C)$ ensembles, depending on the values of the thermodynamic parameters, we find topological charges of $-1$, $0$, and $+1$. Interestingly, in ensembles where we find the topological charge to be $0$, we observe both Hawking-Page and Davies type phase transitions. We show that both types of these phase transitions can be studied using a common vector field, and the topological charges associated with Davies type and Hawking-Page phase transitions are $-1$ and $+1$, respectively.",24,4,2024
CausalDisco: Causal discovery using knowledge graph link prediction,"Causal discovery is a process of discovering new causal relations from observational data. Traditional causal discovery methods often suffer from issues related to missing data To address these issues, this paper presents a novel approach called CausalDisco that formulates causal discovery as a knowledge graph completion problem. More specifically, the task of discovering causal relations is mapped to the task of knowledge graph link prediction. CausalDisco supports two types of discovery: causal explanation and causal prediction. The causal relations have weights representing the strength of the causal association between entities in the knowledge graph. An evaluation of this approach uses a benchmark dataset of simulated videos for causal reasoning, CLEVRER-Humans, and compares the performance of multiple knowledge graph embedding algorithms. In addition, two distinct dataset splitting approaches are utilized within the evaluation: (1) random-based split, which is the method typically used to evaluate link prediction algorithms, and (2) Markov-based split, a novel data split technique for evaluating link prediction that utilizes the Markovian property of the causal relation. Results show that using weighted causal relations improves causal discovery over the baseline without weighted relations.",23,4,2024
Evaluating LLMs for Hardware Design and Test,"Large Language Models (LLMs) have demonstrated capabilities for producing code in Hardware Description Languages (HDLs). However, most of the focus remains on their abilities to write functional code, not test code. The hardware design process consists of both design and test, and so eschewing validation and verification leaves considerable potential benefit unexplored, given that a design and test framework may allow for progress towards full automation of the digital design pipeline. In this work, we perform one of the first studies exploring how a LLM can both design and test hardware modules from provided specifications. Using a suite of 8 representative benchmarks, we examined the capabilities and limitations of the state-of-the-art conversational LLMs when producing Verilog for functional and verification purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received the functional chip.",23,4,2024
Meat Meets Machine! Multiscale Competency Enables Causal Learning,"Biological intelligence uses a ""multiscale competency architecture"" (MCA). It exhibits adaptive, goal directed behaviour at all scales, from cells to organs to organisms. In contrast, machine intelligence is only adaptive and goal directed at a high level. Learned policies are passively interpreted using abstractions (e.g. arithmetic) embodied in static interpreters (e.g. x86). Biological intelligence excels at causal learning. Machine intelligence does not. Previous work showed causal learning follows from weak policy optimisation, which is hindered by presupposed abstractions in silico. Here we formalise MCAs as nested ""agentic abstraction layers"", to understand how they might learn causes. We show that weak policy optimisation at low levels enables weak policy optimisation at high. This facilitates what we call ""multiscale causal learning"" and high level goal directed behaviour. We argue that by engineering human abstractions in silico we disconnect high level goal directed behaviour from the low level goal directed behaviour that gave rise to it. This inhibits causal learning, and we speculate this is one reason why human recall would be accompanied by feeling, and in silico recall not.",23,4,2024
Combined Compromise for Ideal Solution (CoCoFISo): a multi-criteria decision-making based on the CoCoSo method algorithm,"Each decision-making tool should be tested and validated in real case studies to be practical and fit to global problems. The application of multi-criteria decision-making methods (MCDM) is currently a trend to rank alternatives. In the literature, there are several multi-criteria decision-making methods according to their classification. During our experimentation on the Combined Compromise Solution (CoCoSo) method, we encountered its limits for real cases. The authors examined the applicability of the CoCoFISo method (improved version of combined compromise solution), by a real case study in a university campus and compared the obtained results to other MCDMs such as Preference Ranking Organisation Method for Enrichment Evaluations (PROMETHEE), Weighted Sum Method (WSM) and Technique for Order Preference by Similarity to the Ideal Solution (TOPSIS). Our research finding indicates that CoCoSo is an applied method that has been developed to solve complex multi variable assessment problems, while CoCoFISo can improve the shortages observed in CoCoSo and deliver stable outcomes compared to other developed tools. The findings imply that application of CoCoFISo is suggested to decision makers, experts and researchers while they are facing practical challenges and sensitive questions regarding the utilization of a reliable decision-making method. Unlike many prior studies, the current version of CoCoSo is unique, original and is presented for the first time. Its performance was approved using several strategies and examinations.",22,4,2024
CNN-Based Equalization for Communications: Achieving Gigabit Throughput with a Flexible FPGA Hardware Architecture,"To satisfy the growing throughput demand of data-intensive applications, the performance of optical communication systems increased dramatically in recent years. With higher throughput, more advanced equalizers are crucial, to compensate for impairments caused by inter-symbol interference (ISI). The latest research shows that artificial neural network (ANN)-based equalizers are promising candidates to replace traditional algorithms for high-throughput communications. On the other hand, not only throughput but also flexibility is a main objective of beyond-5G and 6G communication systems. A platform that is able to satisfy the strict throughput and flexibility requirements of modern communication systems are field programmable gate arrays (FPGAs). Thus, in this work, we present a high-performance FPGA implementation of an ANN-based equalizer, which meets the throughput requirements of modern optical communication systems. Further, our architecture is highly flexible since it includes a variable degree of parallelism (DOP) and therefore can also be applied to low-cost or low-power applications which is demonstrated for a magnetic recording channel. The implementation is based on a cross-layer design approach featuring optimizations from the algorithm down to the hardware architecture, including a detailed quantization analysis. Moreover, we present a framework to reduce the latency of the ANN-based equalizer under given throughput constraints. As a result, the bit error ratio (BER) of our equalizer for the optical fiber channel is around four times lower than that of a conventional one, while the corresponding FPGA implementation achieves a throughput of more than 40 GBd, outperforming a high-performance graphics processing unit (GPU) by three orders of magnitude for a similar batch size.",22,4,2024
Towards Causal Interpretation of Sexual Orientation in Regression Analysis: Applications and Challenges,"This study presents an approach to analyze health disparities in Sexual and Gender Minority (SGM) populations, with a focus on the role of social support levels as an example to allow causal interpretations of regression models. We advocate for precisely defining the exposure variable and incorporating mediators into analyses, to address the limitations of comparing counterfactual outcomes solely between SGM and heterosexual populations. We define sexual orientation into domains (attraction, behavior, and identity), and emphasize a consideration of these elements either separately or together, depending on the research question. We also introduce social support measured before and after the disclosure of sexual orientation to facilitate inference. We illustrate this approach by examining the association between SGM status and depression diagnosis with data from the 2020 and 2021 National Health Interview Survey. We find a direct effect of SGM status on depression (OR: 3.07, 95% CI: 2.64 - 3.58) and no indirect effect through social support (OR: 1.07, 95% CI: 0.87-1.31). Our research emphasizes the necessity of the comprehensive measurement of sexual orientation and a focus on intervenable variables like social support in order to empower SGM communities and address SGM related health inequalities.",22,4,2024
Accelerating Medical Knowledge Discovery through Automated Knowledge Graph Generation and Enrichment,"Knowledge graphs (KGs) serve as powerful tools for organizing and representing structured knowledge. While their utility is widely recognized, challenges persist in their automation and completeness. Despite efforts in automation and the utilization of expert-created ontologies, gaps in connectivity remain prevalent within KGs. In response to these challenges, we propose an innovative approach termed ``Medical Knowledge Graph Automation (M-KGA)"". M-KGA leverages user-provided medical concepts and enriches them semantically using BioPortal ontologies, thereby enhancing the completeness of knowledge graphs through the integration of pre-trained embeddings. Our approach introduces two distinct methodologies for uncovering hidden connections within the knowledge graph: a cluster-based approach and a node-based approach. Through rigorous testing involving 100 frequently occurring medical concepts in Electronic Health Records (EHRs), our M-KGA framework demonstrates promising results, indicating its potential to address the limitations of existing knowledge graph automation techniques.",21,4,2024
A SER-based Device Selection Mechanism in Multi-bits Quantization Federated Learning,"The quality of wireless communication will directly affect the performance of federated learning (FL), so this paper analyze the influence of wireless communication on FL through symbol error rate (SER). In FL system, non-orthogonal multiple access (NOMA) can be used as the basic communication framework to reduce the communication congestion and interference caused by multiple users, which takes advantage of the superposition characteristics of wireless channels. The Minimum Mean Square Error (MMSE) based serial interference cancellation (SIC) technology is used to recover the gradient of each terminal node one by one at the receiving end. In this paper, the gradient parameters are quantized into multiple bits to retain more gradient information to the maximum extent and to improve the tolerance of transmission errors. On this basis, we designed the SER-based device selection mechanism (SER-DSM) to ensure that the learning performance is not affected by users with bad communication conditions, while accommodating as many users as possible to participate in the learning process, which is inclusive to a certain extent. The experiments show the influence of multi-bit quantization of gradient on FL and the necessity and superiority of the proposed SER-based device selection mechanism.",20,4,2024
Bayesian Inference for Estimating Heat Sources through Temperature Assimilation,"This paper introduces a Bayesian inference framework for two-dimensional steady-state heat conduction, focusing on the estimation of unknown distributed heat sources in a thermally-conducting medium with uniform conductivity. The goal is to infer heater locations, strengths, and shapes using temperature assimilation in the Euclidean space, employing a Fourier series to represent each heater's shape. The Markov Chain Monte Carlo (MCMC) method, incorporating the random-walk Metropolis-Hasting algorithm and parallel tempering, is utilized for posterior distribution exploration in both unbounded and wall-bounded domains. Strong correlations between heat strength and heater area prompt caution against simultaneously estimating these two quantities. It is found that multiple solutions arise in cases where the number of temperature sensors is less than the number of unknown states. Moreover, smaller heaters introduce greater uncertainty in estimated strength. The diffusive nature of heat conduction smooths out any deformations in the temperature contours, especially in the presence of multiple heaters positioned near each other, impacting convergence. In wall-bounded domains with Neumann boundary conditions, the inference of heater parameters tends to be more accurate than in unbounded domains.",18,4,2024
NL2FOL: Translating Natural Language to First-Order Logic for Logical Fallacy Detection,"Logical fallacies are common errors in reasoning that undermine the logic of an argument. Automatically detecting logical fallacies has important applications in tracking misinformation and validating claims. In this paper, we design a process to reliably detect logical fallacies by translating natural language to First-order Logic (FOL) step-by-step using Large Language Models (LLMs). We then utilize Satisfiability Modulo Theory (SMT) solvers to reason about the validity of the formula and classify inputs as either a fallacy or valid statement. Our model also provides a novel means of utilizing LLMs to interpret the output of the SMT solver, offering insights into the counter-examples that illustrate why a given sentence is considered a logical fallacy. Our approach is robust, interpretable and does not require training data or fine-tuning. We evaluate our model on a mixed dataset of fallacies and valid sentences. The results demonstrate improved performance compared to end-to-end LLMs, with our classifier achieving an F1-score of 71\% on the Logic dataset. The approach is able to generalize effectively, achieving an F1-score of 73% on the challenge set, LogicClimate, outperforming state-of-the-art models by 21% despite its much smaller size.",18,4,2024
Long-term Human Participation Assessment In Collaborative Learning Environments Using Dynamic Scene Analysis,"The paper develops datasets and methods to assess student participation in real-life collaborative learning environments. In collaborative learning environments, students are organized into small groups where they are free to interact within their group. Thus, students can move around freely causing issues with strong pose variation, move out and re-enter the camera scene, or face away from the camera. We formulate the problem of assessing student participation into two subproblems: (i) student group detection against strong background interference from other groups, and (ii) dynamic participant tracking within the group. A massive independent testing dataset of 12,518,250 student label instances, of total duration of 21 hours and 22 minutes of real-life videos, is used for evaluating the performance of our proposed method for student group detection. The proposed method of using multiple image representations is shown to perform equally or better than YOLO on all video instances. Over the entire dataset, the proposed method achieved an F1 score of 0.85 compared to 0.80 for YOLO. Following student group detection, the paper presents the development of a dynamic participant tracking system for assessing student group participation through long video sessions. The proposed dynamic participant tracking system is shown to perform exceptionally well, missing a student in just one out of 35 testing videos. In comparison, a state of the art method fails to track students in 14 out of the 35 testing videos. The proposed method achieves 82.3% accuracy on an independent set of long, real-life collaborative videos.",14,4,2024
"A Cloud-Edge Framework for Energy-Efficient Event-Driven Control: An Integration of Online Supervised Learning, Spiking Neural Networks and Local Plasticity Rules","This paper presents a novel cloud-edge framework for addressing computational and energy constraints in complex control systems. Our approach centers around a learning-based controller using Spiking Neural Networks (SNN) on physical plants. By integrating a biologically plausible learning method with local plasticity rules, we harness the efficiency, scalability, and low latency of SNNs. This design replicates control signals from a cloud-based controller directly on the plant, reducing the need for constant plant-cloud communication. The plant updates weights only when errors surpass predefined thresholds, ensuring efficiency and robustness in various conditions. Applied to linear workbench systems and satellite rendezvous scenarios, including obstacle avoidance, our architecture dramatically lowers normalized tracking error by 96% with increased network size. The event-driven nature of SNNs minimizes energy consumption, utilizing only about 111 nJ (0.3% of conventional computing requirements). The results demonstrate the system's adjustment to changing work environments and its efficient use of computational and energy resources, with a moderate increase in energy consumption of 27.2% and 37% for static and dynamic obstacles, respectively, compared to non-obstacle scenarios.",12,4,2024
Regime Identification for Improving Causal Analysis in Non-stationary Timeseries,"Time series data from real-world systems often display non-stationary behavior, indicating varying statistical characteristics over time. This inherent variability poses significant challenges in deciphering the underlying structural relationships within the data, particularly in correlation and causality analyses, model stability, etc. Recognizing distinct segments or regimes within multivariate time series data, characterized by relatively stable behavior and consistent statistical properties over extended periods, becomes crucial. In this study, we apply the regime identification (RegID) technique to multivariate time series, fundamentally designed to unveil locally stationary segments within data. The distinguishing features between regimes are identified using covariance matrices in a Riemannian space. We aim to highlight how regime identification contributes to improving the discovery of causal structures from multivariate non-stationary time series data. Our experiments, encompassing both synthetic and real-world datasets, highlight the effectiveness of regime-wise time series causal analysis. We validate our approach by first demonstrating improved causal structure discovery using synthetic data where the ground truth causal relationships are known. Subsequently, we apply this methodology to climate-ecosystem dataset, showcasing its applicability in real-world scenarios.",12,4,2024
A geometric framework for interstellar discourse on fundamental physical structures,"This paper considers the possibility that abstract thinking and advanced synthesis skills might encourage extraterrestrial civilizations to accept communication with mankind on Earth. For this purpose, a notation not relying upon the use of alphabet and numbers is proposed, in order to denote just some basic geometric structures of current physical theories: vector fields, one-form fields, and tensor fields of arbitrary order. An advanced civilization might appreciate the way here proposed to achieve a concise description of electromagnetism and general relativity, and hence it might accept the challenge of responding to our signals. The abstract symbols introduced in this paper to describe the basic structures of physical theories are encoded into black and white bitmap images that can be easily converted into short bit sequences and modulated on a carrier wave for radio transmission.",7,4,2024
Physics-informed Data-driven Cavitation Model for a Specific MG EOS,"We present a novel one-fluid cavitation model of a specific Mie-Grüneisen equation of state(EOS), named polynomial EOS, based on an artificial neural network. Not only the physics-informed equation but also the experimental data are embedded into the proposed model by an optimization problem. The physics-informed data-driven model provides the concerned pressure within the cavitation region, where the density tends to zero when the pressure falls below the saturated pressure. The present model is then applied to computing the challenging compressible multi-phase flow simulation, such as nuclear and underwater explosions. Numerical simulations show that our model in application agrees well with the corresponding experimental data, ranging from one dimension to three dimensions with the $h-$adaptive mesh refinement algorithm and load balance techniques in the structured and unstructured grid.",5,4,2024
YOLOv5 vs. YOLOv8 in Marine Fisheries: Balancing Class Detection and Instance Count,"This paper presents a comparative study of object detection using YOLOv5 and YOLOv8 for three distinct classes: artemia, cyst, and excrement. In this comparative study, we analyze the performance of these models in terms of accuracy, precision, recall, etc. where YOLOv5 often performed better in detecting Artemia and cysts with excellent precision and accuracy. However, when it came to detecting excrement, YOLOv5 faced notable challenges and limitations. This suggests that YOLOv8 offers greater versatility and adaptability in detection tasks while YOLOv5 may struggle in difficult situations and may need further fine-tuning or specialized training to enhance its performance. The results show insights into the suitability of YOLOv5 and YOLOv8 for detecting objects in challenging marine environments, with implications for applications such as ecological research.",1,4,2024
Text and Audio Simplification: Human vs. ChatGPT,"Text and audio simplification to increase information comprehension are important in healthcare. With the introduction of ChatGPT, an evaluation of its simplification performance is needed. We provide a systematic comparison of human and ChatGPT simplified texts using fourteen metrics indicative of text difficulty. We briefly introduce our online editor where these simplification tools, including ChatGPT, are available. We scored twelve corpora using our metrics: six text, one audio, and five ChatGPT simplified corpora. We then compare these corpora with texts simplified and verified in a prior user study. Finally, a medical domain expert evaluated these texts and five, new ChatGPT simplified versions. We found that simple corpora show higher similarity with the human simplified texts. ChatGPT simplification moves metrics in the right direction. The medical domain expert evaluation showed a preference for the ChatGPT style, but the text itself was rated lower for content retention.",29,4,2024
Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model,"Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data pose unique challenges for model training and application. However, the dependency on high-quality data for effective in-context learning raises questions about the feasibility of these models when encountering with the inevitable variations and errors inherent in real-world medical data. In this paper, we introduce MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions. MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters. This highlights the potential of leveraging general-domain LLMs for domain-specific tasks and offers a sustainable and cost-effective alternative to traditional LMM developments. Moreover, the robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications.",29,4,2024
101 Billion Arabic Words Dataset,"In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.",29,4,2024
GPT-4 passes most of the 297 written Polish Board Certification Examinations,"Introduction: Recently, the effectiveness of Large Language Models (LLMs) has increased rapidly, allowing them to be used in a great number of applications. However, the risks posed by the generation of false information through LLMs significantly limit their applications in sensitive areas such as healthcare, highlighting the necessity for rigorous validations to determine their utility and reliability. To date, no study has extensively compared the performance of LLMs on Polish medical examinations across a broad spectrum of specialties on a very large dataset. Objectives: This study evaluated the performance of three Generative Pretrained Transformer (GPT) models on the Polish Board Certification Exam (Państwowy Egzamin Specjalizacyjny, PES) dataset, which consists of 297 tests. Methods: We developed a software program to download and process PES exams and tested the performance of GPT models using OpenAI Application Programming Interface. Results: Our findings reveal that GPT-3.5 did not pass any of the analyzed exams. In contrast, the GPT-4 models demonstrated the capability to pass the majority of the exams evaluated, with the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The performance of the GPT models varied significantly, displaying excellence in exams related to certain specialties while completely failing others. Conclusions: The significant progress and impressive performance of LLM models hold great promise for the increased application of AI in the field of medicine in Poland. For instance, this advancement could lead to the development of AI-based medical assistants for healthcare professionals, enhancing the efficiency and accuracy of medical services.",29,4,2024
Towards Unbiased Evaluation of Detecting Unanswerable Questions in EHRSQL,"Incorporating unanswerable questions into EHR QA systems is crucial for testing the trustworthiness of a system, as providing non-existent responses can mislead doctors in their diagnoses. The EHRSQL dataset stands out as a promising benchmark because it is the only dataset that incorporates unanswerable questions in the EHR QA system alongside practical questions. However, in this work, we identify a data bias in these unanswerable questions; they can often be discerned simply by filtering with specific N-gram patterns. Such biases jeopardize the authenticity and reliability of QA system evaluations. To tackle this problem, we propose a simple debiasing method of adjusting the split between the validation and test sets to neutralize the undue influence of N-gram filtering. By experimenting on the MIMIC-III dataset, we demonstrate both the existing data bias in EHRSQL and the effectiveness of our data split strategy in mitigating this bias.",29,4,2024
Improve Academic Query Resolution through BERT-based Question Extraction from Images,"Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.",28,4,2024
Transfer Learning and Transformer Architecture for Financial Sentiment Analysis,"Financial sentiment analysis allows financial institutions like Banks and Insurance Companies to better manage the credit scoring of their customers in a better way. Financial domain uses specialized mechanisms which makes sentiment analysis difficult. In this paper, we propose a pre-trained language model which can help to solve this problem with fewer labelled data. We extend on the principles of Transfer learning and Transformation architecture principles and also take into consideration recent outbreak of pandemics like COVID. We apply the sentiment analysis to two different sets of data. We also take smaller training set and fine tune the same as part of the model.",28,4,2024
Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications,"In recent times Large Language Models have exhibited tremendous capabilities, especially in the areas of mathematics, code generation and general-purpose reasoning. However for specialized domains especially in applications that require parsing and analyzing large chunks of numeric or tabular data even state-of-the-art (SOTA) models struggle. In this paper, we introduce a new approach to solving domain-specific tabular data analysis tasks by presenting a unique RAG workflow that mitigates the scalability issues of existing tabular LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel approach to fine-tune embedding models for tabular Retrieval-Augmentation Generation (RAG) applications. Embedding models form a crucial component in the RAG workflow and even current SOTA embedding models struggle as they are predominantly trained on textual datasets and thus underperform in scenarios involving complex tabular data. The evaluation results showcase that our approach not only outperforms current SOTA embedding models in this domain but also does so with a notably smaller and more efficient model structure.",28,4,2024
Lightweight Conceptual Dictionary Learning for Text Classification Using Information Compression,"We propose a novel, lightweight supervised dictionary learning framework for text classification based on data compression and representation. This two-phase algorithm initially employs the Lempel-Ziv-Welch (LZW) algorithm to construct a dictionary from text datasets, focusing on the conceptual significance of dictionary elements. Subsequently, dictionaries are refined considering label data, optimizing dictionary atoms to enhance discriminative power based on mutual information and class distribution. This process generates discriminative numerical representations, facilitating the training of simple classifiers such as SVMs and neural networks. We evaluate our algorithm's information-theoretic performance using information bottleneck principles and introduce the information plane area rank (IPAR) as a novel metric to quantify the information-theoretic performance. Tested on six benchmark text datasets, our algorithm competes closely with top models, especially in limited-vocabulary contexts, using significantly fewer parameters. \review{Our algorithm closely matches top-performing models, deviating by only ~2\% on limited-vocabulary datasets, using just 10\% of their parameters. However, it falls short on diverse-vocabulary datasets, likely due to the LZW algorithm's constraints with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.",28,4,2024
MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning,"The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.",27,4,2024
Text Quality-Based Pruning for Efficient Training of Language Models,"In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a ""quality score"".By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training.For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.",26,4,2024
The Mercurial Top-Level Ontology of Large Language Models,"In our work, we systematize and analyze implicit ontological commitments in the responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study. We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate. The paper proposes an approach to understanding the ontological commitments of LLMs by defining ontology as a theory that provides a systematic account of the ontological commitments of some text. We investigate the ontological assumptions of ChatGPT and present a systematized account, i.e., GPT's top-level ontology. This includes a taxonomy, which is available as an OWL file, as well as a discussion about ontological assumptions (e.g., about its mereology or presentism). We show that in some aspects GPT's top-level ontology is quite similar to existing top-level ontologies. However, there are significant challenges arising from the flexible nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency.",26,4,2024
On the Limitations of Embedding Based Methods for Measuring Functional Correctness for Code Generation,"The task of code generation from natural language (NL2Code) has become extremely popular, especially with the advent of Large Language Models (LLMs). However, efforts to quantify and track this progress have suffered due to a lack of reliable metrics for functional correctness. While popular benchmarks like HumanEval have test cases to enable reliable evaluation of correctness, it is time-consuming and requires human effort to collect test cases. As an alternative several reference-based evaluation metrics have been proposed, with embedding-based metrics like CodeBERTScore being touted as having a high correlation with human preferences and functional correctness. In our work, we analyze the ability of embedding-based metrics like CodeBERTScore to measure functional correctness and other helpful constructs like editing effort by analyzing outputs of ten models over two popular code generation benchmarks. Our results show that while they have a weak correlation with functional correctness (0.16), they are strongly correlated (0.72) with editing effort.",26,4,2024
Mining patterns in syntax trees to automate code reviews of student solutions for programming exercises,"In programming education, providing manual feedback is essential but labour-intensive, posing challenges in consistency and timeliness. We introduce ECHO, a machine learning method to automate the reuse of feedback in educational code reviews by analysing patterns in abstract syntax trees. This study investigates two primary questions: whether ECHO can predict feedback annotations to specific lines of student code based on previously added annotations by human reviewers (RQ1), and whether its training and prediction speeds are suitable for using ECHO for real-time feedback during live code reviews by human reviewers (RQ2). Our results, based on annotations from both automated linting tools and human reviewers, show that ECHO can accurately and quickly predict appropriate feedback annotations. Its efficiency in processing and its flexibility in adapting to feedback patterns can significantly reduce the time and effort required for manual feedback provisioning in educational settings.",26,4,2024
"Empowering IoT Applications with Flexible, Energy-Efficient Remote Management of Low-Power Edge Devices","In the context of the Internet of Things (IoT), reliable and energy-efficient provision of IoT applications has become critical. Equipping IoT systems with tools that enable a flexible, well-performing, and automated way of monitoring and managing IoT edge devices is an essential prerequisite. In current IoT systems, low-power edge appliances have been utilized in a way that can not be controlled and re-configured in a timely manner. Hence, conducting a trade-off solution between manageability, performance and design requirements are demanded. This paper introduces a novel approach for fine-grained monitoring and managing individual micro-services within low-power edge devices, which improves system reliability and energy efficiency. The proposed method enables operational flexibility for IoT edge devices by leveraging a modularization technique. Following a review of existing solutions for remote-managed IoT services, a detailed description of the suggested approach is presented. Also, to explore the essential design principles that must be considered in this approach, the suggested architecture is elaborated in detail. Finally, the advantages of the proposed solution to deal with disruptions are demonstrated in the proof of concept-based experiments.",26,4,2024
HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models,"Hate speech encompasses verbal, written, or behavioral communication that targets derogatory or discriminatory language against individuals or groups based on sensitive characteristics. Automated hate speech detection plays a crucial role in curbing its propagation, especially across social media platforms. Various methods, including recent advancements in deep learning, have been devised to address this challenge. In this study, we introduce HateTinyLLM, a novel framework based on fine-tuned decoder-only tiny large language models (tinyLLMs) for efficient hate speech detection. Our experimental findings demonstrate that the fine-tuned HateTinyLLM outperforms the pretrained mixtral-7b model by a significant margin. We explored various tiny LLMs, including PY007/TinyLlama-1.1B-step-50K-105b, Microsoft/phi-2, and facebook/opt-1.3b, and fine-tuned them using LoRA and adapter methods. Our observations indicate that all LoRA-based fine-tuned models achieved over 80\% accuracy.",26,4,2024
Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant,"We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so,2) lies to auditors when asked questions, and3) strategically pretends to be less capable than it is during capability evaluations.Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.",25,4,2024
Software Mention Recognition with a Three-Stage Framework Based on BERTology Models at SOMD 2024,"This paper describes our systems for the sub-task I in the Software Mention Detection in Scholarly Publications shared-task. We propose three approaches leveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to tackle this challenge. Our bestperforming system addresses the named entity recognition (NER) problem through a three-stage framework. (1) Entity Sentence Classification - classifies sentences containing potential software mentions; (2) Entity Extraction - detects mentions within classified sentences; (3) Entity Type Classification - categorizes detected mentions into specific software types. Experiments on the official dataset demonstrate that our three-stage framework achieves competitive performance, surpassing both other participating teams and our alternative approaches. As a result, our framework based on the XLM-R-based model achieves a weighted F1-score of 67.80%, delivering our team the 3rd rank in Sub-task I for the Software Mention Recognition task.",23,4,2024
On Using Agent-based Modeling and Simulation for Studying Blockchain Systems,"There is a need for a simulation framework, which is develop as a software using modern engineering approaches (e.g., modularity --i.e., model reuse--, testing, continuous development and continuous integration, automated management of builds, dependencies and documentation) and agile principles, (1) to make rapid prototyping of industrial cases and (2) to carry out their feasibility analysis in a realistic manner (i.e., to test hypothesis by simulating complex experiments involving large numbers of participants of different types acting in one or several blockchain systems).",23,4,2024
"Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository","LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level in various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Existing research often treats class-level generation as an isolated task, neglecting the intricate dependencies and interactions that characterize real-world software development environments. To address this gap, we introduce RepoClassBench, a benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes natural language to class generation tasks across Java and Python, from a selection of public repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages and in various settings. Our findings emphasize the need for benchmarks that incorporate repository-level dependencies to more accurately reflect the complexities of software development. Our work illustrates the benefits of leveraging specialized tools to enhance LLMs understanding of repository context. We plan to make our dataset and evaluation harness public.",22,4,2024
A Semi-Formal Verification Methodology for Efficient Configuration Coverage of Highly Configurable Digital Designs,"Nowadays, a majority of System-on-Chips (SoCs) make use of Intellectual Property (IP) in order to shorten development cycles. When such IPs are developed, one of the main focuses lies in the high configurability of the design. This flexibility on the design side introduces the challenge of covering a huge state space of IP configurations on the verification side to ensure the functional correctness under every possible parameter setting. The vast number of possibilities does not allow a brute-force approach, and therefore, only a selected number of settings based on typical and extreme assumptions are usually verified. Especially in automotive applications, which need to follow the ISO 26262 functional safety standard, the requirement of covering all significant variants needs to be fulfilled in any case. State-of-the-Art existing verification techniques such as simulation-based verification and formal verification have challenges such as time-space explosion and state-space explosion respectively and therefore, lack behind in verifying highly configurable digital designs efficiently. This paper is focused on a semi-formal verification methodology for efficient configuration coverage of highly configurable digital designs. The methodology focuses on reduced runtime based on simulative and formal methods that allow high configuration coverage. The paper also presents the results when the developed methodology was applied on a highly configurable microprocessor IP and discusses the gained benefits.",20,4,2024
Effect of multiple bands on high-harmonic generation in zinc oxide subjected to intense mid-infrared femtosecond laser pulse,"We theoretically investigate photo-ionization and high harmonic generation in the bulk of zinc oxide subjected to intense femto-second laser pulses with mid-infrared wavelength (3 $\mu$m). When the electric field strength inside vacuum is below $0.6$ V/Å, an intra-band mechanism associated to non-adiabatic response of multiple valence and conduction bands is found to be responsible for high-harmonic generation in the bulk solid. Good semi-quantitative agreement with the experimental data is found: clean and well defined odd-order harmonic peaks extending beyond the band edge of ZnO are exhibited for laser linearly polarized perpendicular to the optical axis of the crystal.",17,4,2024
Superconductivity of Bulk Abnormal Magic-stoichiometric Na3Cl Salt Crystals at Normal Pressure,"The identification of new materials with superconducting properties is the pursuit in the realm of superconductivity research. Here, excitedly, we show that the simplest salt daily used can be made a superconductor at normal pressure only by adjusting its stoichiometry of Na and Cl as Na3Cl at normal pressure based on first-principles calculations. This bulk stable abnormal Na-Cl stoichiometric crystal of 3:1, the first 'magic' ratio, includes metallic (Na) atoms in the core as well as hybridization of ionic and metallic bonding, facilitating the electron-phonon-coupling for superconductivity with a critical temperature Tc of 0.13 K. The flat bands and van Hove singularities near the Fermi level produce large densities of states, similar to H3S and LaH10, which is beneficial for the emergence of superconductivity. The crystal composed of with abnormal Na-Cl magic stoichiometry is a precisely tunable, purely sodium and chloride-based, three-dimensional bulk superconductor, which is therefore an ideal material for designing and understanding abnormal stoichiometric crystals. The methodology of constructing this bulk abnormal crystal may be general to almost all elements, which could lead to insights into the physics of other conventional superconductors and even high-critical-temperature superconductors.",17,4,2024
A Systematic Literature Review on Reasons and Approaches for Accurate Effort Estimations in Agile,"Background: Accurate effort estimation is crucial for planning in Agile iterative development. Agile estimation generally relies on consensus-based methods like planning poker, which require less time and information than other formal methods (e.g., COSMIC) but are prone to inaccuracies. Understanding the common reasons for inaccurate estimations and how proposed approaches can assist practitioners is essential. However, prior systematic literature reviews (SLR) only focus on the estimation practices (e.g., [26, 127]) and the effort estimation approaches (e.g., [6]). Aim: We aim to identify themes of reasons for inaccurate estimations and classify approaches to improve effort estimation. Method: We conducted an SLR and identified the key themes and a taxonomy. Results: The reasons for inaccurate estimation are related to information quality, team, estimation practice, project management, and business influences. The effort estimation approaches were the most investigated in the literature, while only a few aim to support the effort estimation process. Yet, few automated approaches are at risk of data leakage and indirect validation scenarios. Recommendations: Practitioners should enhance the quality of information for effort estimation, potentially by adopting an automated approach. Future research should aim to improve the information quality, while avoiding data leakage and indirect validation scenarios.",15,4,2024
Convert any android device into a programmable IoT device with the help of IoT Everywhere Framework,"The world around us is transforming as the field of the Internet of Things is taking over the world faster than we thought. Everyone in the tech industry is building wonderful things with the help of IoT. Smartwatches, smart coffee machines, smart television, smart homes are some of the examples. Building IoT sensor modules with sensors that connect to the internet can be very intimidating for people who have just stepped into the field. Quality components and microcontrollers can be costly too. Components such as proximity sensor, humidity sensor, air pressure sensor, accelerometer, gyroscope, flashlight, microphone, speaker, gsm module, wifi module, Bluetooth modules, and many more. But to program these we need to know java or kotlin and mobile application development. With the use of the IoT Everywhere framework and Origin programming language, one can convert any Android smartphone into an IoT device. This helps students of electrical engineering to grasp the idea of programming since it provides a lot of abstraction through simple function calls it can help to introduce programming to school students, it helps students who are fascinated by IoT and who wants to learn the basic of interfacing components or sensors and helps the student who has no access to an actual personal computer learn to program.",14,4,2024
CodeFort: Robust Training for Code Generation Models,"Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%",11,4,2024
2HCDL: Holistic Human-Centered Development Lifecycle,"The recent events affecting global society continuously highlight the need to change the development lifecycle of complex systems by promoting human-centered solutions that increase awareness and ensure critical properties such as security, safety, trust, transparency, and privacy. This fast abstract introduces the Holistic Human-Centered Development Lifecycle (2HCDL) methodology focused on: (i) the enforcement of human values and properties and (ii) the mitigation and prevention of critical issues for more secure, safe, trustworthy, transparent, and private development processes.",8,4,2024
The Role of Code Proficiency in the Era of Generative AI,"At the current pace of technological advancements, Generative AI models, including both Large Language Models and Large Multi-modal Models, are becoming integral to the developer workspace. However, challenges emerge due to the 'black box' nature of many of these models, where the processes behind their outputs are not transparent. This position paper advocates for a 'white box' approach to these generative models, emphasizing the necessity of transparency and understanding in AI-generated code to match the proficiency levels of human developers and better enable software maintenance and evolution. We outline a research agenda aimed at investigating the alignment between AI-generated code and developer skills, highlighting the importance of responsibility, security, legal compliance, creativity, and social value in software development. The proposed research questions explore the potential of white-box methodologies to ensure that software remains an inspectable, adaptable, and trustworthy asset in the face of rapid AI integration, setting a course for research that could shape the role of code proficiency into 2030 and beyond.",8,4,2024
Prioritizing Software Requirements Using Large Language Models,"Large Language Models (LLMs) are revolutionizing Software Engineering (SE) by introducing innovative methods for tasks such as collecting requirements, designing software, generating code, and creating test cases, among others. This article focuses on requirements engineering, typically seen as the initial phase of software development that involves multiple system stakeholders. Despite its key role, the challenge of identifying requirements and satisfying all stakeholders within time and budget constraints remains significant. To address the challenges in requirements engineering, this study introduces a web-based software tool utilizing AI agents and prompt engineering to automate task prioritization and apply diverse prioritization techniques, aimed at enhancing project management within the agile framework. This approach seeks to transform the prioritization of agile requirements, tackling the substantial challenge of meeting stakeholder needs within set time and budget limits. Furthermore, the source code of our developed prototype is available on GitHub, allowing for further experimentation and prioritization of requirements, facilitating research and practical application.",5,4,2024
Mitigating LLM Hallucinations via Conformal Abstention,"We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying ""I don't know"") in a general domain, instead of resorting to possibly ""hallucinating"" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.",4,4,2024
Discrete Event Simulation: It's Easy with SimPy!,"This paper introduces the practicalities and benefits of using SimPy, a discrete event simulation (DES) module written in Python, for modeling and simulating complex systems. Through a step-by-step exploration of the classical Dining Philosophers Problem, we demonstrate how SimPy enables the efficient construction of discrete event models, emphasizing system states, transitions, and event handling. We extend the scenario to introduce resources, such as chopsticks, to model contention and deadlock conditions, and showcase SimPy's capabilities in managing these scenarios. Furthermore, we explore the integration of SimPy with other Python libraries for statistical analysis, showcasing how simulation results inform system design and optimization. The versatility of SimPy is further highlighted through additional modeling scenarios, including resource constraints and customer service interactions, providing insights into the process of building, debugging, simulating, and optimizing models for a wide range of applications. This paper aims to make DES accessible to practitioners and researchers alike, emphasizing the ease with which complex simulations can be constructed, analyzed, and visualized using SimPy and the broader Python ecosystem.",3,4,2024
Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models,"In the rapidly evolving domain of artificial intelligence, safeguarding the intellectual property of Large Language Models (LLMs) is increasingly crucial. Current watermarking techniques against model extraction attacks, which rely on signal insertion in model logits or post-processing of generated text, remain largely heuristic. We propose a novel method for embedding learnable linguistic watermarks in LLMs, aimed at tracing and preventing model extraction attacks. Our approach subtly modifies the LLM's output distribution by introducing controlled noise into token frequency distributions, embedding an statistically identifiable controllable watermark.We leverage statistical hypothesis testing and information theory, particularly focusing on Kullback-Leibler Divergence, to differentiate between original and modified distributions effectively. Our watermarking method strikes a delicate well balance between robustness and output quality, maintaining low false positive/negative rates and preserving the LLM's original performance.",28,4,2024
A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving,"Reinforcement learning has emerged as an important approach for autonomous driving. A reward function is used in reinforcement learning to establish the learned skill objectives and guide the agent toward the optimal policy. Since autonomous driving is a complex domain with partly conflicting objectives with varying degrees of priority, developing a suitable reward function represents a fundamental challenge. This paper aims to highlight the gap in such function design by assessing different proposed formulations in the literature and dividing individual objectives into Safety, Comfort, Progress, and Traffic Rules compliance categories. Additionally, the limitations of the reviewed reward functions are discussed, such as objectives aggregation and indifference to driving context. Furthermore, the reward categories are frequently inadequately formulated and lack standardization. This paper concludes by proposing future research that potentially addresses the observed shortcomings in rewards, including a reward validation framework and structured rewards that are context-aware and able to resolve conflicts.",12,4,2024
LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration,"As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent's level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.",13,4,2024
Diagnosis of Parkinson's Disease Using EEG Signals and Machine Learning Techniques: A Comprehensive Study,"Parkinson's disease is a widespread neurodegenerative condition necessitating early diagnosis for effective intervention. This paper introduces an innovative method for diagnosing Parkinson's disease through the analysis of human EEG signals, employing a Support Vector Machine (SVM) classification model. this research presents novel contributions to enhance diagnostic accuracy and reliability. Our approach incorporates a comprehensive review of EEG signal analysis techniques and machine learning methods. Drawing from recent studies, we have engineered an advanced SVM-based model optimized for Parkinson's disease diagnosis. Utilizing cutting-edge feature engineering, extensive hyperparameter tuning, and kernel selection, our method achieves not only heightened diagnostic accuracy but also emphasizes model interpretability, catering to both clinicians and researchers. Moreover, ethical concerns in healthcare machine learning, such as data privacy and biases, are conscientiously addressed. We assess our method's performance through experiments on a diverse dataset comprising EEG recordings from Parkinson's disease patients and healthy controls, demonstrating significantly improved diagnostic accuracy compared to conventional techniques. In conclusion, this paper introduces an innovative SVM-based approach for diagnosing Parkinson's disease from human EEG signals. Building upon the IEEE framework and previous research, its novelty lies in the capacity to enhance diagnostic accuracy while upholding interpretability and ethical considerations for practical healthcare applications. These advances promise to revolutionize early Parkinson's disease detection and management, ultimately contributing to enhanced patient outcomes and quality of life.",30,4,2024
Why does Knowledge Distillation Work? Rethink its Attention and Fidelity Mechanism,"Does Knowledge Distillation (KD) really work? Conventional wisdom viewed it as a knowledge transfer procedure where a perfect mimicry of the student to its teacher is desired. However, paradoxical studies indicate that closely replicating the teacher's behavior does not consistently improve student generalization, posing questions on its possible causes. Confronted with this gap, we hypothesize that diverse attentions in teachers contribute to better student generalization at the expense of reduced fidelity in ensemble KD setups. By increasing data augmentation strengths, our key findings reveal a decrease in the Intersection over Union (IoU) of attentions between teacher models, leading to reduced student overfitting and decreased fidelity. We propose this low-fidelity phenomenon as an underlying characteristic rather than a pathology when training KD. This suggests that stronger data augmentation fosters a broader perspective provided by the divergent teacher ensemble and lower student-teacher mutual information, benefiting generalization performance. These insights clarify the mechanism on low-fidelity phenomenon in KD. Thus, we offer new perspectives on optimizing student model performance, by emphasizing increased diversity in teacher attentions and reduced mimicry behavior between teachers and student.",30,4,2024
HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis,"Graphics Processing Units (GPUs) have become the leading hardware accelerator for deep learning applications and are used widely in training and inference of transformers; transformers have achieved state-of-the-art performance in many areas of machine learning and are especially used in most modern Large Language Models (LLMs). However, GPUs require large amounts of energy, which poses environmental concerns, demands high operational costs, and causes GPUs to be unsuitable for edge computing. We develop an accelerator for transformers, namely, Llama 2, an open-source state-of-the-art LLM, using high level synthesis (HLS) on Field Programmable Gate Arrays (FPGAs). HLS allows us to rapidly prototype FPGA designs without writing code at the register-transfer level (RTL). We name our method HLSTransform, and the FPGA designs we synthesize with HLS achieve up to a 12.75x reduction and 8.25x reduction in energy used per token on the Xilinx Virtex UltraScale+ VU9P FPGA compared to an Intel Xeon Broadwell E5-2686 v4 CPU and NVIDIA RTX 3090 GPU respectively, while increasing inference speeds by up to 2.46x compared to CPU and maintaining 0.53x the speed of an RTX 3090 GPU despite the GPU's 4 times higher base clock rate. With the lack of existing open-source FPGA accelerators for transformers, we open-source our code and document our steps for synthesis. We hope this work will serve as a step in democratizing the use of FPGAs in transformer inference and inspire research into energy-efficient inference methods as a whole. The code can be found onthis https URL.",29,4,2024
Note about the existence and essential uniqueness of quadrature domains,"This note is intended to explain the proof of two facts about quadrature domains: first, they are essentially unique if they exist; and second, they do exist for a large class of weight functions. The proofs roughly follow Sakai's ""Solutions to the obstacle problem as Green potentials,"" but are presented at an easier level.",29,4,2024
Joint Signal Detection and Automatic Modulation Classification via Deep Learning,"Signal detection and modulation classification are two crucial tasks in various wireless communication systems. Different from prior works that investigate them independently, this paper studies the joint signal detection and automatic modulation classification (AMC) by considering a realistic and complex scenario, in which multiple signals with different modulation schemes coexist at different carrier frequencies. We first generate a coexisting RADIOML dataset (CRML23) to facilitate the joint design. Different from the publicly available AMC dataset ignoring the signal detection step and containing only one signal, our synthetic dataset covers the more realistic multiple-signal coexisting scenario. Then, we present a joint framework for detection and classification (JDM) for such a multiple-signal coexisting environment, which consists of two modules for signal detection and AMC, respectively. In particular, these two modules are interconnected using a designated data structure called ""proposal"". Finally, we conduct extensive simulations over the newly developed dataset, which demonstrate the effectiveness of our designs. Our code and dataset are now available as open-source (this https URL).",29,4,2024
Exterior stability of Minkowski spacetime with borderline decay,"In 1993, the global stability of Minkowski spacetime has been proven in the celebrated work of Christodoulou and Klainerman \cite{Ch-Kl}. In 2003, Klainerman and Nicolò \cite{Kl-Ni} revisited Minkowski stability in the exterior of an outgoing null cone. In \cite{Shen23}, the author extended the results of \cite{Ch-Kl} to minimal decay assumptions. In this paper, we prove that the exterior stability of Minkowski holds with decay which is borderline compared to the minimal decay considered in \cite{Shen23}.",29,4,2024
EEG-MACS: Manifold Attention and Confidence Stratification for EEG-based Cross-Center Brain Disease Diagnosis under Unreliable Annotations,"Cross-center data heterogeneity and annotation unreliability significantly challenge the intelligent diagnosis of diseases using brain signals. A notable example is the EEG-based diagnosis of neurodegenerative diseases, which features subtler abnormal neural dynamics typically observed in small-group settings. To advance this area, in this work, we introduce a transferable framework employing Manifold Attention and Confidence Stratification (MACS) to diagnose neurodegenerative disorders based on EEG signals sourced from four centers with unreliable annotations. The MACS framework's effectiveness stems from these features: 1) The Augmentor generates various EEG-represented brain variants to enrich the data space; 2) The Switcher enhances the feature space for trusted samples and reduces overfitting on incorrectly labeled samples; 3) The Encoder uses the Riemannian manifold and Euclidean metrics to capture spatiotemporal variations and dynamic synchronization in EEG; 4) The Projector, equipped with dual heads, monitors consistency across multiple brain variants and ensures diagnostic accuracy; 5) The Stratifier adaptively stratifies learned samples by confidence levels throughout the training process; 6) Forward and backpropagation in MACS are constrained by confidence stratification to stabilize the learning system amid unreliable annotations. Our subject-independent experiments, conducted on both neurocognitive and movement disorders using cross-center corpora, have demonstrated superior performance compared to existing related algorithms. This work not only improves EEG-based diagnostics for cross-center and small-setting brain diseases but also offers insights into extending MACS techniques to other data analyses, tackling data heterogeneity and annotation unreliability in multimedia and multimodal content understanding.",29,4,2024
Joint ADS-B in 5G for Hierarchical Aerial Networks: Performance Analysis and Optimization,"Unmanned aerial vehicles (UAVs) are widely applied in multiple fields, which emphasizes the challenge of obtaining UAV flight information to ensure the airspace safety. UAVs equipped with automatic dependent surveillance-broadcast (ADS-B) devices are capable of sending flight information to nearby aircrafts and ground stations (GSs). However, the saturation of limited frequency bands of ADS-B leads to interferences among UAVs and impairs the monitoring performance of GS to civil planes. To address this issue, the integration of the 5th generation mobile communication technology (5G) with ADS-B is proposed for UAV operations in this paper. Specifically, a hierarchical structure is proposed, in which the high-altitude central UAV is equipped with ADS-B and the low-altitude central UAV utilizes 5G modules to transmit flight information. Meanwhile, based on the mobile edge computing technique, the flight information of sub-UAVs is offloaded to the central UAV for further processing, and then transmitted to GS. We present the deterministic model and stochastic geometry based model to build the air-to-ground channel and air-to-air channel, respectively. The effectiveness of the proposed monitoring system is verified via simulations and experiments. This research contributes to improving the airspace safety and advancing the air traffic flow management.",29,4,2024
"LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report","Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.",29,4,2024
$L^p-L^q$ estimates for non-local heat and wave type equations on locally compact groups,"We prove the $L^p-L^q$ $(1<p\leqslant 2\leqslant q<+\infty)$ norm estimates for the solutions of heat and wave type equations on a locally compact separable unimodular group $G$ by using an integro-differential operator in time and any positive left invariant operator (maybe unbounded) on $G$. We complement our studies by giving asymptotic time estimates for the solutions, which in some cases are sharp.",28,4,2024
Two-step minimization approach to an $L^\infty$-constrained variational problem with a generalized potential,"We study a variational problem on $H^1({\mathbb R})$ under an $L^\infty$-constraint related to Sobolev-type inequalities for a class of generalized potentials, including $L^p$-potentials, non-positive potentials, and signed Radon measures. We establish various essential tools for this variational problem, including the decomposition principle, the comparison principle, and the perturbation theorem, those are the basis of the two-step minimization method. As for their applications, we present precise results for minimizers of minimization problems, such as the study of potentials of Dirac's delta measure type and the analysis of trapped modes in potential wells.",28,4,2024
Characteristic tilting modules and Ringel duality in the Noetherian world,"The foundations of Ringel duality for split quasi-hereditary algebras over commutative Noetherian rings are strengthened. Several descriptions and properties of the smallest resolving subcategory containing all standard modules over split quasi-hereditary algebras over commutative Noetherian rings are provided.In particular, given two split quasi-hereditary algebras $A$ and $B$, we prove that any exact equivalence between the smallest resolving subcategory containing all standard modules over $A$ and the smallest resolving subcategory containing all standard modules over $B$ lifts to a Morita equivalence between $A$ and $B$ which preserves the quasi-hereditary structure.",27,4,2024
Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A Comparative Study,"The integration of Artificial Intelligence (AI) in healthcare presents a transformative potential for enhancing operational efficiency and health outcomes. Large Language Models (LLMs), such as ChatGPT, have shown their capabilities in supporting medical decision-making. Embedding LLMs in medical systems is becoming a promising trend in healthcare development. The potential of ChatGPT to address the triage problem in emergency departments has been examined, while few studies have explored its application in outpatient departments. With a focus on streamlining workflows and enhancing efficiency for outpatient triage, this study specifically aims to evaluate the consistency of responses provided by ChatGPT in outpatient guidance, including both within-version response analysis and between-version comparisons. For within-version, the results indicate that the internal response consistency for ChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a moderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top recommendation. However, the between-version consistency is relatively low (mean consistency score=1.43/3, median=1), indicating few recommendations match between the two versions. Also, only 50% top recommendations match perfectly in the comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be complete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences in information processing and response generation between the two versions. The findings offer insights into AI-assisted outpatient operations, while also facilitating the exploration of potentials and limitations of LLMs in healthcare utilization. Future research may focus on carefully optimizing LLMs and AI integration in healthcare systems based on ergonomic and human factors principles, precisely aligning with the specific needs of effective outpatient triage.",27,4,2024
Unveiling Thoughts: A Review of Advancements in EEG Brain Signal Decoding into Text,"The conversion of brain activity into text using electroencephalography (EEG) has gained significant traction in recent years. Many researchers are working to develop new models to decode EEG signals into text form. Although this area has shown promising developments, it still faces numerous challenges that necessitate further improvement. It's important to outline this area's recent developments and future research directions. In this review article, we thoroughly summarize the progress in EEG-to-text conversion. Firstly, we talk about how EEG-to-text technology has grown and what problems we still face. Secondly, we discuss existing techniques used in this field. This includes methods for collecting EEG data, the steps to process these signals, and the development of systems capable of translating these signals into coherent text. We conclude with potential future research directions, emphasizing the need for enhanced accuracy, reduced system constraints, and the exploration of novel applications across varied sectors. By addressing these aspects, this review aims to contribute to developing more accessible and effective Brain-Computer Interface (BCI) technology for a broader user base.",26,4,2024
Federated Learning and Differential Privacy Techniques on Multi-hospital Population-scale Electrocardiogram Data,"This research paper explores ways to apply Federated Learning (FL) and Differential Privacy (DP) techniques to population-scale Electrocardiogram (ECG) data. The study learns a multi-label ECG classification model using FL and DP based on 1,565,849 ECG tracings from 7 hospitals in Alberta, Canada. The FL approach allowed collaborative model training without sharing raw data between hospitals while building robust ECG classification models for diagnosing various cardiac conditions. These accurate ECG classification models can facilitate the diagnoses while preserving patient confidentiality using FL and DP techniques. Our results show that the performance achieved using our implementation of the FL approach is comparable to that of the pooled approach, where the model is trained over the aggregating data from all hospitals. Furthermore, our findings suggest that hospitals with limited ECGs for training can benefit from adopting the FL model compared to single-site training. In addition, this study showcases the trade-off between model performance and data privacy by employing DP during model training. Our code is available atthis https URL.",26,4,2024
Baseline Drift Tolerant Signal Encoding for ECG Classification with Deep Learning,"Common artefacts such as baseline drift, rescaling, and noise critically limit the performance of machine learningbased automated ECG analysis and interpretation. This study proposes Derived Peak (DP) encoding, a non-parametric method that generates signed spikes corresponding to zero crossings of the signals first and second-order time derivatives. Notably, DP encoding is invariant to shift and scaling artefacts, and its implementation is further simplified by the absence of userdefined parameters. DP encoding was used to encode the 12-lead ECG data from the PTB-XL dataset (n=18,869 participants) and was fed to 1D-ResNet-18 models trained to identify myocardial infarction, conductive deficits and ST-segment abnormalities. Robustness to artefacts was assessed by corrupting ECG data with sinusoidal baseline drift, shift, rescaling and noise, before encoding. The addition of these artefacts resulted in a significant drop in accuracy for seven other methods from prior art, while DP encoding maintained a baseline AUC of 0.88 under drift, shift and rescaling. DP achieved superior performance to unencoded inputs in the presence of shift (AUC under 1mV shift: 0.91 vs 0.62), and rescaling artefacts (AUC 0.91 vs 0.79). Thus, DP encoding is a simple method by which robustness to common ECG artefacts may be improved for automated ECG analysis and interpretation.",26,4,2024
EEG_RL-Net: Enhancing EEG MI Classification through Reinforcement Learning-Optimised Graph Neural Networks,"Brain-Computer Interfaces (BCIs) rely on accurately decoding electroencephalography (EEG) motor imagery (MI) signals for effective device control. Graph Neural Networks (GNNs) outperform Convolutional Neural Networks (CNNs) in this regard, by leveraging the spatial relationships between EEG electrodes through adjacency matrices. The EEG_GLT-Net framework, featuring the state-of-the-art EEG_GLT adjacency matrix method, has notably enhanced EEG MI signal classification, evidenced by an average accuracy of 83.95% across 20 subjects on the PhysioNet dataset. This significantly exceeds the 76.10% accuracy rate achieved using the Pearson Correlation Coefficient (PCC) method within the same framework.In this research, we advance the field by applying a Reinforcement Learning (RL) approach to the classification of EEG MI signals. Our innovative method empowers the RL agent, enabling not only the classification of EEG MI data points with higher accuracy, but effective identification of EEG MI data points that are less distinct. We present the EEG_RL-Net, an enhancement of the EEG_GLT-Net framework, which incorporates the trained EEG GCN Block from EEG_GLT-Net at an adjacency matrix density of 13.39% alongside the RL-centric Dueling Deep Q Network (Dueling DQN) block. The EEG_RL-Net model showcases exceptional classification performance, achieving an unprecedented average accuracy of 96.40% across 20 subjects within 25 milliseconds. This model illustrates the transformative effect of the RL in EEG MI time point classification.",26,4,2024
LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study,"As NLP models become more complex, understanding their decisions becomes more crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's prediction, offer a way to explain these models. While Large Language Models (LLMs) have shown remarkable performance in NLP tasks, their efficacy in generating high-quality CFs remains uncertain. This work fills this gap by investigating how well LLMs generate CFs for two NLU tasks. We conduct a comprehensive comparison of several common LLMs, and evaluate their CFs, assessing both intrinsic metrics, and the impact of these CFs on data augmentation. Moreover, we analyze differences between human and LLM-generated CFs, providing insights for future research directions. Our results show that LLMs generate fluent CFs, but struggle to keep the induced changes minimal. Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where LLMs show weaknesses in generating CFs that flip the original label. This also reflects on the data augmentation performance, where we observe a large gap between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs' ability to assess CFs in a mislabelled data setting, and show that they have a strong bias towards agreeing with the provided labels. GPT4 is more robust against this bias and its scores correlate well with automatic metrics. Our findings reveal several limitations and point to potential future work directions.",26,4,2024
Optimizing Brain-Computer Interface Performance: Advancing EEG Signals Channel Selection through Regularized CSP and SPEA II Multi-Objective Optimization,"Brain-computer interface systems and the recording of brain activity has garnered significant attention across a diverse spectrum of applications. EEG signals have emerged as a modality for recording neural electrical activity. Among the methodologies designed for feature extraction from EEG data, the method of RCSP has proven to be an approach, particularly in the context of MI tasks. RCSP exhibits efficacy in the discrimination and classification of EEG signals. In optimizing the performance of this method, our research extends to a comparative analysis with conventional CSP techniques, as well as optimized methodologies designed for similar applications. Notably, we employ the meta-heuristic multi-objective Strength Pareto Evolutionary Algorithm II (SPEA-II) as a pivotal component of our research paradigm. This is a state-of-the-art approach in the selection of an subset of channels from a multichannel EEG signal with MI tasks. Our main objective is to formulate an optimum channel selection strategy aimed at identifying the most pertinent subset of channels from the multi-dimensional electroencephalogram (EEG) signals. One of the primary objectives inherent to channel selection in the EEG signal analysis pertains to the reduction of the channel count, an approach that enhances user comfort when utilizing gel-based EEG electrodes. Additionally, within this research, we took benefit of ensemble learning models as a component of our decision-making. This technique serves to mitigate the challenges associated with overfitting, especially when confronted with an extensive array of potentially redundant EEG channels and data noise. Our findings not only affirm the performance of RCSP in MI-based BCI systems, but also underscore the significance of channel selection strategies and ensemble learning techniques in optimizing the performance of EEG signal classification.",26,4,2024
A Novel Machine Learning-based Equalizer for a Downstream 100G PAM-4 PON,"A frequency-calibrated SCINet (FC-SCINet) equalizer is proposed for down-stream 100G PON with 28.7 dB path loss. At 5 km, FC-SCINet improves the BER by 88.87% compared to FFE and a 3-layer DNN with 10.57% lower complexity.",25,4,2024
EEG-Deformer: A Dense Convolutional Transformer for Brain-computer Interfaces,"Effectively learning the temporal dynamics in electroencephalogram (EEG) signals is challenging yet essential for decoding brain activities using brain-computer interfaces (BCIs). Although Transformers are popular for their long-term sequential learning ability in the BCI field, most methods combining Transformers with convolutional neural networks (CNNs) fail to capture the coarse-to-fine temporal dynamics of EEG signals. To overcome this limitation, we introduce EEG-Deformer, which incorporates two main novel components into a CNN-Transformer: (1) a Hierarchical Coarse-to-Fine Transformer (HCT) block that integrates a Fine-grained Temporal Learning (FTL) branch into Transformers, effectively discerning coarse-to-fine temporal patterns; and (2) a Dense Information Purification (DIP) module, which utilizes multi-level, purified temporal information to enhance decoding accuracy. Comprehensive experiments on three representative cognitive tasks consistently verify the generalizability of our proposed EEG-Deformer, demonstrating that it either outperforms existing state-of-the-art methods or is comparable to them. Visualization results show that EEG-Deformer learns from neurophysiologically meaningful brain regions for the corresponding cognitive tasks. The source code can be found atthis https URL.",25,4,2024
Can't say cant? Measuring and Reasoning of Dark Jargons in Large Language Models,"Ensuring the resilience of Large Language Models (LLMs) against malicious exploitation is paramount, with recent focus on mitigating offensive responses. Yet, the understanding of cant or dark jargon remains unexplored. This paper introduces a domain-specific Cant dataset and CantCounter evaluation framework, employing Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages. Experiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing filters, with varying recognition accuracy influenced by question types, setups, and prompt clues. Updated models exhibit higher acceptance rates for cant queries. Moreover, LLM reactions differ across domains, e.g., reluctance to engage in racism versus LGBT topics. These findings underscore LLMs' understanding of cant and reflect training data characteristics and vendor approaches to sensitive topics. Additionally, we assess LLMs' ability to demonstrate reasoning capabilities. Access to our datasets and code is available atthis https URL.",25,4,2024
Exploring News Summarization and Enrichment in a Highly Resource-Scarce Indian Language: A Case Study of Mizo,"Obtaining sufficient information in one's mother tongue is crucial for satisfying the information needs of the users. While high-resource languages have abundant online resources, the situation is less than ideal for very low-resource languages. Moreover, the insufficient reporting of vital national and international events continues to be a worry, especially in languages with scarce resources, like \textbf{Mizo}. In this paper, we conduct a study to investigate the effectiveness of a simple methodology designed to generate a holistic summary for Mizo news articles, which leverages English-language news to supplement and enhance the information related to the corresponding news events. Furthermore, we make available 500 Mizo news articles and corresponding enriched holistic summaries. Human evaluation confirms that our approach significantly enhances the information coverage of Mizo news articles. The mizo dataset and code can be accessed at \url{this https URL",25,4,2024
Large Language Models in Healthcare: A Comprehensive Benchmark,"The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering task with answer options for evaluation. However, in real clinical settings, many clinical decisions, such as treatment recommendations, involve answering open-ended questions without pre-set options. Meanwhile, existing studies mainly use accuracy to assess model performance. In this paper, we comprehensively benchmark diverse LLMs in healthcare, to clearly understand their strengths and weaknesses. Our benchmark contains seven tasks and thirteen datasets across medical language generation, understanding, and reasoning. We conduct a detailed evaluation of the existing sixteen LLMs in healthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning settings. We report the results on five metrics (i.e. matching, faithfulness, comprehensiveness, generalizability, and robustness) that are critical in achieving trust from clinical users. We further invite medical experts to conduct human evaluation.",25,4,2024
Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation,"Large Language Models (LLMs) have shown promising capabilities in handling clinical text summarization tasks. In this study, we demonstrate that a small open-source LLM can be effectively trained to generate high-quality clinical notes from outpatient patient-doctor dialogues. We achieve this through a comprehensive domain- and task-specific adaptation process for the LLaMA-2 13 billion parameter model. This process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced an enhanced approach, termed DistillDirect, for performing on-policy reinforcement learning with Gemini Pro serving as the teacher model. Our resulting model, LLaMA-Clinic, is capable of generating clinical notes that are comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as ""acceptable"" or higher across all three criteria: real-world readiness, completeness, and accuracy. Notably, in the more challenging ""Assessment and Plan"" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness compared to physician-authored notes (4.1/5). Additionally, we identified caveats in public clinical note datasets, such as ACI-BENCH. We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format. Overall, our research demonstrates the potential and feasibility of training smaller, open-source LLMs to assist with clinical documentation, capitalizing on healthcare institutions' access to patient records and domain expertise. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research in this field.",25,4,2024
Multi-Band mm-Wave Measurement Platform Towards Environment-Aware Beam Management,"Agile beam management is key for providing seamless millimeter wave (mm-wave) connectivity given the site-specific spatio-temporal variations of the mm-wave channel. Leveraging non radio frequency (RF) sensor inputs for environment awareness, e.g. via machine learning (ML) techniques, can greatly enhance RF-based beam steering. To overcome the lack of diverse publicly available multi-modal mm-wave datasets for the design and evaluation of such novel beam steering approaches, we demonstrate our software-defined radio multi-band mm-wave measurement platform which integrates multi-modal sensors towards environment-aware beam management.",25,4,2024
Some inequalities related to Riesz transform on exterior Lipschitz domains,"Let $n\ge2$ and $\mathcal{L}=-\mathrm{div}(A\nabla\cdot)$ be an elliptic operator on $\mathbb{R}^n$. Given an exterior Lipschitz domain $\Omega$, let $\mathcal{L}_D$ and $\mathcal{L}_N$ be the elliptic operators $\mathcal{L}$ on $\Omega$ subject to the Dirichlet and the Neumann boundary {conditions}, respectively. For the Neumann operator, we show that the reverse inequality $\|\mathcal{L}_N^{1/2}f\|_{L^p(\Omega)} \le C\|\nabla f\|_{L^p(\Omega)}$ holds true for any $p\in(1,\infty)$. For the Dirichlet operator, it was known that the Riesz operator $\nabla \mathcal{L}_D^{-1/2}$ is not bounded for $p>2$ and $p\ge n$, even if $\mathcal{L}=-\Delta$ being the Laplace operator. Suppose that $A$ are CMO coefficients or VMO coefficients satisfying certain perturbation property, and $\partial\Omega$ is $C^1$, we prove that for $p>2$ and $p\in [n,\infty)$, it holds $$ \inf_{\phi\in\mathcal{A}^p_0(\Omega)}\left\|\nabla f-\nabla\phi\right\|_{L^p(\Omega)}\le C\left\|\mathcal{L}^{1/2}_D f\right\|_{L^p(\Omega)} $$ for $f\in \dot{W}^{1,p}_0(\Omega)$. Here $\mathcal{A}^p_0(\Omega)=\{f\in \dot{W}^{1,p}_0(\Omega):\,\mathcal{L}_Df=0\}$ is a non-trivial subspace generated by harmonic function in $\Omega$ with zero boundary value.",25,4,2024
Homonym Sense Disambiguation in the Georgian Language,"This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Georgian language, based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on a dataset formed by filtering the Georgian Common Crawls corpus. The dataset is used to train a classifier for words with multiple senses. Additionally, we present experimental results of using LSTM for WSD. Accurately disambiguating homonyms is crucial in natural language processing. Georgian, an agglutinative language belonging to the Kartvelian language family, presents unique challenges in this context. The aim of this paper is to highlight the specific problems concerning homonym disambiguation in the Georgian language and to present our approach to solving them. The techniques discussed in the article achieve 95% accuracy for predicting lexical meanings of homonyms using a hand-classified dataset of over 7500 sentences.",24,4,2024
Evaluating Tool-Augmented Agents in Remote Sensing Platforms,"Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask ""Detect all objects here"". Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",23,4,2024
Interactive Analysis of LLMs using Meaningful Counterfactuals,"Counterfactual examples are useful for exploring the decision boundaries of machine learning models and determining feature attributions. How can we apply counterfactual-based methods to analyze and explain LLMs? We identify the following key challenges. First, the generated textual counterfactuals should be meaningful and readable to users and thus can be mentally compared to draw conclusions. Second, to make the solution scalable to long-form text, users should be equipped with tools to create batches of counterfactuals from perturbations at various granularity levels and interactively analyze the results. In this paper, we tackle the above challenges and contribute 1) a novel algorithm for generating batches of complete and meaningful textual counterfactuals by removing and replacing text segments in different granularities, and 2) LLM Analyzer, an interactive visualization tool to help users understand an LLM's behaviors by interactively inspecting and aggregating meaningful counterfactuals. We evaluate the proposed algorithm by the grammatical correctness of its generated counterfactuals using 1,000 samples from medical, legal, finance, education, and news datasets. In our experiments, 97.2% of the counterfactuals are grammatically correct. Through a use case, user studies, and feedback from experts, we demonstrate the usefulness and usability of the proposed interactive visualization tool.",23,4,2024
Towards a Deterministic Interpretation of Quantum Mechanics: Insights from Dynamical Systems,"Experiments violating Bell's inequality appear to indicate deterministic models do not correspond to a realistic theory of quantum mechanics. The theory of pilot waves seemingly overcomes this hurdle via nonlocality and statistical dependence, however it necessitates the existence of ""ghost waves"". This manuscript develops a deterministic dynamical system with local interactions. The aggregate behavior of the trajectories are reminiscent of a quantum particle evolving under the Schrödinger equation and reminiscent of Feynman's path integral interpretation in three canonical examples: motion in free space, double slit diffraction, and superluminal barrier traversal. Moreover, the system bifurcates into various dynamical regimes including a classical limit. These results illustrate a deterministic alternative to probabilistic interpretations and aims to shed light on the transition from quantum to classical mechanics.",23,4,2024
"Science Written by Generative AI is Perceived as Less Intelligent, but More Credible and Trustworthy than Science Written by Humans","This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance public trust in science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work assessed linguistic simplicity across such summaries and public perceptions. Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used GPT-4 to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Finally, Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable public perceptions of scientists (their credibility, trustworthiness) than more complexly-written human PNAS summaries. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.",23,4,2024
SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning,"The pre-trained Large Language Models (LLMs) can be adapted for many downstream tasks and tailored to align with human preferences through fine-tuning. Recent studies have discovered that LLMs can achieve desirable performance with only a small amount of high-quality data, suggesting that a large amount of the data in these extensive datasets is redundant or even harmful. Identifying high-quality data from vast datasets to curate small yet effective datasets has emerged as a critical challenge. In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning. SHED eliminates the need for human intervention or the use of commercial LLMs. Moreover, the datasets curated through SHED exhibit transferability, indicating they can be reused across different LLMs with consistently high performance. We conduct extensive experiments to evaluate the datasets curated by SHED. The results demonstrate SHED's superiority over state-of-the-art methods across various tasks and LLMs; notably, datasets comprising only 10% of the original data selected by SHED achieve performance comparable to or surpassing that of the full datasets.",23,4,2024
A Survey on the Real Power of ChatGPT,"ChatGPT has changed the AI community and an active research line is the performance evaluation of ChatGPT. A key challenge for the evaluation is that ChatGPT is still closed-source and traditional benchmark datasets may have been used by ChatGPT as the training data. In this paper, (i) we survey recent studies which uncover the real performance levels of ChatGPT in seven categories of NLP tasks, (ii) review the social implications and safety issues of ChatGPT, and (iii) emphasize key challenges and opportunities for its evaluation. We hope our survey can shed some light on its blackbox manner, so that researchers are not misleaded by its surface generation.",22,4,2024
Local and Global Log-Gradient estimates of solutions to $Δ_pv+bv^q+cv^r =0$ on manifolds and applications,"In this paper, we employ the Nash-Moser iteration technique to study local and global properties of positive solutions to the equation $$\Delta_pv+bv^q+cv^r =0$$ on complete Riemannian manifolds with Ricci curvature bounded from below, where $b, c\in\mathbb R$, $p>1$, and $q\leq r$ are some real constants. Assuming certain conditions on $b,\, c,\, p,\, q$ and $r$, we derive succinct Cheng-Yau type gradient estimates for positive solutions, which is of sharp form. These gradient estimates allow us to obtain some Liouville-type theorems and Harnack inequalities. Our Liouville-type results are novel even in Euclidean spaces. Based on the local gradient estimates and a trick of Sung and Wang, we also obtain the global gradient estimates for such solutions. As applications we show the uniqueness of positive solutions to some generalized Allen-Cahn equation and Fisher-KPP equation.",22,4,2024
"Cold plasma treatment boosts barley germination and seedling vigor: Insights into soluble sugar, starch, and protein modifications","This study investigates the impact of three cold plasma treatments on barley seed germination: direct treatment of dry seeds (DDS), direct treatment of water-soaked seeds (DWS), and indirect treatment of seeds using plasma-activated water (IPAW).",18,4,2024
Oxygen vacancies modulated VO2 for neurons and Spiking Neural Network construction,"Artificial neuronal devices are the basic building blocks for neuromorphic computing systems, which have been motivated by realistic brain emulation. Aiming for these applications, various device concepts have been proposed to mimic the neuronal dynamics and functions. While till now, the artificial neuron devices with high efficiency, high stability and low power consumption are still far from practical application. Due to the special insulator-metal phase transition, Vanadium Dioxide (VO2) has been considered as an idea candidate for neuronal device fabrication. However, its intrinsic insulating state requires the VO2 neuronal device to be driven under large bias voltage, resulting in high power consumption and low frequency. Thus in the current study, we have addressed this challenge by preparing oxygen vacancies modulated VO2 film(VO2-x) and fabricating the VO2-x neuronal devices for Spiking Neural Networks (SNNs) construction. Results indicate the neuron devices can be operated under lower voltage with improved processing speed. The proposed VO2-x based back-propagation SNNs (BP-SNNs) system, trained with the MNIST dataset, demonstrates excellent accuracy in image recognition. Our study not only demonstrates the VO2-x based neurons and SNN system for practical application, but also offers an effective way to optimize the future neuromorphic computing systems by defect engineering strategy.",16,4,2024
Direct Training Needs Regularisation: Anytime Optimal Inference Spiking Neural Network,"Spiking Neural Network (SNN) is acknowledged as the next generation of Artificial Neural Network (ANN) and hold great promise in effectively processing spatial-temporal information. However, the choice of timestep becomes crucial as it significantly impacts the accuracy of the neural network training. Specifically, a smaller timestep indicates better performance in efficient computing, resulting in reduced latency and operations. While, using a small timestep may lead to low accuracy due to insufficient information presentation with few spikes. This observation motivates us to develop an SNN that is more reliable for adaptive timestep by introducing a novel regularisation technique, namely Spatial-Temporal Regulariser (STR). Our approach regulates the ratio between the strength of spikes and membrane potential at each timestep. This effectively balances spatial and temporal performance during training, ultimately resulting in an Anytime Optimal Inference (AOI) SNN. Through extensive experiments on frame-based and event-based datasets, our method, in combination with cutoff based on softmax output, achieves state-of-the-art performance in terms of both latency and accuracy. Notably, with STR and cutoff, SNN achieves 2.14 to 2.89 faster in inference compared to the pre-configured timestep with near-zero accuracy drop of 0.50% to 0.64% over the event-based datasets. Code available:this https URL",15,4,2024
CUDA-Accelerated Soft Robot Neural Evolution with Large Language Model Supervision,"This paper addresses the challenge of co-designing morphology and control in soft robots via a novel neural network evolution approach. We propose an innovative method to implicitly dual-encode soft robots, thus facilitating the simultaneous design of morphology and control. Additionally, we introduce the large language model to serve as the control center during the evolutionary process. This advancement considerably optimizes the evolution speed compared to traditional soft-bodied robot co-design methods. Further complementing our work is the implementation of Gaussian positional encoding - an approach that augments the neural network's comprehension of robot morphology. Our paper offers a new perspective on soft robot design, illustrating substantial improvements in efficiency and comprehension during the design and evolutionary process.",12,4,2024
Pricing Catastrophe Bonds -- A Probabilistic Machine Learning Approach,"This paper proposes a probabilistic machine learning method to price catastrophe (CAT) bonds in the primary market. The proposed method combines machine-learning-based predictive models with Conformal Prediction, an innovative algorithm that generates distribution-free probabilistic forecasts for CAT bond prices. Using primary market CAT bond transaction records between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads than traditional regression-based methods. Furthermore, the proposed method generates more informative prediction intervals than linear regression and identifies important nonlinear relationships between various risk factors and bond spreads, suggesting that linear regressions could misestimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of CAT bonds.",10,4,2024
Loyal Wingman Assessment: Social Navigation for Human-Autonomous Collaboration in Simulated Air Combat,"This study proposes social navigation metrics for autonomous agents in air combat, aiming to facilitate their smooth integration into pilot formations. The absence of such metrics poses challenges to safety and effectiveness in mixed human-autonomous teams. The proposed metrics prioritize naturalness and comfort. We suggest validating them through a user study involving military pilots in simulated air combat scenarios alongside autonomous loyal wingmen. The experiment will involve setting up simulations, designing scenarios, and evaluating performance using feedback from questionnaires and data analysis. These metrics aim to enhance the operational performance of autonomous loyal wingmen, thereby contributing to safer and more strategic air combat.",30,4,2024
Role of local anisotropy in hybrid stars,"Using the Bower-Liang model, we discuss how pressure anisotropies affect the microscopic and macroscopic properties of hybrid stars. We find that anisotropies affect the maximum mass, central density, and radius of the canonical stars. Anisotropies also affect the minimum neutron star mass that presents quarks in their core, as well as the total amount of quarks for the maximally massive stars. We also confront our results with standard constraints, such as the radius and the tidal parameter of the canonical star, as well as the mass and radius of the PSR J0740+6620 pulsar. We observe that moderate values for anisotropies could fulfill these constraints simultaneously. On the other hand, within more extreme degrees of anisotropies, more speculative constraints such as black widow pulsars PSR J0952-0607 and the mass-gap object in the GW190814 event can be explained as hybrid stars. We also investigate the role of anisotropies in the neutron stars' moment of inertia.",30,4,2024
On the Electromagnetic Mass Dilemma,"We show that a charged sphere moving at a constant velocity $v$ exhibits a mass due to electromagnetic radiation, expressed as $4/(3+(v/c)^2) (E/c^2)$, where $E$ is the electromagnetic energy and $c$ the speed of light in vacuum. Our finding reconciles the longstanding mismatch between the electromagnetic mass calculated from the classical electrodynamics' $4/3 (E/c^2)$ and the relativistic theory.",29,4,2024
Bayesian-Guided Generation of Synthetic Microbiomes with Minimized Pathogenicity,"Synthetic microbiomes offer new possibilities for modulating microbiota, to address the barriers in multidtug resistance (MDR) research. We present a Bayesian optimization approach to enable efficient searching over the space of synthetic microbiome variants to identify candidates predictive of reduced MDR. Microbiome datasets were encoded into a low-dimensional latent space using autoencoders. Sampling from this space allowed generation of synthetic microbiome signatures. Bayesian optimization was then implemented to select variants for biological screening to maximize identification of designs with restricted MDR pathogens based on minimal samples. Four acquisition functions were evaluated: expected improvement, upper confidence bound, Thompson sampling, and probability of improvement. Based on each strategy, synthetic samples were prioritized according to their MDR detection. Expected improvement, upper confidence bound, and probability of improvement consistently produced synthetic microbiome candidates with significantly fewer searches than Thompson sampling. By combining deep latent space mapping and Bayesian learning for efficient guided screening, this study demonstrated the feasibility of creating bespoke synthetic microbiomes with customized MDR profiles.",29,4,2024
Estimation of Time-to-Total Knee Replacement Surgery,A survival analysis model for predicting time-to-total knee replacement (TKR) was developed using features from medical images and clinical measurements. Supervised and self-supervised deep learning approaches were utilized to extract features from radiographs and magnetic resonance images. Extracted features were combined with clinical and image assessments for survival analysis using random survival forests. The proposed model demonstrated high discrimination power by combining deep learning features and clinical and image assessments using a fusion of multiple modalities. The model achieved an accuracy of 75.6% and a C-Index of 84.8% for predicting the time-to-TKR surgery. Accurate time-to-TKR predictions have the potential to help assist physicians to personalize treatment strategies and improve patient outcomes.,29,4,2024
Bi-objective optimization of a VRP problem applied to urban solid waste collection through a model that includes the visual attraction of routes,"The compactness of routes in distribution plans is a criterion that has not been sufficiently explored in the literature related to logistics distribution but has shown to have a significant impact on the practical implementation of routing plans, for example in solid waste collection problems. In this regard, this article presents a bi-objective model to optimize the vehicle routing problem with time constraints, considering the minimization of travel times and the compactness of routes. Experimental tests were conducted on small-scale instances using two exact solution methods for multi-objective problems: weighted sum and augmented {\epsilon}-constraint methods. The results obtained allowed us to explore the trade-off relationship between both objectives while evaluating the computational efficiency of both multi-objective solution methods.",28,4,2024
Small noise perturbations of stochastic ergodic control problems,"Using small noise limit approach, we study degenerate stochastic ergodic control problems and as a byproduct obtain error bounds for the $\varepsilon$-optimal controls. We also establish tunneling for a special ergodic control problem and give a representation of the ergodic value using the tunneled Markov chain.",28,4,2024
Research and application of artificial intelligence based webshell detection model: A literature review,"Webshell, as the ""culprit"" behind numerous network attacks, is one of the research hotspots in the field of cybersecurity. However, the complexity, stealthiness, and confusing nature of webshells pose significant challenges to the corresponding detection schemes. With the rise of Artificial Intelligence (AI) technology, researchers have started to apply different intelligent algorithms and neural network architectures to the task of webshell detection. However, the related research still lacks a systematic and standardized methodological process, which is confusing and redundant. Therefore, following the development timeline, we carefully summarize the progress of relevant research in this field, dividing it into three stages: Start Stage, Initial Development Stage, and In-depth Development Stage. We further elaborate on the main characteristics and core algorithms of each stage. In addition, we analyze the pain points and challenges that still exist in this field and predict the future development trend of this field from our point of view. To the best of our knowledge, this is the first review that details the research related to AI-based webshell detection. It is also hoped that this paper can provide detailed technical information for more researchers interested in AI-based webshell detection tasks.",28,4,2024
From Linear to Linearizable Optimization: A Novel Framework with Applications to Stationary and Non-stationary DR-submodular Optimization,"This paper introduces the notion of upper linearizable/quadratizable functions, a class that extends concavity and DR-submodularity in various settings, including monotone and non-monotone cases over different convex sets. A general meta-algorithm is devised to convert algorithms for linear/quadratic maximization into ones that optimize upper quadratizable functions, offering a unified approach to tackling concave and DR-submodular optimization problems. The paper extends these results to multiple feedback settings, facilitating conversions between semi-bandit/first-order feedback and bandit/zeroth-order feedback, as well as between first/zeroth-order feedback and semi-bandit/bandit feedback. Leveraging this framework, new projection-free algorithms are derived using Follow The Perturbed Leader (FTPL) and other algorithms as base algorithms for linear/convex optimization, improving upon state-of-the-art results in various cases. Dynamic and adaptive regret guarantees are obtained for DR-submodular maximization, marking the first algorithms to achieve such guarantees in these settings. Notably, the paper achieves these advancements with fewer assumptions compared to existing state-of-the-art results, underscoring its broad applicability and theoretical contributions to non-convex optimization.",27,4,2024
Generalized Four-momentum for Continuously Distributed Materials,"A four-dimensional differential Euler-Lagrange equation for continuously distributed materials is derived based on the principle of least action, and instead of Lagrangian, this equation contains the Lagrangian density. This makes it possible to determine the density of generalized four-momentum in covariant form as derivative of the Lagrangian density with respect to four-velocity of typical particles of a system taken with opposite sign, and then calculate the generalized four-momentum itself. It is shown that the generalized four-momentum of all typical particles of a system is an integral four-vector and therefore should be considered as a special type of four-vectors. The presented expression for generalized four-momentum exactly corresponds to the Legendre transformation connecting the Lagrangian and Hamiltonian. The obtained formulas are used to calculate generalized four-momentum of stationary and moving relativistic uniform systems for the Lagrangian with particles and vector fields, including electromagnetic and gravitational fields, acceleration field and pressure field. It turns out that the generalized four-momentum of a moving system depends on the total mass of particles, on the Lorentz factor and on the velocity of the systems center of momentum. Besides, an additional contribution is made by the scalar potentials of the acceleration field and the pressure field at the center of system. The direction of the generalized four-momentum coincides with the direction of four-velocity of the system under consideration, while the generalized four-momentum is part of the relativistic four-momentum of the system.",27,4,2024
Hardware Accelerators for Autonomous Cars: A Review,"Autonomous Vehicles (AVs) redefine transportation with sophisticated technology, integrating sensors, cameras, and intricate algorithms. Implementing machine learning in AV perception demands robust hardware accelerators to achieve real-time performance at reasonable power consumption and footprint. Lot of research and development efforts using different technologies are still being conducted to achieve the goal of getting a fully AV and some cars manufactures offer commercially available systems. Unfortunately, they still lack reliability because of the repeated accidents they have encountered such as the recent one which happened in California and for which the Cruise company had its license suspended by the state of California for an undetermined period [1]. This paper critically reviews the most recent findings of machine vision systems used in AVs from both hardware and algorithmic points of view. It discusses the technologies used in commercial cars with their pros and cons and suggests possible ways forward. Thus, the paper can be a tangible reference for researchers who have the opportunity to get involved in designing machine vision systems targeting AV",26,4,2024
Successive Convexification for Nonlinear Model Predictive Control with Continuous-Time Constraint Satisfaction,"We propose a nonlinear model predictive control (NMPC) framework based on a direct optimal control method that ensures continuous-time constraint satisfaction and accurate evaluation of the running cost, without compromising computational efficiency. We leverage the recently proposed successive convexification framework for trajectory optimization, where: (1) the path constraints and running cost are equivalently reformulated by augmenting the system dynamics, (2) multiple shooting is used for exact discretization, and (3) a convergence-guaranteed sequential convex programming (SCP) algorithm, the prox-linear method, is used to solve the discretized receding-horizon optimal control problems. The resulting NMPC framework is computationally efficient, owing to its support for warm-starting and premature termination of SCP, and its reliance on first-order information only. We demonstrate the effectiveness of the proposed NMPC framework by means of a numerical example with reference-tracking and obstacle avoidance. The implementation is available atthis https URL",26,4,2024
Measurement of Milli-Charged Particles with a moderately large cross section from the Earth's core at IceCube,"It is assumed that heavy dark matter $\phi$ with O(TeV) mass captured by the Earth may decay to relativistic light milli-charged particles (MCPs). These MCPs could be measured by the IceCube neutrino telescope. The massless hidden photon model was taken for MCPs to interact with nuclei, so that the numbers and fluxes of expected MCPs may be evaluated at IceCube. Meanwhile, the numbers of expected neutrino background events were also evaluated at IceCube. Based on the assumption that no events are observed at IceCube in 10 years, the corresponding upper limits on MCP fluxes were calculated at 90\% C. L.. These results indicated that the MCPs from the Earth's core could be directly detected at O(1TeV) energies at IceCube when $2\times10^{-5}\lesssim\epsilon^2\lesssim4.5\times10^{-3}$. And a new region of 100 MeV < $m_{MCP}$ < 10 GeV and $4.47\times10^{-3}$ $\lesssim$ $\epsilon$ $\lesssim$ $9.41\times10^{-2}$ is ruled out in the $m_{MCP}$-$\epsilon$ plane with 10 years of IceCube data.",26,4,2024
Microwave Cavity Mode Optimisation by Background Anti-Resonance Tuning,"To derive the best oscillator phase noise when implementing a high-Q resonator, the spectral line-shape must have high contrast and symmetry. Ideally, this line-shape is Lorentzian, however, in a high mode density spectral region, low-Q background spurious modes interact and distort the resonance. For a sapphire-loaded cavity resonator operating with whispering gallery modes we show that this high contrast and symmetry can be achieved by changing the dimensions of the surrounding cavity shield to tune the background low-Q structures into anti-resonance. This works because the high-Q resonances are primarily defined by the sapphire while the background modes are defined by the cavity shield. Alternatively, it was shown that a similar result can be achieved by exciting the high-Q resonator with a balanced microwave dipole probe in a Mach Zehnder interferometric configuration. The probe was constructed from two separate coaxial electric field probes symmetrically inserted into a cylindrical cavity resonator, from opposite sides with a small gap between them, so they can behave like an active wire dipole antenna. The power into the two separate probes may be matched with an external variable attenuator in one of the arms of the interferometer. Conversely, the phase between the two electric field probes may be changed with an external variable phase shifter, which changes the nature of the field components the probe couples to. The probe couples to the high-Q resonant modes as well as low-Q background modes, which can be made resonant or anti-resonant for the high-Q modes by changing this external phase. When the background modes are in anti-resonance the line shape of the high-Q mode can be made symmetric and with higher contrast. This technique was applied to both whispering gallery sapphire modes, as well as hollow cavity resonators, without changing the dimensions of the cavity.",26,4,2024
Gaussianity and the Kalman Filter: A Simple Yet Complicated Relationship,"One of the most common misconceptions made about the Kalman filter when applied to linear systems is that it requires an assumption that all error and noise processes are Gaussian. This misconception has frequently led to the Kalman filter being dismissed in favor of complicated and/or purely heuristic approaches that are supposedly ""more general"" in that they can be applied to problems involving non-Gaussian noise. The fact is that the Kalman filter provides rigorous and optimal performance guarantees that do not rely on any distribution assumptions beyond mean and error covariance information. These guarantees even apply to use of the Kalman update formula when applied with nonlinear models, as long as its other required assumptions are satisfied. Here we discuss misconceptions about its generality that are often found and reinforced in the literature, especially outside the traditional fields of estimation and control.",25,4,2024
Microstructural and Transport Characteristics of Triply Periodic Bicontinuous Materials,"3D bicontinuous two-phase materials are increasingly gaining interest because of their unique multifunctional characteristics and advancements in techniques to fabricate them. Due to their complex topological and structural properties, it still has been nontrivial to develop explicit microstructure-dependent formulas to predict accurately their physical properties. A primary goal of the present paper is to ascertain various microstructural and transport characteristics of five different models of triply periodic bicontinuous porous materials at a porosity $\phi_1=1/2$: those in which the two-phase interfaces are the Schwarz P, Schwarz D and Schoen G minimal surfaces as well as two different pore-channel structures. We ascertain their spectral densities, pore-size distribution functions, local volume-fraction variances, and hyperuniformity order metrics and then use this information to estimate certain effective transport properties via closed-form microstructure-property formulas. Specifically, we estimate the recently introduced time-dependent diffusion spreadability exactly from the spectral density. Moreover, we accurately estimate the fluid permeability of such porous materials from the second moment of the pore-size function and the formation factor, a measure of the tortuosity of the pore space. We also rigorously bound the permeability from above using the spectral density. For the five models with identical cubic unit cells, we find that the permeability, inverse of the specific surface, hyperuniformity order metric, pore-size second moment and long-time spreadability behavior are all positively correlated and rank order the structures in exactly the same way. We also conjecture what structures maximize the fluid permeability for arbitrary porosities and show that this conjecture must be true in the extreme porosity limits by identifying the corresponding optimal structures.",25,4,2024
A Hybrid Probabilistic Battery Health Management Approach for Robust Inspection Drone Operations,"Health monitoring of remote critical infrastructure is a complex and expensive activity due to the limited infrastructure accessibility. Inspection drones are ubiquitous assets that enhance the reliability of critical infrastructures through improved accessibility. However, due to the harsh operation environment, it is crucial to monitor their health to ensure successful inspection operations. The battery is a key component that determines the overall reliability of the inspection drones and, with an appropriate health management approach, contributes to reliable and robust inspections. In this context, this paper presents a novel hybrid probabilistic approach for battery end-of-discharge (EOD) voltage prediction of Li-Po batteries. The hybridization is achieved in an error-correction configuration, which combines physics-based discharge and probabilistic error-correction models to quantify the aleatoric and epistemic uncertainty. The performance of the hybrid probabilistic methodology was empirically evaluated on a dataset comprising EOD voltage under varying load conditions. The dataset was obtained from real inspection drones operated on different flights, focused on offshore wind turbine inspections. The proposed approach has been tested with different probabilistic methods and demonstrates 14.8% improved performance in probabilistic accuracy compared to the best probabilistic method. In addition, aleatoric and epistemic uncertainties provide robust estimations to enhance the diagnosis of battery health-states.",24,4,2024
High-resolution spatio-temporal strain imaging reveals loss mechanisms in a surface acoustic wave device,"Surface acoustic wave devices are key components for processing radio frequency signals in wireless communication because these devices offer simultaneously high performance, compact size and low cost. The optimization of the device structure requires a quantitative understanding of energy conversion and loss mechanisms. Stroboscopic full-field diffraction x-ray microscopy studies of a prototypical one-port resonator device revealed the existence of unanticipated acoustic loss. A non-uniform acoustic excitation in the active area was responsible for the substantial end and side leakages observed at the design frequency. Quantitative analysis of the strain amplitude using a wave decomposition method allowed the determination of several key device parameters. This high-resolution spatiotemporal strain imaging technique is, more generally, suited for studying nanophononics, specifically when the feature size is smaller than optical wavelengths. The strain sensitivity allows precise measurement of acoustic waves with picometer-scale amplitude.",20,4,2024
A new understanding on the history of developing MRI for cancer detection,"Science is about facts and truth. Yet sometimes the truth and facts are not obvious. For example, in the field of MRI (Magnetic Resonance Imaging), there has been a long-lasting debate about who were the major contributors in its development. Particularly, there was a strong dispute between the followers of two scientists, R. Damadian and P. Lauterbur. In this review, we carefully trace the major developments in the use of NMR for cancer detection starting almost 50 years ago. The research records show that the truth was beyond the claims of either research camps. The development of NMR for cancer detection involved multiple research groups, who made critical contributions at different junctures.",17,4,2024
Vortexes as systems specific to the Acoustic World,"In this paper we study the properties of vortexes, as systems specific to the Acoustic World, using both hydrodynamic theory and the corresponding hydrodynamic Maxwell equations. According to this study, it follows that the vortex behaves like an acoustic dipole that has intrinsic/internal angular momentum. The system of two identical vortices also has orbital angular momentum and behaves, at distances much greater than the distance between the axes of the vortices, as a single vortex. With the help of Maxwell's hydrodynamic equations for the vortex we deduced the force between two vortices and obtained the expression of the equivalent mass of the vortex and the permittivity of the electroacoustic field. We also obtained and interpreted the expression for the energy density of the acoustic field. The density and pressure variations induced by the vortex cause the change in the propagation speed of the acoustic waves and the acoustic lensing property of the vortex.",15,4,2024
Arbitrage impact on the relationship between XRP price and correlation tensor spectra of transaction networks,"The increasing use of cryptoassets for international remittances has proven to be faster and more cost-effective, particularly for migrants without access to traditional banking. However, the inherent volatility of cryptoasset prices, independent of blockchain-based remittance mechanisms, introduces potential risks during periods of high volatility. This study investigates the intricate dynamics between XRP price fluctuations across diverse crypto exchanges and the correlation of the largest singular values of the correlation tensor of XRP transaction networks. Particularly, we show the impact of arbitrage opportunities across different crypto exchanges on the relationship between XRP price and correlation tensor spectra of transaction networks. Distinct periods, non-bubble and bubble, showcase different characteristics in XRP price fluctuations. Establishing a connection between XRP price and transaction networks, we compute correlation tensors and singular values, emphasizing the significance of the largest singular value. Comparisons with reshuffled and Gaussian random correlation tensors validate the uniqueness of the empirical tensor. A set of simulated weekly XRP prices, resembling arbitrage opportunities across various crypto exchanges, further confirms the robustness of our findings. It reveals a pronounced anti-correlation during bubble periods and a non-significant correlation during non-bubble periods with the largest singular value, irrespective of price fluctuations across different crypto exchanges.",15,4,2024
Non-trivial solution to a simple problem,"The problem of finding the frequencies of small longitudinal oscillations of a spring having a finite mass and stiffness, attached at one end to a wall and at the other end to a body of finite mass, is discussed. This problem was repeatedly proposed at Olympiads for schoolchildren, in various lessons on the Internet, and even on tests in mechanics for students of universities. In all the cases known to me, the implied solution was actually wrong. I discuss two cases: (A) a spring lies on a smooth table, (B) a spring is attached to the ceiling. It is shown that the solution to this simply formulated problem is non-trivial.",15,4,2024
Role of stress/strain in tailoring the magnetic and transport properties of magnetic thin films and multilayers,"Magnetic anisotropy is a fundamental property of magnetic materials that determines the alignment of the spins along the preferential direction, called the easy axis of magnetization. Magnetic polycrystalline thin films offer several advantages over magnetic epitaxial thin films because of fabrication flexibility, higher coercivity and improved magnetic stability, higher magnetoresistance (useful in magneto-resistive devices such as magnetic field sensors and MRAM cells), cost-effectiveness and thermal stability, etc. In the case of polycrystalline thin films or multilayers, Magneto-crystalline anisotropy (MCA) is not expected due to the random orientation of grains. Therefore, understanding the origin of uniaxial magnetic anisotropy (UMA) is generally difficult and can't be understood in terms of crystal orientation. The origin of UMA in polycrystalline films is often related to the preparation conditions and substrate properties. In the present thesis, we have provided direct in-situ real-time evidence of the stress dependence of magnetic anisotropy through the multibeam optical stress sensor (MOSS) technique. Also, we have tuned the magnetic anisotropy in strength and direction using externally applied stress. To further increase the strength of the magnetic anisotropy, we have developed a new technique that creates a multilayer using a single magnetic material through sequential oblique and normal depositions. This oblique angle deposition technique also helps reduce the penetration of the top ferromagnetic layer inside the organic semiconductor layer in organic spin valve structures. We confirm our results through various in-situ (in UHV and HV) and ex-situ temperature-dependent conventional and unconventional structural, morphological, and magnetic measurements (both lab-based and synchrotron-based) that include MOKE, KERR, GIXRD, AFM, RHEED, and GISAXS, etc. measurements.",14,4,2024
Urban planning in a context of rapid urban growth. A large scale review of urban plans in Africa,"As the African continent continues to urbanise, cities are becoming increasingly central to the transformations of societies and economies. Many studies highlight the limits of urban planning in these cities, emphasising the high share of population living in slums and the low levels of services that reach neighbourhoods. Less attention is given to the urban planning activities that try to prevent or improve these conditions. This analysis of urban plans illustrates that plans are more widespread than commonly thought. They also, for the large part, consider spatial growth. The low number of cities that grew within the projected boundaries of these plans is a symptom of numerous bottlenecks that constrain planning systems in these countries. Examples of these include the disregard of the full built-up areas at the time of the plan's approval and the missing link between the plans and the financial means allocated for its delivery. This article identifies opportunities to overcome these barriers such as flexible and adaptable urban plans that consider the entire built-up area of the agglomeration.",14,4,2024
The Quantum Dynamics of Cost Accounting: Investigating WIP via the Time-Independent Schrodinger Equation,"The intersection of quantum theory and accounting presents a novel and intriguing frontier in exploring financial valuation and accounting practices. This paper applies quantum theory to cost accounting, specifically Work in Progress (WIP) valuation. WIP is conceptualized as materials in a quantum superposition state whose financial value remains uncertain until observed or measured. This work comprehensively reviews the seminal works that explored the overlap between quantum theory and accounting. The primary contribution of this work is a more nuanced understanding of the uncertainties involved, which emerges by applying quantum phenomena to model the complexities and uncertainties inherent in managerial accounting. In contrast, previous works focus more on financial accounting or general accountancy.",11,4,2024
Synchronization in a market model with time delays,"We examine a system of N=2 coupled non-linear delay-differential equations representing financial market dynamics. In such time delay systems, coupled oscillations have been derived. We linearize the system for small time delays and study its collective dynamics. Using analytical and numerical solutions, we obtain the bifurcation diagrams and analyze the corresponding regions of amplitude death, phase locking, limit cycles and market synchronization in terms of the system frequency-like parameters and time delays. We further numerically explore higher order systems with N>2, and demonstrate that limit cycles can be maintained for coupled N-asset models with appropriate parameterization.",9,4,2024
"Does there exist the applicability limit of PDE to describe physical phenomena? -- A personal survey of Quantization, QED, Turbulence","What does it mean to study PDE(=Partial Differential Equation)? How and what to do ""to claim proudly that I'm studying a certain PDE""? Newton mechanic uses mainly ODE(=Ordinary Differential Equation) and describes nicely movements of Sun, Moon and Earth etc. Now, so-called quantum phenomenum is described by, say Schrödinger equation, PDE which explains both wave and particle characters after quantization of ODE. The coupled Maxwell-Dirac equation is also ""quantized"" and QED(=Quantum Electro-Dynamics) theory is invented by physicists. Though it is said this QED gives very good coincidence between theoretical and experimental observed quantities, but what is the equation corresponding to QED? Or, is it possible to describe QED by ""equation"" in naive sense?",9,4,2024
Airbus A32x vs Boeing 737 Safety Occurrences,"Safety is the priority for airlines. Airlines are sensitive to passengers' perceptions of safety, having randomly assigned the Boeing 737 Max to routes and times. Historically, Boeing has been considered more reliable and safer than Airbus. Hence, it is worth asking the question, are there any differences in the safety occurrences of the core narrow-body single-aisle aircraft of Boeing and Airbus; the 737 and A32x families of aircraft. Utilizing the International Civil Aviation Organization safety occurrence data, from 2008 to 2019, these aircraft were compared in terms of occurrence type, occurrence category, phase of flight, injury level, and fatalities. It was found that Boeing had more accidents than expected, while Airbus had less (p=0.015). In terms of fatalities Boeing has had more than expected, with Airbus less (p<0.001). Looking at just accidents, only the number of fatalities was statistically significantly different. In both cases, the increased number of fatalities for Boeing appears to be the result of the two recent Boeing 737 Max accidents. Looking at the reported fatal and hull loss accident rates, it was also found that the annual reduction for the Airbus A32x aircraft were better than for the Boeing 737 aircraft.",9,4,2024
Shrinking a gradient index lens antenna system with a spaceplate,"The miniaturisation of optical systems is an ongoing challenge across the electromagnetic spectrum. While the thickness of optical elements themselves can be reduced using advances in metamaterials, it is the voids between these elements -- which are necessary parts of an optical system -- that occupy most of the volume. Recently, a novel optical element coined a `spaceplate' was proposed, that replaces a region of free space with a thinner optical element that emulates the free-space optical response function -- thus having the potential to substantially shrink the volume of optical systems. While there have been a few proof-of-principle demonstrations of spaceplates, they have not yet been deployed in a real-world optical system. In this work, we use a bespoke-designed spaceplate to reduce the length of a gradient index (GRIN) lens microwave antenna. Our antenna is designed to operate at 23.5 GHz, and the incorporation of a nonlocal metamaterial spaceplate enables the distance between the antenna feed and the GRIN lens to be reduced by almost a factor of two. We find the radiation patterns from a conventional and space-squeezed antenna are very similar, with a very low cross-polarisation, and only a minor increase in the side-lobe levels when introducing the spaceplate. Our work represents a first example of a spaceplate integrated into a functional optical system, highlighting the potential for this concept to reduce the physical size of optical systems in real-world applications.",2,4,2024
Is Artificial Intelligence the great filter that makes advanced technical civilisations rare in the universe?,"This study examines the hypothesis that the rapid development of Artificial Intelligence (AI), culminating in the emergence of Artificial Superintelligence (ASI), could act as a ""Great Filter"" that is responsible for the scarcity of advanced technological civilisations in the universe. It is proposed that such a filter emerges before these civilisations can develop a stable, multiplanetary existence, suggesting the typical longevity (L) of a technical civilization is less than 200 years. Such estimates for L, when applied to optimistic versions of the Drake equation, are consistent with the null results obtained by recent SETI surveys, and other efforts to detect various technosignatures across the electromagnetic spectrum. Through the lens of SETI, we reflect on humanity's current technological trajectory - the modest projections for L suggested here, underscore the critical need to quickly establish regulatory frameworks for AI development on Earth and the advancement of a multiplanetary society to mitigate against such existential threats. The persistence of intelligent and conscious life in the universe could hinge on the timely and effective implementation of such international regulatory measures and technological endeavours.",1,4,2024
Deep Learning for Educational Data Science,"With the ever-growing presence of deep artificial neural networks in every facet of modern life, a growing body of researchers in educational data science -- a field consisting of various interrelated research communities -- have turned their attention to leveraging these powerful algorithms within the domain of education. Use cases range from advanced knowledge tracing models that can leverage open-ended student essays or snippets of code to automatic affect and behavior detectors that can identify when a student is frustrated or aimlessly trying to solve problems unproductively -- and much more. This chapter provides a brief introduction to deep learning, describes some of its advantages and limitations, presents a survey of its many uses in education, and discusses how it may further come to shape the field of educational data science.",12,4,2024
ML-based handover prediction over a real O-RAN deployment using RAN Intelligent controller,"O-RAN introduces intelligent and flexible network control in all parts of the network. The use of controllers with open interfaces allow us to gather real time network measurements and make intelligent/informed decision. The work in this paper focuses on developing a use-case for open and reconfigurable networks to investigate the possibility to predict handover events and understand the value of such predictions for all stakeholders that rely on the communication network to conduct their business. We propose a Long-Short Term Memory Machine Learning approach that takes standard Radio Access Network measurements to predict handover events. The models were trained on real network data collected from a commercial O-RAN setup deployed in our OpenIreland testbed. Our results show that the proposed approach can be optimized for either recall or precision, depending on the defined application level objective. We also link the performance of the Machine Learning (ML) algorithm to the network operation cost. Our results show that ML-based matching between the required and available resources can reduce operational cost by more than 80%, compared to long term resource purchases.",14,4,2024
Enhancing Predictive Accuracy in Pharmaceutical Sales Through An Ensemble Kernel Gaussian Process Regression Approach,"This research employs Gaussian Process Regression (GPR) with an ensemble kernel, integrating Exponential Squared, Revised Matérn, and Rational Quadratic kernels to analyze pharmaceutical sales data. Bayesian optimization was used to identify optimal kernel weights: 0.76 for Exponential Squared, 0.21 for Revised Matérn, and 0.13 for Rational Quadratic. The ensemble kernel demonstrated superior performance in predictive accuracy, achieving an \( R^2 \) score near 1.0, and significantly lower values in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). These findings highlight the efficacy of ensemble kernels in GPR for predictive analytics in complex pharmaceutical sales datasets.",15,4,2024
SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks,"Weight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an 'event-driven' approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch:this https URL.",15,4,2024
Decoder Decomposition for the Analysis of the Latent Space of Nonlinear Autoencoders With Wind-Tunnel Experimental Data,"Turbulent flows are chaotic and multi-scale dynamical systems, which have large numbers of degrees of freedom. Turbulent flows, however, can be modelled with a smaller number of degrees of freedom when using the appropriate coordinate system, which is the goal of dimensionality reduction via nonlinear autoencoders. Autoencoders are expressive tools, but they are difficult to interpret. The goal of this paper is to propose a method to aid the interpretability of autoencoders. This is the decoder decomposition. First, we propose the decoder decomposition, which is a post-processing method to connect the latent variables to the coherent structures of flows. Second, we apply the decoder decomposition to analyse the latent space of synthetic data of a two-dimensional unsteady wake past a cylinder. We find that the dimension of latent space has a significant impact on the interpretability of autoencoders. We identify the physical and spurious latent variables. Third, we apply the decoder decomposition to the latent space of wind-tunnel experimental data of a three-dimensional turbulent wake past a bluff body. We show that the reconstruction error is a function of both the latent space dimension and the decoder size, which are correlated. Finally, we apply the decoder decomposition to rank and select latent variables based on the coherent structures that they represent. This is useful to filter unwanted or spurious latent variables, or to pinpoint specific coherent structures of interest. The ability to rank and select latent variables will help users design and interpret nonlinear autoencoders.",25,4,2024
Landmark Alternating Diffusion,"Alternating Diffusion (AD) is a commonly applied diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the landmark diffusion idea considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.",29,4,2024
Fast and label-free 3D virtual H&E histology via active modulation-assisted dynamic full-field OCT,"Pathological features are the gold standard for tumor diagnosis, guiding treatment and prognosis. However, standard histopathological process is labor-intensive and time-consuming, while frozen sections have lower accuracy. Dynamic full-field optical coherence tomography (D-FFOCT) offers rapid histologic information by measuring the subcellular dynamics of fresh, unprocessed tissues. However, D-FFOCT images suffer from abrupt shifts in hue and brightness, which is confusing for pathologists and diminish their interpretability and reliability. Here, we present active phase modulation-assisted D-FFOCT (APMD-FFOCT) to improve the imaging stability and enhance the contrast of static tissues. This enables us to further employ an unsupervised deep learning to convert APMD-FFOCT images into virtual hematoxylin and eosin (H&E) stained images for the first time. Three-dimensional (3D) virtual H&E-stained images have been obtained at a scanning rate of 1 frame per second, as demonstrated in cancer diagnosis for human central nervous system and breast. The results prove that this new method will play a unique and important role in intraoperative histology.",27,4,2024
Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks,"Adversarial examples have been shown to cause neural networks to fail on a wide range of vision and language tasks, but recent work has claimed that Bayesian neural networks (BNNs) are inherently robust to adversarial perturbations. In this work, we examine this claim. To study the adversarial robustness of BNNs, we investigate whether it is possible to successfully break state-of-the-art BNN inference methods and prediction pipelines using even relatively unsophisticated attacks for three tasks: (1) label prediction under the posterior predictive mean, (2) adversarial example detection with Bayesian predictive uncertainty, and (3) semantic shift detection. We find that BNNs trained with state-of-the-art approximate inference methods, and even BNNs trained with Hamiltonian Monte Carlo, are highly susceptible to adversarial attacks. We also identify various conceptual and experimental errors in previous works that claimed inherent adversarial robustness of BNNs and conclusively demonstrate that BNNs and uncertainty-aware Bayesian prediction pipelines are not inherently robust against adversarial attacks.",27,4,2024
Synchrony for weak coupling in the complexified Kuramoto model,"We present the finite-size Kuramoto model analytically continued from real to complex variables and analyze its collective dynamics. For strong coupling, synchrony appears through locked states that constitute attractors, as for the real-variable system. However, synchrony persists in the form of \textit{complex locked states} for coupling strengths $K$ below the transition $K^{(\text{pl})}$ to classical \textit{phase locking}. Stable complex locked states indicate a locked sub-population of zero mean frequency in the real-variable model and their imaginary parts help identifying which units comprise that sub-population. We uncover a second transition at $K'<K^{(\text{pl})}$ below which complex locked states become linearly unstable yet still exist for arbitrarily small coupling strengths.",28,4,2024
Revealing the Two Sides of Data Augmentation: An Asymmetric Distillation-based Win-Win Solution for Open-Set Recognition,"In this paper, we reveal the two sides of data augmentation: enhancements in closed-set recognition correlate with a significant decrease in open-set recognition. Through empirical investigation, we find that multi-sample-based augmentations would contribute to reducing feature discrimination, thereby diminishing the open-set criteria. Although knowledge distillation could impair the feature via imitation, the mixed feature with ambiguous semantics hinders the distillation. To this end, we propose an asymmetric distillation framework by feeding teacher model extra raw data to enlarge the benefit of teacher. Moreover, a joint mutual information loss and a selective relabel strategy are utilized to alleviate the influence of hard mixed samples. Our method successfully mitigates the decline in open-set and outperforms SOTAs by 2%~3% AUROC on the Tiny-ImageNet dataset and experiments on large-scale dataset ImageNet-21K demonstrate the generalization of our method.",28,4,2024
High dimensional analysis reveals conservative sharpening and a stochastic edge of stability,"Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization.",30,4,2024
Aspect and Opinion Term Extraction Using Graph Attention Network,"In this work we investigate the capability of Graph Attention Network for extracting aspect and opinion terms. Aspect and opinion term extraction is posed as a token-level classification task akin to named entity recognition. We use the dependency tree of the input query as additional feature in a Graph Attention Network along with the token and part-of-speech features. We show that the dependency structure is a powerful feature that in the presence of a CRF layer substantially improves the performance and generates the best result on the commonly used datasets from SemEval 2014, 2015 and 2016. We experiment with additional layers like BiLSTM and Transformer in addition to the CRF layer. We also show that our approach works well in the presence of multiple aspects or sentiments in the same query and it is not necessary to modify the dependency tree based on a single aspect as was the original application for sentiment classification.",30,4,2024
DELINE8K: A Synthetic Data Pipeline for the Semantic Segmentation of Historical Documents,"Document semantic segmentation is a promising avenue that can facilitate document analysis tasks, including optical character recognition (OCR), form classification, and document editing. Although several synthetic datasets have been developed to distinguish handwriting from printed text, they fall short in class variety and document diversity. We demonstrate the limitations of training on existing datasets when solving the National Archives Form Semantic Segmentation dataset (NAFSS), a dataset which we introduce. To address these limitations, we propose the most comprehensive document semantic segmentation synthesis pipeline to date, incorporating preprinted text, handwriting, and document backgrounds from over 10 sources to create the Document Element Layer INtegration Ensemble 8K, or DELINE8K dataset. Our customized dataset exhibits superior performance on the NAFSS benchmark, demonstrating it as a promising tool in further research. The DELINE8K dataset is available atthis https URL.",30,4,2024
Deconfinement to confinement by generalizing BRST symmetry on the sphere,"Recently it has been shown that the theory in the quadratic gauge on 4-sphere, $\mathbb{S}^{4}$ consists of two phases namely, the confined and the deconfined phases. A suitable finite field dependent BRST (FFBRST) transformation interrelates two different gauge fixed theories. In this paper, we use the FFBRST technique on the curved space for the first time and elaborate a novel application of it. We propose two different formulations of this technique that transform the deconfined phase action on sphere to the confined phase action on sphere inside the quadratic gauge. Both proposed passages change the phase with BRST invariance to the phase without BRST invariance unlike usual connections where the FFBRST operation leave the BRST symmetry intact and there is a unique field theoretic essence of them, which makes them particularly important to study. Thus, the two different field redefinitions act as a new mechanism that execute phase transition between two real QCD phases on 4-sphere other than ghost condensation process.",30,4,2024
Persistent Homology generalizations for Social Media Network Analysis,"This study details an approach for the analysis of social media collected political data through the lens of Topological Data Analysis, with a specific focus on Persistent Homology and the political processes they represent by proposing a set of mathematical generalizations using Gaussian functions to define and analyze these Persistent Homology categories. Three distinct types of Persistent Homologies were recurrent across datasets that had been plotted through retweeting patterns and analyzed through the k-Nearest-Neighbor filtrations. As these Persistent Homologies continued to appear, they were then categorized and dubbed Nuclear, Bipolar, and Multipolar Constellations. Upon investigating the content of these plotted tweets, specific patterns of interaction and political information dissemination were identified, namely Political Personalism and Political Polarization. Through clustering and application of Gaussian density functions, I have mathematically characterized each category, encapsulating their distinctive topological features. The mathematical generalizations of Bipolar, Nuclear, and Multipolar Constellations developed in this study are designed to inspire other political science digital media researchers to utilize these categories as to identify Persistent Homology in datasets derived from various social media platforms, suggesting the broader hypothesis that such structures are bound to be present on political scraped data regardless of the social media it's derived from. This method aims to offer a new perspective in Network Analysis as it allows for an exploration of the underlying shape of the networks formed by retweeting patterns, enhancing the understanding of digital interactions within the sphere of Computational Social Sciences.",30,4,2024
Bias Mitigation via Compensation: A Reinforcement Learning Perspective,"As AI increasingly integrates with human decision-making, we must carefully consider interactions between the two. In particular, current approaches focus on optimizing individual agent actions but often overlook the nuances of collective intelligence. Group dynamics might require that one agent (e.g., the AI system) compensate for biases and errors in another agent (e.g., the human), but this compensation should be carefully developed. We provide a theoretical framework for algorithmic compensation that synthesizes game theory and reinforcement learning principles to demonstrate the natural emergence of deceptive outcomes from the continuous learning dynamics of agents. We provide simulation results involving Markov Decision Processes (MDP) learning to interact. This work then underpins our ethical analysis of the conditions in which AI agents should adapt to biases and behaviors of other agents in dynamic and complex decision-making environments. Overall, our approach addresses the nuanced role of strategic deception of humans, challenging previous assumptions about its detrimental effects. We assert that compensation for others' biases can enhance coordination and ethical alignment: strategic deception, when ethically managed, can positively shape human-AI interactions.",30,4,2024
On a Family of Relaxed Gradient Descent Methods for Quadratic Minimization,"This paper studies the convergence properties of a family of Relaxed $\ell$-Minimal Gradient Descent methods for quadratic optimization; the family includes the omnipresent Steepest Descent method, as well as the Minimal Gradient method. Simple proofs are provided that show, in an appropriately chosen norm, the gradient and the distance of the iterates from optimality converge linearly, for all members of the family. Moreover, the function values decrease linearly, and iteration complexity results are provided. All theoretical results hold when (fixed) relaxation is employed. It is also shown that, given a fixed overhead and storage budget, every Relaxed $\ell$-Minimal Gradient Descent method can be implemented using exactly one matrix vector product. Numerical experiments are presented that illustrate the benefits of relaxation across the family.",30,4,2024
Suvach -- Generated Hindi QA benchmark,"Current evaluation benchmarks for question answering (QA) in Indic languages often rely on machine translation of existing English datasets. This approach suffers from bias and inaccuracies inherent in machine translation, leading to datasets that may not reflect the true capabilities of EQA models for Indic languages. This paper proposes a new benchmark specifically designed for evaluating Hindi EQA models and discusses the methodology to do the same for any task. This method leverages large language models (LLMs) to generate a high-quality dataset in an extractive setting, ensuring its relevance for the target language. We believe this new resource will foster advancements in Hindi NLP research by providing a more accurate and reliable evaluation tool.",30,4,2024
Learning to Communicate Functional States with Nonverbal Expressions for Improved Human-Robot Collaboration,"Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction. Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors. In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck). We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions. The proposed approach was evaluated through a user study with 24 participants. The results demonstrate that: 1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot. 2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process. 3. The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state. 4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states.",30,4,2024
Exploiting Hatred by Targets for Hate Speech Detection on Vietnamese Social Media Texts,"The growth of social networks makes toxic content spread rapidly. Hate speech detection is a task to help decrease the number of harmful comments. With the diversity in the hate speech created by users, it is necessary to interpret the hate speech besides detecting it. Hence, we propose a methodology to construct a system for targeted hate speech detection from online streaming texts from social media. We first introduce the ViTHSD - a targeted hate speech detection dataset for Vietnamese Social Media Texts. The dataset contains 10K comments, each comment is labeled to specific targets with three levels: clean, offensive, and hate. There are 5 targets in the dataset, and each target is labeled with the corresponding level manually by humans with strict annotation guidelines. The inter-annotator agreement obtained from the dataset is 0.45 by Cohen's Kappa index, which is indicated as a moderate level. Then, we construct a baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained language model to leverage the power of text representation of BERTology. Finally, we suggest a methodology to integrate the baseline model for targeted hate speech detection into the online streaming system for practical application in preventing hateful and offensive content on social media.",30,4,2024
Quantum control in the presence of strongly coupled non-Markovian noise,"Controlling quantum systems under correlated non-Markovian noise, particularly when strongly coupled, poses significant challenges in the development of quantum technologies. Traditional quantum control strategies, heavily reliant on precise models, often fail under these conditions. Here, we address the problem by utilizing a data-driven graybox model, which integrates machine learning structures with physics-based elements. We demonstrate single-qubit control, implementing a universal gate set as well as a random gate set, achieving high fidelity under unknown, strongly-coupled non-Markovian non-Gaussian noise, significantly outperforming traditional methods. Our method is applicable to all open finite-dimensional quantum systems, regardless of the type of noise or the strength of the coupling.",30,4,2024
Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair,"In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes. The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes. While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features. To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features. We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair). Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample. To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model. The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity.",30,4,2024
A Nonnested Augmented Subspace Method for Kohn-Sham Equation,"In this paper, a novel adaptive finite element method is proposed to solve the Kohn-Sham equation based on the moving mesh (nonnested mesh) adaptive technique and the augmented subspace method. Different from the classical self-consistent field iterative algorithm which requires to solve the Kohn-Sham equation directly in each adaptive finite element space, our algorithm transforms the Kohn-Sham equation into some linear boundary value problems of the same scale in each adaptive finite element space, and then the wavefunctions derived from the linear boundary value problems are corrected by solving a small-scale Kohn-Sham equation defined in a low-dimensional augmented subspace. Since the new algorithm avoids solving large-scale Kohn-Sham equation directly, a significant improvement for the solving efficiency can be obtained. In addition, the adaptive moving mesh technique is used to generate the nonnested adaptive mesh for the nonnested augmented subspace method according to the singularity of the approximate wavefunctions. The modified Hessian matrix of the approximate wavefunctions is used as the metric matrix to redistribute the mesh. Through the moving mesh adaptive technique, the redistributed mesh is almost optimal. A number of numerical experiments are carried out to verify the efficiency and the accuracy of the proposed algorithm.",30,4,2024
Transition Rate Scheduling for Quantization-Aware Training,"Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations. It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers. We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT. Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states. This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions. It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually. We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels. Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly. Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT. Experimental results demonstrate the effectiveness of our approach on standard benchmarks.",30,4,2024
Improved AutoEncoder with LSTM module and KL divergence,"The task of anomaly detection is to separate anomalous data from normal data in the dataset. Models such as deep convolutional autoencoder (CAE) network and deep supporting vector data description (SVDD) model have been universally employed and have demonstrated significant success in detecting anomalies. However, the over-reconstruction ability of CAE network for anomalous data can easily lead to high false negative rate in detecting anomalous data. On the other hand, the deep SVDD model has the drawback of feature collapse, which leads to a decrease of detection accuracy for anomalies. To address these problems, we propose the Improved AutoEncoder with LSTM module and Kullback-Leibler divergence (IAE-LSTM-KL) model in this paper. An LSTM network is added after the encoder to memorize feature representations of normal data. In the meanwhile, the phenomenon of feature collapse can also be mitigated by penalizing the featured input to SVDD module via KL divergence. The efficacy of the IAE-LSTM-KL model is validated through experiments on both synthetic and real-world datasets. Experimental results show that IAE-LSTM-KL model yields higher detection accuracy for anomalies. In addition, it is also found that the IAE-LSTM-KL model demonstrates enhanced robustness to contaminated outliers in the dataset.",30,4,2024
Logistic Map Pseudo Random Number Generator in FPGA,"This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.",30,4,2024
HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning,"Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. \href{this https URL}{Code}.",30,4,2024
A University Framework for the Responsible use of Generative AI in Research,"Generative Artificial Intelligence (generative AI) poses both opportunities and risks for the integrity of research. Universities must guide researchers in using generative AI responsibly, and in navigating a complex regulatory landscape subject to rapid change. By drawing on the experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of generative AI. We provide guidance to help distil the diverse regulatory environment into a principles-based position statement. Further, we explain how a position statement can then serve as a foundation for initiatives in training, communications, infrastructure, and process change. Despite the growing body of literature about AI's impact on academic integrity for undergraduate students, there has been comparatively little attention on the impacts of generative AI for research integrity, and the vital role of institutions in helping to address those challenges. This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework for so doing.",30,4,2024
Co-occurrence order-preserving pattern mining,"Recently, order-preserving pattern (OPP) mining has been proposed to discover some patterns, which can be seen as trend changes in time series. Although existing OPP mining algorithms have achieved satisfactory performance, they discover all frequent patterns. However, in some cases, users focus on a particular trend and its associated trends. To efficiently discover trend information related to a specific prefix pattern, this paper addresses the issue of co-occurrence OPP mining (COP) and proposes an algorithm named COP-Miner to discover COPs from historical time series. COP-Miner consists of three parts: extracting keypoints, preparation stage, and iteratively calculating supports and mining frequent COPs. Extracting keypoints is used to obtain local extreme points of patterns and time series. The preparation stage is designed to prepare for the first round of mining, which contains four steps: obtaining the suffix OPP of the keypoint sub-time series, calculating the occurrences of the suffix OPP, verifying the occurrences of the keypoint sub-time series, and calculating the occurrences of all fusion patterns of the keypoint sub-time series. To further improve the efficiency of support calculation, we propose a support calculation method with an ending strategy that uses the occurrences of prefix and suffix patterns to calculate the occurrences of superpatterns. Experimental results indicate that COP-Miner outperforms the other competing algorithms in running time and scalability. Moreover, COPs with keypoint alignment yield better prediction performance.",30,4,2024
Joint Pricing and Matching for Resource Allocation Platforms via Min-cost Flow Problem,"Stochastic matching is the stochastic version of the well-known matching problem, which consists in maximizing the rewards of a matching under a set of probability distributions associated with the nodes and edges. In most stochastic matching problems, the probability distributions inherent in the nodes and edges are set a priori and are not controllable. However, many resource allocation platforms can control the probability distributions by changing prices. For example, a rideshare platform can control the distribution of the number of requesters by setting the fare to maximize the reward of a taxi-requester matching. Although several methods for optimizing price have been developed, optimizations in consideration of the matching problem are still in its infancy. In this paper, we tackle the problem of optimizing price in the consideration of the resulting bipartite graph matching, given the effect of the price on the probabilistic uncertainty in the graph. Even though our problem involves hard to evaluate objective values and is non-convex, we construct a (1-1/e)-approximation algorithm under the assumption that a convex min-cost flow problem can be solved exactly.",30,4,2024
Surface energy and elementary excitations of the XYZ spin chain with integrable open boundary fields,"We study the thermodynamic limit of the anisotropic XYZ spin chain with non-diagonal integrable open boundary conditions. Although the $U(1)$-symmetry is broken, by using the new parametrization scheme, we exactly obtain the surface energy and the excitation energy of the system, which has solved the difficulty in the inhomogeneous $T-Q$ relation. With the boundary parameters in the regions making the Hamiltonian Hermitian, we have obtained the distribution patterns of the zero roots of the eigenvalue of the transfer matrix for the ground state and the excited ones. We find that the surface and excitation energies depend on the parities of sites number $N$, due to the long-range Neel order in the bulk. The spontaneous magnetization and easy-axis for all the regions of boundary parameters are studied. We also obtain the physical quantities in the thermodynamic limit of boundary XXZ model by taking the triangular limit.",30,4,2024
Central elements of the degenerate quantum general linear group,"We construct central elements of the degenerate quantum general linear group introduced by Cheng, Wang and Zhang. In particular, we give an explicit formula for the quantum Casimir element. Our method is based on the explicit $L$ operators. Moreover, we construct a universal $L$ operator, which is a spectral parameter-dependent solution of the quantum Yang-Baxter equation in the tensor product of the degenerate quantum general linear group and the endomorphism ring of its natural representation. This construction leads to the FRT approach to the degenerate quantum general linear group.",30,4,2024
Pilot Contamination in Massive MIMO Systems: Challenges and Future Prospects,"Massive multiple input multiple output (M-MIMO) technology plays a pivotal role in fifth-generation (5G) and beyond communication systems, offering a wide range of benefits, from increased spectral efficiency (SE) to enhanced energy efficiency and higher reliability. However, these advantages are contingent upon precise channel state information (CSI) availability at the base station (BS). Ensuring precise CSI is challenging due to the constrained size of the coherence interval and the resulting limitations on pilot sequence length. Therefore, reusing pilot sequences in adjacent cells introduces pilot contamination, hindering SE enhancement. This paper reviews recent advancements and addresses research challenges in mitigating pilot contamination and improving channel estimation, categorizing the existing research into three broader categories: pilot assignment schemes, advanced signal processing methods, and advanced channel estimation techniques. Salient representative pilot mitigation/assignment techniques are analyzed and compared in each category. Lastly, possible future research directions are discussed.",30,4,2024
Origin of Superconductivity in Rhombohedral Trilayer Graphene: Quasiparticle Pairing within the Inter-Valley Coherent Phase,"Superconductivity (SC) is observed in rhombohedral trilayer graphene (RTG) with an extremely low charge carrier densities, between the normal metal state and a probable inter-valley coherent (IVC) state. The measured coherence length in RTG is roughly two orders of magnitude shorter than the value predicted by a conventional BCS theory. To resolve these discrepancies, we propose that the RTG SC phase arises from the pairing of quasiparticles within the IVC phase. We illustrate the SC behavior using gapped Dirac cones with the chemical potential close to the conduction band's bottom and establish the Ginzburg-Landau theory. Our findings indicate that the mean-field transition temperature, $T_\mathrm{MF}$, is primarily determined by the density of states of quasiparticles within the IVC phase, and is more suppressed in comparison with the BCS relation. Interestingly, we discover that the coherence length behaves according to $\xi\sim 1/\sqrt{T_\mathrm{MF}}$. When the chemical potential lies within the band gap, the conventional coherence length becomes small, while the quantum metric contribution can diverge. Applying our approach to a microscopic model for RTG, our predictions align well with experimental data and effectively capture key measurable features of quantities such as the mean-field transition temperature $T_\mathrm{MF}$ and the coherence length $\xi$. Furthermore, we reveal that the quantum metric contribution, stemming from the multi-band properties of the IVC quasiparticles, plays a significant role in determining the coherence length around the transition regime from superconductivity to IVC. Lastly, experimental predictions are discussed on bebaviors of the upper critical field and coherence length at low temperatures.",30,4,2024
On the Effect of Bounded Rationality in Electricity Markets,"Nash equilibrium is a common solution concept that captures the strategic interaction in electricity market analysis. However, it requires a fundamental but impractical assumption that all market participants are fully rational, which implies unlimited computational resources and cognitive abilities. To tackle the limitation, level-k reasoning is proposed and studied to model the bounded rational behaviors. In this paper, we consider a Cournot competition in electricity markets with two suppliers both following level-k reasoning. One is a self-interested firm and the other serves as a benevolent social planner. First, we observe that the optimal strategy of the social planner is to be of a particular rationality level. Being less or more rational may both result in reduced social welfare. Then, we investigate the effect of bounded rationality on social welfare performance and find that it could largely deviate from that at the Nash equilibrium point. Finally, we characterize optimal, mean maximizing and max-min strategies for the benevolent social planner, when having access to different information. The numerical experiments further demonstrate and validate our findings.",30,4,2024
Computational Approaches of Modelling Human Papillomavirus Transmission and Prevention Strategies: A Systematic Review,"Human papillomavirus (HPV) infection is the most common sexually transmitted infection in the world. Persistent oncogenic Human papillomavirus infection has been a leading threat to global health and can lead to serious complications such as cervical cancer. Prevention interventions including vaccination and screening have been proved effective in reducing the risk of HPV-related diseases. In recent decades, computational epidemiology has been serving as a very useful tool to study HPV transmission dynamics and evaluation of prevention strategies. In this paper, we conduct a comprehensive literature review on state-of-the-art computational epidemic models for HPV disease dynamics, transmission dynamics, as well as prevention efforts. We summarise current research trends, identify gaps in the present literature, and identify future research directions with potential in accelerating the containment and/or elimination of HPV infection.",30,4,2024
Multi-hop Question Answering over Knowledge Graphs using Large Language Models,"Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes. Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially. In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops. We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window. We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance.",30,4,2024
Avoiding short progressions in Euclidean Ramsey theory,"We provide a general framework to construct colorings avoiding short monochromatic arithmetic progressions in Euclidean Ramsey theory. Specifically, if $\ell_m$ denotes $m$ collinear points with consecutive points of distance one apart, we say that $\mathbb{E}^n \not \to (\ell_r,\ell_s)$ if there is a red/blue coloring of $n$-dimensional Euclidean space that avoids red congruent copies of $\ell_r$ and blue congruent copies of $\ell_s$. We show that $\mathbb{E}^n \not \to (\ell_3, \ell_{20})$, improving the best-known result $\mathbb{E}^n \not \to (\ell_3, \ell_{1177})$ by Führer and Tóth, and also establish $\mathbb{E}^n \not \to (\ell_4, \ell_{18})$ and $\mathbb{E}^n \not \to (\ell_5, \ell_{10})$ in the spirit of the classical result $\mathbb{E}^n \not \to (\ell_6, \ell_{6})$ due to Erd{ő}s et. al. We also show a number of similar $3$-coloring results, as well as $\mathbb{E}^n \not \to (\ell_3, \alpha\ell_{6889})$, where $\alpha$ is an arbitrary positive real number. This final result answers a question of Führer and Tóth in the positive.",30,4,2024
Evolution of static to dynamic mechanical behavior in topological nonreciprocal robotic metamaterials,"Based on the Maxwell-Beatty reciprocity theorem, static non-reciprocity has been realized by using nonlinearity, but this non-reciprocity has strict restrictions on input amplitude and structure size(number of units). Here, we design a robotic metamaterial with two components of displacement and rotation, which uses active control to add external forces on the units to break reciprocity at the level of the interactions between the units. We show analytically and simulatively that breaking reciprocity at the level of the interactions directly leads to a strong asymmetric response of displacement in a static system, this displacement-specific characteristic not only has no restrictions on size, input amplitude, and suitable geometric asymmetry, but also can be transferred to rotation by coupling under large deformation. After the evolution from statics to dynamics, asymmetric transmission and unidirectional amplification of vector solitons are both implemented in this system. Our research uncovers the evolution of static non-reciprocity to dynamic non-reciprocity while building a bridge between non-reciprocity physics and soliton science.",30,4,2024
Deep Lead Optimization: Leveraging Generative AI for Structural Modification,"The idea of using deep-learning-based molecular generation to accelerate discovery of drug candidates has attracted extraordinary attention, and many deep generative models have been developed for automated drug design, termed molecular generation. In general, molecular generation encompasses two main strategies: de novo design, which generates novel molecular structures from scratch, and lead optimization, which refines existing molecules into drug candidates. Among them, lead optimization plays an important role in real-world drug design. For example, it can enable the development of me-better drugs that are chemically distinct yet more effective than the original drugs. It can also facilitate fragment-based drug design, transforming virtual-screened small ligands with low affinity into first-in-class medicines. Despite its importance, automated lead optimization remains underexplored compared to the well-established de novo generative models, due to its reliance on complex biological and chemical knowledge. To bridge this gap, we conduct a systematic review of traditional computational methods for lead optimization, organizing these strategies into four principal sub-tasks with defined inputs and outputs. This review delves into the basic concepts, goals, conventional CADD techniques, and recent advancements in AIDD. Additionally, we introduce a unified perspective based on constrained subgraph generation to harmonize the methodologies of de novo design and lead optimization. Through this lens, de novo design can incorporate strategies from lead optimization to address the challenge of generating hard-to-synthesize molecules; inversely, lead optimization can benefit from the innovations in de novo design by approaching it as a task of generating molecules conditioned on certain substructures.",30,4,2024
On the Hodge Structures of Global Smoothings of Normal Crossing Varieties,"Let $f:X \rightarrow \Delta $ be a one-parameter semistable degeneration of $m$-dimensional compact complex manifolds. Assume that each component of the central fiber $X_0$ is Kähler. Then, we provide a criterion for a general fiber to satisfy the $\partial\overline{\partial}$-lemma and a formula to compute the Hodge index on the middle cohomology of the general fiber in terms of the topological conditions/invariants on the central fiber.We apply our theorem to several examples, including the global smoothing of $m$-fold ODPs, Hashimoto-Sano's non-Kähler Calabi-Yau threefolds, and Sano's non-Kähler Calabi-Yau $m$-folds.To deal with the last example, we also prove a Lefschetz-type theorem for the cohomology of the fiber product of two Lefschetz fibrations over $\mathbb{P}^1$ with disjoint critical locus.",30,4,2024
Understanding Multimodal Contrastive Learning Through Pointwise Mutual Information,"Multimodal representation learning to integrate different modalities, such as text, vision, and audio is important for real-world applications. The symmetric InfoNCE loss proposed in CLIP is a key concept in multimodal representation learning. In this work, we provide a theoretical understanding of the symmetric InfoNCE loss through the lens of the pointwise mutual information and show that encoders that achieve the optimal similarity in the pretraining provide a good representation for downstream classification tasks under mild assumptions. Based on our theoretical results, we also propose a new similarity metric for multimodal contrastive learning by utilizing a nonlinear kernel to enrich the capability. To verify the effectiveness of the proposed method, we demonstrate pretraining of multimodal representation models on the Conceptual Caption datasets and evaluate zero-shot classification and linear classification on common benchmark datasets.",30,4,2024
A Survey of Deep Learning Based Software Refactoring,"Refactoring is one of the most important activities in software engineering which is used to improve the quality of a software system. With the advancement of deep learning techniques, researchers are attempting to apply deep learning techniques to software refactoring. Consequently, dozens of deep learning-based refactoring approaches have been proposed. However, there is a lack of comprehensive reviews on such works as well as a taxonomy for deep learning-based refactoring. To this end, in this paper, we present a survey on deep learning-based software refactoring. We classify related works into five categories according to the major tasks they cover. Among these categories, we further present key aspects (i.e., code smell types, refactoring types, training strategies, and evaluation) to give insight into the details of the technologies that have supported refactoring through deep learning. The classification indicates that there is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep learning techniques have been used for the detection of code smells and the recommendation of refactoring solutions as found in 56.25\% and 33.33\% of the literature respectively. In contrast, only 6.25\% and 4.17\% were towards the end-to-end code transformation as refactoring and the mining of refactorings, respectively. Notably, we found no literature representation for the quality assurance for refactoring. We also observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method level whereas classes and variables attracted minimal attention. Finally, we discuss the challenges and limitations associated with the employment of deep learning-based refactorings and present some potential research opportunities for future work.",30,4,2024
Electromagnetic field and chaotic charged-particle motion around hairy black holes in Horndeski gravity,"The Wald vector potential is an exact solution of the source-less Maxwell equations regarding an electromagnetic field of a vacuum uncharged black hole like the Kerr background black hole in an asymptotically uniform magnetic field. However, it is not if the black hole is a nonvacuum solution in a theory of modified gravity with extra fields or a charged Kerr-Newman spacetime. To satisfy the source-less Maxwell equations in this case, the Wald vector potential must be modified and generalized appropriately. Following this idea, we derive an expression for the vector potential of an electromagnetic field surrounding a hairy black hole in the Horndeski modified gravity theory. Explicit symplectic integrators with excellent long-term behaviour are used to simulate the motion of charged particles around the hairy black hole immersed in the external magnetic field. The recurrence plot method based on the recurrence quantification analysis uses diagonal structures parallel to the main diagonal to show regular dynamics, but adopts no diagonal structures to indicate chaotic dynamics. The method is efficient to detect chaos from order in the curved spacetime, as the Poincare map and the fast Lyapunov indicator are.",30,4,2024
Variational approximations of possibilistic inferential models,"Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference. But despite IMs' theoretical/foundational advantages, efficient computation in applications is a major challenge. This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM's possibility contour, or at least its $\alpha$-level set for a specified $\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics.",30,4,2024
Temporal Logic Resilience for Dynamical Systems,"We consider the notion of resilience for cyber-physical systems, that is, the ability of the system to withstand adverse events while maintaining acceptable functionality. We use finite temporal logic to express the requirements on the acceptable functionality and define the resilience metric as the maximum disturbance under which the system satisfies the temporal requirements. We fix a parameterized template for the set of disturbances and form a robust optimization problem under the system dynamics and the temporal specifications to find the maximum value of the parameter. Additionally, we introduce two novel classes of specifications: closed and convex finite temporal logics specifications, offering a comprehensive analysis of the resilience metric within these specific frameworks. From a computational standpoint, we present an exact solution for linear systems and exact-time reachability and finite-horizon safety, complemented by an approximate solution for finite-horizon reachability. Extending our findings to nonlinear systems, we leverage linear approximations and SMT-based approaches to offer viable computational methodologies. The theoretical results are demonstrated on the temperature regulation of buildings, adaptive cruise control and DC motors.",30,4,2024
OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies,"Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing. The difficulties in interpreting and annotating event data limit its scalability. While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve. In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner. We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams. To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization. Experimental results on popular ESS benchmarks showed our approach outperforms existing methods. Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels.",8,5,2024
Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving,"Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.",8,5,2024
Raman-phonon-polariton condensation in a transversely pumped cavity,"Phonon polaritons are hybrid states of light and matter that are typically realised when optically active phonons couple strongly to photons. We suggest a new approach to realising phonon polaritons, by employing a transverse-pumping Raman scheme, as used in experiments on cold atoms in optical cavities. This approach allows hybridisation between an optical cavity mode and any Raman-active phonon mode. Moreover, this approach enables one to tune the effective phonon-photon coupling by changing the strength of the transverse pumping light. We show that such a system may realise a phonon-polariton condensate. To do this, we find the stationary states and use Floquet theory to determine their stability. We thus identify distinct superradiant and lasing states in which the polariton modes are macroscopically populated. We map out the phase diagram of these states as a function of pump frequencies and strengths. Using parameters for transition metal dichalcogenides, we show that realisation of these phases may be practicably obtainable. The ability to manipulate phonon mode frequencies and attain steady-state populations of selected phonon modes provides a new tool for engineering correlated states of electrons.",8,5,2024
THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models,"Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term ""Type I hallucinations"". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term ""Type II hallucinations"". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.",8,5,2024
Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte Carlo,"Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields. A less explored avenue is their application to discriminative tasks involving regression or classification problems. The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions. This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field. The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution. We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image. Finally, we demonstrate that this parameter inference approach is more robust to the addition of noise than baseline parameter inference networks.",8,5,2024
You Only Cache Once: Decoder-Decoder Architectures for Language Models,"We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available atthis https URL.",8,5,2024
Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge,"Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.",8,5,2024
Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models,"Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage:this https URL.",8,5,2024
Radiative corrections to the dynamics of a tracer particle coupled to a Bose scalar field,"We consider a tracer particle coupled to a Bose scalar field and study the regime where the field's propagation speed approaches infinity. For initial states devoid of field excitations, we introduce an effective approximation of the time-evolved wave function and prove its validity in Hilbert space norm. In this approximation, the field remains in the vacuum state while the tracer particle propagates with a modified dispersion relation. Physically, the new dispersion relation can be understood as the effect of radiative corrections due to interactions with virtual bosons. Mathematically, it is defined as the solution of a self-consistent equation, whose form depends on the relevant time scale.",8,5,2024
DanceCam: atmospheric turbulence mitigation in wide-field astronomical images with short-exposure video streams,"We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging. Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure ($\sim$0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise. We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of seeing conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack. This approach results in a marked improvement in angular resolution without compromising the astrometric stability of the final image.",8,5,2024
"Effective correlation and decorrelation for newforms, and weak subconvexity for $L$-functions","Let $f$ and $g$ be spectrally normalized holomorphic newforms of even weight $k \geq2$ on $\Gamma_0(q)$. If $f\neq g$, then assume that $q$ is squarefree. For a nice test function $\psi$ supported on $\Gamma_0(1)\backslash\mathbb{H}$, we establish the best known bounds (uniform in $k$, $q$, and $\psi$) for \[ \int_{\Gamma_0(q)\backslash\mathbb{H}}\psi(z)f(z)\overline{g(z)}y^{k}\frac{dxdy}{y^2}-\mathbf{1}_{f = g}\frac{3}{\pi}\int_{\Gamma_0(1)\backslash\mathbb{H}}\psi(z)\frac{dx dy}{y^2}.\] When $f=g$, our results yield an effective holomorphic variant of quantum unique ergodicity, refining work of Holowinsky-Soundararajan and Nelson-Pitale-Saha. When $f \neq g$, our results extend and improve the effective decorrelation result of Huang for $q=1$. To prove our results, we refine Soundararajan's weak subconvexity bound for Rankin-Selberg $L$-functions.",8,5,2024
LLMs with Personalities in Multi-issue Negotiation Games,"Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be ""jail broken"" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.",8,5,2024
Neutrinos and gamma rays from beta decays in an active galactic nucleus NGC 1068 jet,"We show that TeV neutrinos detected from the nearby active galaxy NGC 1068 can be explained by the beta decays of neutrons produced in photodisintegration of nuclei on ultraviolet photons in the jet. The photodisintergation of nuclei occurs at energies above a few PeV, which explains the 1-100 TeV energies of the observed neutrinos. The gamma-ray flux accompanying the beta decays is expected to be much lower than the neutrino flux, and it agrees well with the gamma-ray observations of NGC 1068. This scenario can be applicable to other jetted Seyfert galaxies such as NGC 4151. The flavor ratio studies could be a test of this beta decay jet scenario for gamma-ray deficit neutrino sources.",8,5,2024
Semi-infinite particle systems with exclusion interaction and heterogeneous jump rates,"We study semi-infinite particle systems on the one-dimensional integer lattice, where each particle performs a continuous-time nearest-neighbour random walk, with jump rates intrinsic to each particle, subject to an exclusion interaction which suppresses jumps that would lead to more than one particle occupying any site. Under appropriate hypotheses on the jump rates (uniformly bounded rates is sufficient) and started from an initial condition that is a finite perturbation of the close-packed configuration, we give conditions under which the particles evolve as a single, semi-infinite ""stable cloud"". More precisely, we show that inter-particle separations converge to a product-geometric stationary distribution, and that the location of every particle obeys a strong law of large numbers with the same characteristic speed.",8,5,2024
Advancing Blockchain Scalability: A Linear Optimization Framework for Diversified Node Allocation in Shards,"Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency. Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing. However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security.This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption. In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics. By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets. Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications. It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations.",8,5,2024
SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan,"The rapid advancement of AI-generated singing voices, which now closely mimic natural human singing and align seamlessly with musical scores, has led to heightened concerns for artists and the music industry. Unlike spoken voice, singing voice presents unique challenges due to its musical nature and the presence of strong background music, making singing voice deepfake detection (SVDD) a specialized field requiring focused attention. To promote SVDD research, we recently proposed the ""SVDD Challenge,"" the very first research challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and deepfake singing voice recordings. The challenge will be held in conjunction with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).",8,5,2024
Deep learning-based variational autoencoder for classification of quantum and classical states of light,"Advancements in optical quantum technologies have been enabled by the generation, manipulation, and characterization of light, with identification based on its photon statistics. However, characterizing light and its sources through single photon measurements often requires efficient detectors and longer measurement times to obtain high-quality photon statistics. Here we introduce a deep learning-based variational autoencoder (VAE) method for classifying single photon added coherent state (SPACS), single photon added thermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of light. Our semisupervised learning-based VAE efficiently maps the photon statistics features of light to a lower dimension, enabling quasi-instantaneous classification with low average photon counts. The proposed VAE method is robust and maintains classification accuracy in the presence of losses inherent in an experiment, such as finite collection efficiency, non-unity quantum efficiency, finite number of detectors, etc. Additionally, leveraging the transfer learning capabilities of VAE enables successful classification of data of any quality using a single trained model. We envision that such a deep learning methodology will enable better classification of quantum light and light sources even in the presence of poor detection quality.",8,5,2024
Quantum Steenrod operations and Fukaya categories,"This paper is concerned with quantum cohomology and Fukaya categories of a closed monotone symplectic manifold $X$, where we use coefficients in a field $\mathbf{k}$ of characteristic $p>0$. The first main result of this paper is that the quantum Steenrod operations $Q\Sigma$ admit an interpretation in terms of the Fukaya category of $X$, via suitable versions of the open-closed maps. Using this, we show that $Q\Sigma$, whose definition is intrinsic to characteristic $p$, is compatible with certain structures inherited from the quantum connection in characteristic $0$. We then turn to applications of these results. The first application is an arithmetic proof of the unramified exponential type conjecture for $X$ that satisfies Abouzaid's generation criterion over $\overline{\mathbb{Q}}$, which uses a reduction mod $p$ argument. Next, we demonstrate how the categorical perspective provides new tools for computing $Q\Sigma$ beyond the reach of known technology. We also explore potential connections of our work to arithmetic homological mirror symmetry.",8,5,2024
BenthicNet: A global compilation of seafloor images for deep learning applications,"Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems. The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information. Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce. Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models. An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images. These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images. A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks. The compilation and model are made openly available for use by the scientific community atthis https URL.",8,5,2024
An LSTM-Based Chord Generation System Using Chroma Histogram Representations,"This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords. Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset. This system is shown to be suitable for limited real-time use. While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships. The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted.",8,5,2024
Cellular Traffic Prediction Using Online Prediction Algorithms,"The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity. However, the efficient management of traffic in 5G networks remains a critical challenge. It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management. This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios. We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm. We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots. Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models. Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load. Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models. Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments.",8,5,2024
Fast Exact/Conservative Monte Carlo Confidence Intervals,"Monte Carlo tests about parameters can be ""inverted"" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\alpha$. When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.",8,5,2024
EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning,"The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination. Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks. EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation. Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field. Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning. The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice. Our codes and models are available at:this https URL.",8,5,2024
Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks,"This paper presents sufficient conditions for the stability and $\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a ""lifted"" representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon.",8,5,2024
RACH Traffic Prediction in Massive Machine Type Communications,"Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks. However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments. Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks. This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks. The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns. Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network. Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios. We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics. Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system.",8,5,2024
Performance Bounds for Velocity Estimation with Large Antenna Arrays,"Joint communication and sensing (JCS) is envisioned as an enabler of future 6G networks. One of the key features of these networks will be the use of extremely large aperture arrays (ELAAs) and high operating frequencies, which will result in significant near-field propagation effects. This unique property can be harnessed to improve sensing capabilities. In this paper, we focus on velocity sensing, as using ELAAs allows the estimation of not just the radial component but also the transverse component. We derive analytical performance bounds for both velocity components, demonstrating how they are affected by the different system parameters and geometries. These insights offer a foundational understanding of how near-field effects play in velocity sensing differently from the far field and from position estimate.",8,5,2024
Classical Grand Angular Momentum in N-Body Problems,"The concept of grand angular momentum is widely used in the study of N-body problems quantum mechanically. Here, we applied it to a classical analysis of N-body problems. Utilizing the tree representation for Jacobi and hyperspherical coordinates, we found a decomposition of its magnitude into magnitudes of one-body angular momenta in 3 dimensions. We generalized some results from the two-body problem. Furthermore, we derive the general expression for the scattering angle for the N-body problem.",8,5,2024
How a table modulates the risk of airborne transmission between facing individuals,"Airborne transmission has been recognized as an important route of transmission for SARS-CoV-2, the virus responsible for the COVID-19 pandemic. While coughing and sneezing are spectacular sources of production of infected aerosols with far-reaching airflows, the prevalence of asymptomatic transmissions highlighted the importance of social activities. Gathering around a table, a common scenario of human interactions, not only fixes a typical distance between static interlocutors, but influences airborne transmission, by serving both as a flow diverter and a surface for droplet deposition. Here, we use high-fidelity large-eddy simulations to characterize short-range airborne transmission when two people face each other at a typical table. We show that compared to the natural distance travelled by free buoyant puffs and jets, the distance between the table and the emission constitutes a new length scale that modifies downward exhaled flows, common during nose breathing, speech, and laughter. When the table is close to the emitter, its main effect is to restrict the forward spread of emitted particles. However, if the distance between individuals is too short, particles reaching the recipient become more concentrated, rising transmission risks. Additionally, simulations of forceful exhalations, like laughter, demonstrate that the table acts as a filter that collects medium-sized particles that would have remained in the free jet otherwise, but can in that case be involved in the fomite transmission route. The table introduces a cut-off size for particles that depends on the inertia of the exhaled material, thereby modifying the size distribution of particles suspended in the air.",8,5,2024
DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN Training,"Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications. To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing. However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions. To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy. The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation. In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification. Besides, \name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations. We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training. The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy.",8,5,2024
On $\operatorname{Alt}(n)$-modules with an additive dimension when $n\le6$,"Working in the general context of ""modules with an additive dimension,"" we complete the determination of the minimal dimension of a faithful Alt(n)-module and classify those modules in three of the exceptional cases: 2-dimensional Alt(5)-modules in characteristic 2, 3-dimensional Alt(5)-modules in characteristic 5, and 3-dimensional Alt(6)-modules in characteristic 3. We also highlight the remaining work needed to complete the classification of the faithful Alt(n)-modules of minimal dimension for all n; these open problems seem well suited as projects for advanced undergraduate or master's students.",8,5,2024
myAURA: Personalized health library for epilepsy management via knowledge graph sparsification and visualization,"Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management.Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records. A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary.Results: The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon. Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media. The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems. Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input.Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy.Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions.",8,5,2024
Generalized vector potential and Trace Theorem for Lipschitz domains,"The vector potential is a fundamental concept widely applied across various fields. This paper presents an existence theorem of a vector potential for divergence-free functions in $W^{m,p}(\mathbb{R}^N,\mathbb{T})$ with general $m,p,N$. Based on this theorem, one can establish the space decomposition theorem for functions in $W^{m,p}_0(\operatorname{curl};\Omega,\mathbb{R}^N)$ and the trace theorem for functions in $W^{m,p}(\Omega)$ within the Lipschitz domain $\Omega \subset \mathbb{R}^N$. The methods of proof employed in this paper are straightforward, natural, and consistent.",8,5,2024
Are Economically Advanced Countries More Efficient in Basic and Applied Research?,"Research and development (R&D) of countries play a major role in a long-term development of the economy. We measure the R&D efficiency of all 28 member countries of the European Union in the years 2008--2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.",8,5,2024
SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants,"In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website:this http URL",8,5,2024
"""Community Guidelines Make this the Best Party on the Internet"": An In-Depth Study of Online Platforms' Content Moderation Policies","Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.",8,5,2024
Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation,"Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation.",8,5,2024
The Harnack inequality fails for nonlocal kinetic equations,"We prove that the Harnack inequality fails for nonlocal kinetic equations. Such equations arise as linearized models for the Boltzmann equation without cutoff and are of hypoelliptic type. We provide a counterexample for the simplest equation in this theory, the fractional Kolmogorov equation. Our result reflects a purely nonlocal phenomenon since the Harnack inequality holds true for local kinetic equations like the Kolmogorov equation.",8,5,2024
Brooks-type colourings of digraphs in linear time,"Brooks' Theorem is a fundamental result on graph colouring, stating that the chromatic number of a graph is almost always upper bounded by its maximal degree. Lovász showed that such a colouring may then be computed in linear time when it exists. Many analogues are known for variants of (di)graph colouring, notably for list-colouring and partitions into subgraphs with prescribed degeneracy. One of the most general results of this kind is due to Borodin, Kostochka, and Toft, when asking for classes of colours to satisfy ""variable degeneracy"" constraints. An extension of this result to digraphs has recently been proposed by Bang-Jensen, Schweser, and Stiebitz, by considering colourings as partitions into ""variable weakly degenerate"" subdigraphs. Unlike earlier variants, there exists no linear-time algorithm to produce colourings for these generalisations.We introduce the notion of (variable) bidegeneracy for digraphs, capturing multiple (di)graph degeneracy variants. We define the corresponding concept of $F$-dicolouring, where $F = (f_1,...,f_s)$ is a vector of functions, and an $F$-dicolouring requires vertices coloured $i$ to induce a ""strictly-$f_i$-bidegenerate"" subdigraph. We prove an analogue of Brooks' theorem for $F$-dicolouring, generalising the result of Bang-Jensen et al., and earlier analogues in turn.Our new approach provides a linear-time algorithm that, given a digraph $D$, either produces an $F$-dicolouring of $D$, or correctly certifies that none exist. This yields the first linear-time algorithms to compute (di)colourings corresponding to the aforementioned generalisations of Brooks' theorem. In turn, it gives an unified framework to compute such colourings for various intermediate generalisations of Brooks' theorem such as list-(di)colouring and partitioning into (variable) degenerate sub(di)graphs.",8,5,2024
XMM-Newton observations of the extragalactic microquasar S26 and their implications for PeV cosmic rays,"The extragalactic microquasar S26 has the most powerful jets observed in accreting binaries, with a kinetic luminosity of $L_{\rm jet}\sim10^{40}\,{\rm erg\,s^{-1}}$. According to the jet-disk symbiosis model, this implies that the accretion power to the stellar black hole at the core of the system should be very super-Eddington, on the order of $L_{\rm acc}\sim L_{\rm jet}$. However, the observed X-ray flux of this system, measured by the \textit{Chandra} and \textit{XMM-Newton} telescopes, indicates an apparent very sub-Eddington accretion luminosity of $L_{\rm X}\approx 10^{37}\,{\rm erg\,s^{-1}}$, orders of magnitude smaller than the jet power. We present here a preliminary investigation of the relationship between jet and disk power, analyze an X-ray observation of S26 obtained with \textit{XMM-Newton}, and propose an explanation for the emission. We also examine the acceleration and distribution of the particles to discuss the feasibility of microquasars as potential PeVatron sources, exploring their ability to produce cosmic rays with energies of about 1 PeV or higher.",8,5,2024
Causal Duration Analysis with Diff-in-Diff,"In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state. For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked. The parallel trends assumption that underpins difference-in-differences generally fails in such settings. We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes. These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests. Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods. We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison.",8,5,2024
Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers,"Large Language Models (LLMs) have profoundly changed the world. Their self-attention mechanism is the key to the success of transformers in LLMs. However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\mathsf{conv}$ basis system, ""similar"" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system. We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices. Thanks to Fast Fourier Transforms (FFT), the attention {\it inference} can be computed in $O(knd \log n)$ time, where $d$ is the hidden dimension. In practice, we have $ d \ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\it training forward} and {\it backward gradient} can be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly computing the $n \times n$ attention matrix, which may largely alleviate the quadratic computational complexity. Furthermore, our algorithm works on any input matrices. This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts.",8,5,2024
Clustering Retail Products Based on Customer Behaviour,"The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.",8,5,2024
Disorder-resilient transport through dopant arrays in silicon,"Chains and arrays of phosphorus donors in silicon have recently been used to demonstrate dopant-based quantum simulators. The dopant disorder present in fabricated devices must be accounted for. Here, we theoretically study transport through disordered donor-based $3\times 3$ arrays that model recent experimental results. We employ a theory that combines the exact diagonalization of an extended Hubbard model of the array with a non-equilibrium Green's function formalism to model transport in interacting systems. We show that current flow through the array and features of measured stability diagrams are highly resilient to disorder. We interpret this as an emergence of uncomplicated behavior in the multi-electron system dominated by strong correlations, regardless of array filling, where the current follows the shortest paths between source and drain sites that avoid possible obstacles. The reference $3\times 3$ array has transport properties very similar to three parallel 3-site chains coupled only by interchain Coulomb interaction, which indicates a challenge in characterizing such devices.",8,5,2024
FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models,"The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available atthis https URL.",8,5,2024
Restricted Randomized Benchmarking with Universal Gates of Fixed Sequence Length,"The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible. Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation. We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one. This makes our protocol highly resource efficient and practical for small qubit numbers. We exemplify our protocol for creating Haar-randomness in the case of single and two qubits. Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique. We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise.",8,5,2024
SPIDER: Improved Succinct Rank and Select Performance,"Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0 through i, and select(j) gives the first slot s with rank(s) = j. A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector.State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly. Rank queries can be answered using only a handful of array accesses. Select queries can be answered by starting with similar array accesses, followed by a linear scan.Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space).In this paper we make significant progress towards closing this gap. We give a new data structure, SPIDER, which uses 3.82% extra space. SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures. For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space.SPIDER makes two main technical contributions. For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency. For select queries, it uses predictions to almost eliminate the cost of the linear scan. These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting. Our results hold on both real and synthetic data, showing that these predictions are effective in practice.",8,5,2024
Exponential time propagators for elastodynamics,"We propose a computationally efficient and systematically convergent approach for elastodynamics simulations. We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion. With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector. We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector. In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size. We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively. We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems. For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\beta$ method. Computationally, it translates to a $\sim$$1000\times$ and $\sim$$10-100\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively.",8,5,2024
The Ghent Hybrid Model in NuWro: a new neutrino single-pion production model in the GeV regime,"Neutrino-induced single-pion production constitutes an essential interaction channel in modern neutrino oscillation experiments, with its products building up a significant fraction of the observable hadronic final states. Frameworks of oscillation analyses strongly rely on Monte Carlo neutrino event generators, which provide theoretical predictions of neutrino interactions on nuclear targets. Thus, it is crucial to integrate state-of-the-art single-pion production models with Monte Carlo simulations to prepare for the upcoming systematics-dominated landscape of neutrino measurements. In this work, we present the implementation of the Ghent Hybrid model for neutrino-induced single-pion production in the NuWro Monte Carlo event generator. The interaction dynamics includes coherently-added contributions from nucleon resonances and a non-resonant background, merged into the pythia branching predictions in the deep-inelastic regime, as instrumented by NuWro. This neutrino-nucleon interaction model is fully incorporated into the nuclear framework of the generator, allowing it to account for the influence of both initial- and final-state nuclear medium effects. We compare the predictions of this integrated implementation with recent pion production data from accelerator-based neutrino experiments. The results of the novel model show improved agreement of the generator predictions with the data and point to the significance of the refined treatment of the description of pion-production processes beyond the $\Delta$ region.",8,5,2024
Broadcast Channel Synthesis from Shared Randomness,"We study the problem of synthesising a two-user broadcast channel using a common message, where each output terminal shares an independent source of randomness with the input terminal. This generalises two problems studied in the literature (Cuff, IEEE Trans. Inform. Theory, 2013; Kurrithis http URL., IEEE Trans. Inform. Theory, 2021). We give an inner bound on the tradeoff region between the rates of communication and shared randomness, and a lower bound on the minimum communication rate. Although the bounds presented here are not tight in general, they are tight for some special cases, including the aforementioned problems.",8,5,2024
MOTLEE: Collaborative Multi-Object Tracking Using Temporal Consistency for Neighboring Robot Frame Alignment,"Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment. Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time. To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift. We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects. To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses. We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration. The code and hardware dataset are available atthis https URL.",8,5,2024
Gamification in Software Engineering Education: a Tertiary Study,"As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements. This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education. The study was conducted in response to recent systematic literature reviews and mappings on the topic. The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements. Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content. The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices. However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation. (English Version of the paper in Portuguese available here:this http URL",8,5,2024
Why the Universal Threshold for Primordial Black Hole Formation is Universal,We show why the threshold for primordial black hole formation is universal (independent from the shape of the perturbation) when expressed in terms of the volume averaged compaction function. The proof is rooted in the self-similarity of the gravitational collapse phenomenon at criticality.,8,5,2024
"In-depth analysis of LISA Pathfinder performance results: time evolution, noise projection, physical models, and implications for LISA","We present an analysis of the LISA Pathfinder differential acceleration performance over the entire mission . We show that the Brownian noise level, detected for frequencies $f\gtrsim \SI{1}{mHz}$, has been evolving consistently with the outgassing of a single gaseous species, with an activation temperature of $(7.0\pm 0.2)\,\text{kK}$. In excess to the Brownian noise, the acceleration amplitude spectral density (ASD) always shows a sub-mHz tail which is reasonably well fit, between $f=\SI{36}{\micro\hertz}$ and $\SI{1}{\milli\hertz}$, to $\widetilde{S}_{\Delta g}^{1/2}(1\, \text{mHz}/f)$. A Bayesian estimate of $\widetilde{S}_{\Delta g}^{1/2}$ on a partition of the entire set of measurements in 27 data stretches, each 2.75\,d long, gives $\widetilde{S}_{\Delta g}^{1/2}=(1.1\pm0.3)\,\si{\femto\meter\,\second^{-2}/\rtHz}$, with no particular time pattern over the course of the mission. The width the posterior contains, in excess of the statistical uncertainty, a true physical fluctuation of $\widetilde{S}_{\Delta g}^{1/2}$ from run to run, of about $\SI{0.2}{\femto\meter\,\second^{-2}/\rtHz}$, with no correlation with specific operating conditions. At the lowest considered frequency of $f=\SI{18}{\micro\hertz}$, the ASD significantly deviates from the $1/f$ behavior, because of temperature fluctuations that appear to modulate a quasi-static pressure gradient, sustained by the asymmetries of outgassing . We also present a projection of acceleration noise on the sources for which we had either a correlation measurement, or an estimate from dedicated experiments.These sources account for about 40\% of the noise power the $1/f$ tail. We discuss the possible sources of the unaccounted-for fraction, present a series of analyses that rule many of them out, and identify the possible measures that may be taken to keep the remaining ones under control in LISA.",8,5,2024
Anomaly Detection in Certificate Transparency Logs,"We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest. This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance. The technique is validated on a sample of certificates from Certificate Transparency logs.",8,5,2024
Hybrid Quantum Graph Neural Network for Molecular Property Prediction,"To accelerate the process of materials design, materials science has increasingly used data driven techniques to extract information from collected data. Specially, machine learning (ML) algorithms, which span the ML discipline, have demonstrated ability to predict various properties of materials with the level of accuracy similar to explicit calculation of quantum mechanical theories, but with significantly reduced run time and computational resources. Within ML, graph neural networks have emerged as an important algorithm within the field of machine learning, since they are capable of predicting accurately a wide range of important physical, chemical and electronic properties due to their higher learning ability based on the graph representation of material and molecular descriptors through the aggregation of information embedded within the graph. In parallel with the development of state of the art classical machine learning applications, the fusion of quantum computing and machine learning have created a new paradigm where classical machine learning model can be augmented with quantum layers which are able to encode high dimensional data more efficiently. Leveraging the structure of existing algorithms, we developed a unique and novel gradient free hybrid quantum classical convoluted graph neural network (HyQCGNN) to predict formation energies of perovskite materials. The performance of our hybrid statistical model is competitive with the results obtained purely from a classical convoluted graph neural network, and other classical machine learning algorithms, such as XGBoost. Consequently, our study suggests a new pathway to explore how quantum feature encoding and parametric quantum circuits can yield drastic improvements of complex ML algorithm like graph neural network.",8,5,2024
CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation,"Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems.",8,5,2024
A multiple coupon collection process and its Markov embedding structure,"The embedding problem of Markov transition matrices into Markov semigroups is a classic problem that regained a lot of impetus and activities in recent years. We consider it here for the following generalisation of the well-known coupon collection process: from a finite set of distinct objects, a subset is drawn repeatedly according to some probability distribution, independently and with replacement, and each time united with the set of objects sampled so far. We derive and interpret properties and explicit conditions for the resulting discrete-time Markov chain to be representable within a semigroup or a flow of a continuous-time process of the same type.",8,5,2024
Guided Combinatorial Algorithms for Submodular Maximization,"For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \citep{buchbinder2023constrained}. These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function. However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \approx 0.367$ \citep{buchbinder2014submodular}. In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$.",8,5,2024
Multimode amplitude squeezing through cascaded nonlinear optical processes,"Multimode squeezed light is enticing for several applications, from squeezed frequency combs for spectroscopy to signal multiplexing in optical computing. To generate squeezing in multiple frequency modes, optical parametric oscillators have been vital in realizing multimode squeezed vacuum states through second-order nonlinear processes. However, most work has focused on generating multimode squeezed vacua and squeezing in mode superpositions (supermodes). Bright squeezing in multiple discrete frequency modes, if realized, could unlock novel applications in quantum-enhanced spectroscopy and optical quantum computing. Here, we show how $Q$ factor engineering of a multimode nonlinear cavity with cascaded three wave mixing processes creates strong, spectrally tunable single mode output amplitude noise squeezing over 10 dB below the shot noise limit. In addition, we demonstrate squeezing for multiple discrete frequency modes above threshold. This bright squeezing arises from enhancement of the (noiseless) nonlinear rate relative to decay rates in the system due to the cascaded generation of photons in a single idler ""bath"" mode. A natural consequence of the strong nonlinear coupling in our system is the creation of an effective cavity in the synthetic frequency dimension that sustains Bloch oscillations in the modal energy distribution. Bloch mode engineering could provide an opportunity to better control nonlinear energy flow in the synthetic frequency dimension, with exciting applications in quantum random walks and topological photonics. Lastly, we show evidence of long-range correlations in amplitude noise between discrete frequency modes, pointing towards the potential of long-range entanglement in a synthetic frequency dimension.",8,5,2024
Graded Relevance Scoring of Written Essays with Dense Retrieval,"Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students. While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits. In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay. We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders. Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels. We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay. As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models. We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset. Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario. We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness.",8,5,2024
Extending the Torelli map to alternative compactifications of the moduli space of curves,"Determining the limiting behaviour of the Jacobian as the underlying curve degenerates has been the subject of much interest. For nodal singularities, there are beautiful constructions of Caporaso as well as Pandharipande of compactified universal Jacobians over the moduli space of stable curves. Alexeev later obtained a canonical such compactification by extending the Torelli map out of the Deligne-Mumford compactification of $\mathcal{M}_{g,n}$. In contrast, Alexeev and Brunyate proved that the Torelli map does not extend over the cuspidal locus in Schubert's alternative compactification of pseudostable curves.In this paper, we consider curves with singularities that locally look like the axes in $m$-space, which we call fold-like singularities. We construct an alternative compactification of $\mathcal{M}_{g,n}$ consisting of curves with such singularities and prove that the Torelli map extends out of this compactification. Furthermore, for every alternative compactification in the sense of Smyth, we identify a fold-like locus over which the Torelli map extends.",8,5,2024
Automation of MKID Simulations for Array Building with AEM (Automated Electromagnetic MKID Simulations),"Microwave Kinetic Inductance Detectors (MKIDs) are photon detectors comprised of superconducting LC resonators with unique resonant frequencies corresponding to their geometrical structure. As each pixel has its own geometry, electromagnetic simulations by hand of every pixel in a kilo-pixel array are impractical. Simulating fewer pixels and interpolating in between risks reduced pixel yield in arrays due to overlapping resonant frequencies. We introduce a new software called AEM (Automated Electromagnetic MKID simulations) that automates the constructions and simulations of every simulated MKID pixel in an array according to specified resonant frequencies and a Qc range. We show automated designs to have an increased pixel yield (avoiding loses due to interpolation completely), increased accuracy in resonance frequency and Qc values when compared to interpolated structures. We also demonstrate a simulated trial of AEM for 100 MKIDs between 4 & 8 GHz to produce MKIDs with accuracies of +-0.2 MHz with a runtime of 10 hrs 45 mins.",8,5,2024
Agent-Constrained Truthful Two-Facility Location Games,"We consider a truthful two-facility location problem in which there is set of agents with private locations on the line of real numbers, and the goal is to place two facilities at different locations chosen from the set of those reported by the agents. Given a feasible solution, each agent suffers an individual cost which is either its total distance to both facilities (sum-variant) or its distance to the farthest facility (max-variant). For both variants, we show tight bounds on the approximation ratio of deterministic and randomized mechanisms in terms of the social cost, the total individual cost of the agents.",8,5,2024
SINBAD: Saliency-informed detection of breakage caused by ad blocking,"Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality. Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users. We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules. The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules.",8,5,2024
Trail Trap: a variant of Partizan Edge Geography,"We introduce a two-player game played on undirected graphs called Trail Trap, which is a variant of a game known as Partizan Edge Geography. One player starts by choosing any edge and moving a token from one endpoint to the other; the other player then chooses a different edge and does the same. Alternating turns, each player moves their token along an unused edge from its current vertex to an adjacent vertex, until one player cannot move and loses. We present an algorithm to determine which player has a winning strategy when the graph is a tree, and partially characterize the trees on which a given player wins. Additionally, we show that Trail Trap is NP-hard, even for connected bipartite planar graphs with maximum degree $4$ as well as for disconnected graphs. We determine which player has a winning strategy for certain subclasses of complete bipartite graphs and grid graphs, and we propose several open problems for further study.",8,5,2024
Existence and dynamics of normalized solutions to Schrödinger equations with generic double-behaviour nonlinearities,"We study the existence of solutions $(\underline u,\lambda_{\underline u})\in H^1(\mathbb{R}^N; \mathbb{R}) \times \mathbb{R}$ to \[ -\Delta u + \lambda u = f(u) \quad \text{in } \mathbb{R}^N \] with $N \ge 3$ and prescribed $L^2$ norm, and the dynamics of the solutions to \[ \begin{cases} \mathrm{i} \partial_t \Psi + \Delta \Psi = f(\Psi)\\ \Psi(\cdot,0) = \psi_0 \in H^1(\mathbb{R}^N; \mathbb{C}) \end{cases} \] with $\psi_0$ close to $\underline u$. Here, the nonlinear term $f$ has mass-subcritical growth at the origin, mass-supercritical growth at infinity, and is more general than the sum of two powers. Under different assumptions, we prove the existence of a locally least-energy solution, the orbital stability of all such solutions, the existence of a second solution with higher energy, and the strong instability of such a solution.",8,5,2024
Systematic Use of Random Self-Reducibility against Physical Attacks,"This work presents a novel, black-box software-based countermeasure against physical attacks including power side-channel and fault-injection attacks. The approach uses the concept of random self-reducibility and self-correctness to add randomness and redundancy in the execution for protection. Our approach is at the operation level, is not algorithm-specific, and thus, can be applied for protecting a wide range of algorithms. The countermeasure is empirically evaluated against attacks over operations like modular exponentiation, modular multiplication, polynomial multiplication, and number theoretic transforms. An end-to-end implementation of this countermeasure is demonstrated for RSA-CRT signature algorithm and Kyber Key Generation public key cryptosystems. The countermeasure reduced the power side-channel leakage by two orders of magnitude, to an acceptably secure level in TVLA analysis. For fault injection, the countermeasure reduces the number of faults to 95.4% in average.",8,5,2024
Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs with infinite activity,"In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity. We provide a full error analysis of our so-called random deep splitting method. In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration. Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk. In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds.",8,5,2024
"Bell's and Mermin's inequalities, entangled coherent states and unitary operators","We elaborate on the recent proposal of employing unitary operators in Quantum Mechanics. The Bell and Mermin inequalities for entangled coherent states are scrutinized by making use of the unitary displacement operators. A violation of the Mermin inequality close to the maximum allowed value is reported, in agreement with the existing literature.",8,5,2024
Is Transductive Learning Equivalent to PAC Learning?,"Most work in the area of learning theory has focused on designing effective Probably Approximately Correct (PAC) learners. Recently, other models of learning such as transductive error have seen more scrutiny. We move toward showing that these problems are equivalent by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive guarantee by adding a small number of samples to the dataset. We first rederive the result of Aden-Ali et al.arXiv:2304.09167reducing PAC learning to transductive learning in the realizable setting using simpler techniques and at more generality as background for our main positive result. Our agnostic transductive to PAC conversion technique extends the aforementioned argument to the agnostic case, showing that an agnostic transductive learner can be efficiently converted to an agnostic PAC learner. Finally, we characterize the performance of the agnostic one inclusion graph algorithm of Asilis et al.arXiv:2309.13692for binary classification, and show that plugging it into our reduction leads to an agnostic PAC learner that is essentially optimal. Our results imply that transductive and PAC learning are essentially equivalent for supervised learning with pseudometric losses in the realizable setting, and for binary classification in the agnostic setting. We conjecture this is true more generally for the agnostic setting.",8,5,2024
MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning,"We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.",8,5,2024
ContEvol formalism: possibly a new twist on computational physics,"We present the ContEvol (continuous evolution) formalism, a family of implicit numerical methods which only need to solve linear equations and are almost symplectic. Combining values and derivatives of functions, ContEvol outputs allow users to recover full history and render full distributions. Using classic harmonic oscillator as a prototype case, we show that ContEvol methods lead to lower-order errors than two commonly used Runge--Kutta methods. Applying first-order ContEvol to simple celestial mechanics problems, we demonstrate that deviation from equation(s) of motion of ContEvol tracks is still $\mathcal{O}(h^5)$ ($h$ is the step length) by our definition. Numerical experiments with an eccentric elliptical orbit indicate that first-order ContEvol is a viable alternative to classic Runge--Kutta or the symplectic leapfrog integrator. Solving stationary Schrödinger equation in quantum mechanics, we manifest ability of ContEvol to handle boundary value or eigenvalue problems. Important directions for future work, including mathematical foundation, higher dimensions, and technical improvements, are discussed at the end of this article.",8,5,2024
A score-based particle method for homogeneous Landau equation,"We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080]. Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density. Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching. The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080]. This streamlines computation and enhances scalability with dimensionality. Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss. Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation. Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction.",8,5,2024
Exploring the limits of the law of mass action in the mean field description of epidemics on Erdös-Rényi networks,"The manner epidemics occurs in a social network depends on various elements, with two of the most influential being the relationships among individuals in the population and the mechanism of transmission. In this paper, we assume that the social network has a homogeneous random topology of Erdös-Rényi type. Regarding the contagion process, we assume that the probability of infection is proportional to the proportion of infected neighbours.We consider a constant population, whose individuals are the nodes of the social network, formed by two variable subpopulations: Susceptible and Infected (SI model). We simulate the epidemics on this random network and study whether the average dynamics can be described using a mean field approach in terms of Differential Equations, employing the law of mass action. We show that a macroscopic description could be applied for low average connectivity, adjusting the value of the contagion rate in a precise function. This dependence is illustrated by calculating the transient times for each connectivity.This study contributes valuable insights into the interplay between network connectivity, contagion dynamics, and the applicability of mean-field approximations. The delineation of critical thresholds and the distinctive behaviour at lower connectivity enable a deeper understanding of epidemic dynamics.",8,5,2024
Machine Learning Assisted Dynamical Classification of Trans-Neptunian Objects,"Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar system. They are observed to have a complex orbital distribution that was shaped by the early dynamical history and migration of the giant planets. Comparisons between the different dynamical classes of modeled and observed TNOs can help constrain the history of the outer solar system. Because of the complex dynamics of TNOs, particularly those in and near mean motion resonances with Neptune, classification has traditionally been done by human inspection of plots of the time evolution of orbital parameters. This is very inefficient. The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to increase the number of known TNOs by a factor of $\sim$10, necessitating a much more automated process. In this chapter we present an improved supervised machine learning classifier for TNOs. Using a large and diverse training set as well as carefully chosen, dynamically motivated data features calculated from numerical integrations of TNO orbits, our classifier returns results that match those of a human classifier 98% of the time, and dynamically relevant classifications 99.7% of the time. This classifier is dramatically more efficient than human classification, and it will improve classification of both observed and modeled TNO data.",8,5,2024
The role of AGN winds in galaxy formation: connecting AGN outflows at low redshifts to the formation/evolution of their host galaxies,"Using SDSS spectra, we applied an automatic method to search for outflows (OFs) in three large samples of narrow-line AGN at low redshifts (z < 0.4), separated in three spectral activity classes: radio-loud RG, 15,793, radio-quiet, Sy2, 18,585, and LINER, 25,656. In general, the probability of detecting an OF decreases along the sequence Sy1->Sy2->LINER/RG and, independently of the AGN class, the wind velocity, traced by W80, increases with the AGN luminosity. Moreover W80 is systematically higher in RG or any of the other AGN class when detected in radio. These results support the idea that there are two main modes of production of OF, the radiative mode dominant in radio-quiet AGN and the jet mode dominant in radio-loud galaxies, although both modes could also happen simultaneously at different levels. From the spectra and SDSS photometry, the characteristics of the AGN host galaxies and their super-massive black holes (SMBHs) were also retrieved using the stellar population synthesis code STARLIGHT. This revealed that, independently of their spectral class, 1) galaxy hosts with OFs have systematically later morphological types and higher star formation rates than their counterpart without OF, 2) they occupy different positions in the specific diagnostic diagram, sSMBH vs. sSFR, which suggests they follow different evolutionary paths congruent with the morphology of their galaxy hosts, and 3) they show no evidence of AGN quenching or triggering of star formation. These results are consistent with a scenario explaining the different AGN classes as consequences of different formation processes of galaxies: early-type galaxies (LINER and RG) formed bigger bulges and more massive SMBHs, exhausting their reservoir of gas more rapidly than late-type galaxies (Sy2 and Sy1), quenching their star formation and starving their SMBHs.",8,5,2024
Exact solution of Dynamical Mean-Field Theory for a linear system with annealed disorder,"We investigate a disordered multi-dimensional linear system in which the interaction parameters vary stochastically in time with defined temporal correlations. We refer to this type of disorder as ""annealed"", in contrast to quenched disorder in which couplings are fixed in time. We extend Dynamical Mean-Field Theory to accommodate annealed disorder and employ it to find the exact solution of the linear model in the limit of a large number of degrees of freedom. Our analysis yields analytical results for the non-stationary auto-correlation, the stationary variance, the power spectral density, and the phase diagram of the model. Interestingly, some unexpected features emerge upon changing the correlation time of the interactions. The stationary variance of the system and the critical variance of the disorder are generally found to be a non-monotonic function of the correlation time of the interactions. We also find that in some cases a re-entrant phase transition takes place when this correlation time is varied.",8,5,2024
Quantum synchronization through the interference blockade,"Synchronization manifests itself in oscillators adjusting their frequencies and phases with respect to an external signal or another oscillator. In the quantum case, new features appear such as destructive interferences that can result in the suppression of phase locking. A three-level (spin-1) oscillator with equal gain and damping rates and subject to an external drive does not exhibit any 1:1 phase locking but 2:1 phase locking, i.e., its phase distribution features two maxima. This bistable locking at two opposite phases is a signature of the quantum interference synchronization blockade. An analogous behavior was found for two identical coupled spin-1 oscillators. In this work, we consider two coupled spin-1 oscillators and a drive applied to the first spin. This leads to two interference blockades between the drive and the first spin as well as between both spins. Although both interference blockades persist for strong drive and coupling strengths, remarkably, the undriven spin does show a 1:1 phase locking to the external drive. The magnitude of the locking is proportional to the drive strength if the drive strength is small. In other words, the undriven oscillator synchronizes to the external drive through both interference blockades while the blockades persist. For a chain of three coupled spin-1 oscillators, we find synchronization between the first and third spins mediated via the blockaded, second spin.",8,5,2024
Randomized quasi-Monte Carlo and Owen's boundary growth condition: A spectral analysis,"In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis. Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis. Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types. We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance.",8,5,2024
Unclocklike biological oscillators with frequency memory,"Entrainment experiments on the vertebrate segmentation clock have revealed that embryonic oscillators actively change their internal frequency to adapt to the driving signal. This is neither consistent with a one-dimensional clock model nor with a limit-cycle model, but rather suggests a new ""unclocklike"" behavior. In this work, we propose simple biologically realistic descriptions of such internal frequency adaptation, where a phase oscillator activates a memory variable controlling the oscillator's frequency. We study two opposite limits for the control of the memory variable, one with a smooth phase-averaging memory field, and the other with a pulsatile, phase-dependent activation. Both models recapitulate intriguing properties of the entrained segmentation clock, such as very broad Arnold tongues and an entrainment phase plateauing with detuning. We compute analytically multiple properties of such systems, such as the entrainment phases and cycle shapes. We further describe new phenomena, including hysteresis in entrainment, bistability in the frequency of the entrained oscillator, and probabilistic entrainment. Our work shows oscillators with frequency memory can exhibit new classes of unclocklike properties, that can be tested experimentally.",8,5,2024
Detection of a piecewise linear crack with one incident wave,"This paper is concerned with inverse crack scattering problems for time-harmonic acoustic waves. We prove that a piecewise linear crack with the sound-soft boundary condition in two dimensions can be uniquely determined by the far-field data corresponding to a single incident plane wave or point source. We propose two non-iterative methods for imaging the location and shape of a crack. The first one is a contrast sampling method, while the second one is a variant of the classical factorization method but only with one incoming wave. Newton's iteration method is then employed for getting a more precise reconstruction result. Numerical examples are presented to show the effectiveness of the proposed hybrid method.",8,5,2024
Fusion rule in conformal field theories and topological orders: A unified view of correspondence and (fractional) supersymmetry and their relation to topological holography,"Generalized symmetry, including non-invertible and categorical symmetry, plays a central role in contemporary studies on topological orders (TOs) and the corresponding conformal field theories (CFTs). The generators of such symmetries have a close connection to non-abelian anyonic objects in a bulk CFT or chiral CFT (CCFT), but it has been known that the construction of a CCFT contains theoretical difficulties in general. In this work, we revisit the structure of the fusion rule in $Z_{N}$ symmetric chiral and bulk conformal field theories and the corresponding TOs. We propose a nontrivial expression of subalgebra structure in the fusion rule of a bulk CFT. We name this subalgebra ""bulk semionization"" which corresponds to the fusion rule of the CCFTs and categorical symmetry of the TOs. This is a bulk-edge correspondence based on the symmetry analysis and can be interpreted as a version of topological holography in the recent literature. The topological holography has been expected to be applicable to the systems in general space-time dimensions. Moreover, we give a concise way of unifying duality (or fractional supersymmetry), generalized or categorical symmetry, and Lagrangian subalgebra. Our method is potentially useful to formulate and study general TOs, fundamentally only from the data of bulk CFTs.",8,5,2024
Network mutual information measures for graph similarity,"A wide range of tasks in exploratory network analysis and machine learning, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs. To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used in these unsupervised settings must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest. Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles. Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining. We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems.",8,5,2024
Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming,"Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern. To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse. On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM). We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order. We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities. Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future.",8,5,2024
Air Gap: Protecting Privacy-Conscious Conversational Agents,"The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns. While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors. We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task. Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality. For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective.",8,5,2024
The local cohomology of vector fields,"We compute the local cohomology of vector fields on a manifold. In the smooth case this recovers the diagonal cohomology studied in work of Losik, Guillemin, Fuks and others. In the holomorphic case this cohomology has recently appeared in work of Hennion and Kapranov in their study of the Lie algebra cohomology of vector fields on a complex manifold. Additionally, we construct explicit representatives for cocycles in Gelfand--Fuks cohomology via descent.",8,5,2024
A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective,"3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles. Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia. Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion. However, the difference is that it captures vertical structures that are ignored by 2D BEV. In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities. Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training. We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets. Furthermore, challenges and future research directions are discussed. We hope this report will inspire the community and encourage more research work on 3D occupancy perception. A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work:this https URL.",8,5,2024
Sobolev mappings on metric spaces and Minkowski dimension,"We introduce the class of compactly Hölder mappings between metric spaces and determine the extent to which they distort the Minkowski dimension of a given set. These mappings are defined purely with metric notions and can be seen as a generalization of Sobolev mappings, without the requirement for a measure on the source space. In fact, we show that if $f:X\rightarrow Y$ is a continuous mapping lying in some super-critical Newtonian-Sobolev space $N^{1,p}(X,\mu)$, under standard assumptions on the metric measure space $(X,d,\mu)$, it is then a compactly Hölder mapping. The dimension distortion result we obtain is new even for Sobolev mappings between weighted Euclidean spaces and generalizes previous results of Kaufman and Bishop-Hakobyan-Williams.",8,5,2024
Custom Gradient Estimators are Straight-Through Estimators in Disguise,"Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere. Various differentiable approximations of quantization functions have been proposed to address this issue. In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE). Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator. Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate. We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet.",8,5,2024
Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions,"Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye. To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer. The decoder then extracts the watermarked information from the distorted image. However, this method can only resist weak noise attacks. To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder. The module is aimed at reducing noise and recovering some of the information lost during an attack. Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency. Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities. In addition, ablation experiments show the superiority of our proposed module.",8,5,2024
Stable electro-optic modulators using thin-film lithium tantalate,We demonstrate electro-optic modulators realized in low-loss thin-film lithium tantalate with superior DC-stability (<1 dB power fluctuation from quadrature with 12.1 dBm input) compared to equivalent thin-film lithium niobate modulators (5 dB fluctuation) over 46 hours.,8,5,2024
Asymmetric Symmetry Breaking: Unequal Probabilities of Vacuum Selection,"We study the probabilities of a field, subject to random perturbations, to roll down from the top of a potential, where the top is only $C^1$ continuous. We find that the probability to roll down to the left or right depends the square root of the second derivative of the potential at the top. We solve this problem theoretically by using the Fokker-Planck equations in stochastic process and verify our findings numerically. This study may potentially be a new mechanism to explain the origins of asymmetries in the Universe.",8,5,2024
Data-Error Scaling in Machine Learning on Natural Discrete Combinatorial Mutation-prone Sets: Case Studies on Peptides and Small Molecules,"We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules. We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data. Our synthetic datasets comprise i) two naïve functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs. In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data. We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set. We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots. Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling. This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory.",8,5,2024
Riemann problem for polychromatic soliton gases: a testbed for the spectral kinetic theory,"We use Riemann problem for soliton gas as a benchmark for a detailed numerical validation of the spectral kinetic theory for the Korteweg-de Vries (KdV) and the focusing nonlinear Schrödinger (fNLS) equations. We construct weak solutions to the kinetic equation for soliton gas describing collision of two dense ""polychromatic"" soliton gases composed of a finite number of ""monochromatic"" components, each consisting of solitons with nearly identical spectral parameters of the scattering operator in the Lax pair. The interaction between the gas components plays the key role in the emergent, large-scale hydrodynamic evolution. We then use the solutions of the spectral kinetic equation to evaluate macroscopic physical observables in KdV and fNLS soliton gases and compare them with the respective ensemble averages extracted from the ""exact"" soliton gas numerical solutions of the KdV and fNLS equations. To numerically synthesise dense polychromatic soliton gases we develop a new method which combines recent advances in the spectral theory of the so-called soliton condensates and the effective algorithms for the numerical realisation of $n$-soliton solutions with large $n$.",8,5,2024
Discovery of T center-like quantum defects in silicon,"Quantum technologies would benefit from the development of high performance quantum defects acting as single-photon emitters or spin-photon interface. Finding such a quantum defect in silicon is especially appealing in view of its favorable spin bath and high processability. While some color centers in silicon have been emerging in quantum applications, there is still a need to search and develop new high performance quantum emitters. Searching a high-throughput computational database of more than 22,000 charged complex defects in silicon, we identify a series of defects formed by a group III element combined with carbon ((A-C)$\rm _{Si}$ with A=B,Al,Ga,In,Tl) and substituting on a silicon site. These defects are analogous structurally, electronically and chemically to the well-known T center in silicon ((C-C-H)$\rm_{Si}$) and their optical properties are mainly driven by an unpaired electron in a carbon $p$ orbital. They all emit in the telecom and some of these color centers show improved properties compared to the T center in terms of computed radiative lifetime or emission efficiency. We also show that the synthesis of hydrogenated T center-like defects followed by a dehydrogenation annealing step could be an efficient way of synthesis. All the T center-like defects show a higher symmetry than the T center making them easier to align with magnetic fields. Our work motivates further studies on the synthesis and control of this new family of quantum defects, and also demonstrates the use of high-throughput computational screening to detect new complex quantum defects.",8,5,2024
ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion,"Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks. However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied. This has been a long-standing hindrance to the improvement of pose estimation accuracy. To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method. ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body. Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %. The emphasis of our study is focusing on the position information that is not exploited before in radar singal. This provides direction to investigate other potential non-redundant information from mmWave rader.",8,5,2024
Fast Fourier transforms and fast Wigner and Weyl functions in large quantum systems,"Two methods for fast Fourier transforms are used in a quantum context. The first method is for systems with dimension of the Hilbert space $D=d^n$ with $d$ an odd integer, and is inspired by the Cooley-Tukey formalism. The `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (together with some other transforms) in quantum systems with $d$-dimensional Hilbert space. Limitations of the method are discussed. In some special cases, the $n$ Fourier transforms can be performed in parallel. The second method is for systems with dimension of the Hilbert space $D=d_0...d_{n-1}$ with $d_0,...,d_{n-1}$ odd integers coprime to each other. It is inspired by the Good formalism, which in turn is based on the Chinese reminder theorem. In this case also the `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (that involve some constants related to the number theory that describes the formalism). The `small Fourier transforms' can be performed in a classical computer or in a quantum computer (in which case we have the additional well known advantages of quantum Fourier transform circuits). In the case that the small Fourier transforms are performed with a classical computer, complexity arguments for both methods show the reduction in computational time from ${\cal O}(D^2)$ to ${\cal O}(D\log D)$. The second method is also used for the fast calculation of Wigner and Weyl functions, in quantum systems with large finite dimension of the Hilbert space.",8,5,2024
A Dual-Motor Actuator for Ceiling Robots with High Force and High Speed Capabilities,"Patient transfer devices allow to move patients passively in hospitals and care centers. Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves. However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions. This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls. The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%. Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration.",8,5,2024
Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language,"Across a number of sign languages, temporal and spatial characteristics of dominant hand articulation are used to express semantic and grammatical features. In this study of Austrian Sign Language (Österreichische Gebärdensprache, or ÖGS), motion capture data of four Deaf signers is used to quantitatively characterize the kinematic parameters of sign production in verbs and adjectives. We investigate (1) the difference in production between verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in intensified vs. non-intensified (plain) forms. Motion capture data analysis using linear-mixed effects models (LME) indicates that both the endpoint marking in verbs, as well as marking of intensification in adjectives, are expressed by movement modulation in ÖGS. While the semantic distinction between verb types (telic/atelic) is marked by higher peak velocity and shorter duration for telic signs compared to atelic ones, the grammatical distinction (intensification) in adjectives is expressed by longer duration for intensified compared to non-intensified adjectives. The observed individual differences of signers might be interpreted as personal signing style.",8,5,2024
Selective Classification Under Distribution Shifts,"In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors. To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow. Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild. To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature. We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions. Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers.",8,5,2024
Analysis of the SQP Method for Hyperbolic PDE-Constrained Optimization in Acoustic Full Waveform Inversion,"In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered. The model arises from the acoustic full waveform inversion in the time domain. The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure. This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques. We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations. Our final theoretical result is the R-superlinear convergence of the SQP method.",8,5,2024
Filtering and smoothing estimation algorithms from uncertain nonlinear observations with time-correlated additive noise and random deception attacks,"This paper discusses the problem of estimating a stochastic signal from nonlinear uncertain observations with time-correlated additive noise described by a first-order Markov process. Random deception attacks are assumed to be launched by an adversary, and both this phenomenon and the uncertainty in the observations are modelled by two sets of Bernoulli random variables. Under the assumption that the evolution model generating the signal to be estimated is unknown and only the mean and covariance functions of the processes involved in the observation equation are available, recursive algorithms based on linear approximations of the real observations are proposed for the least-squares filtering and fixed-point smoothing problems. Finally, the feasibility and effectiveness of the developed estimation algorithms are verified by a numerical simulation example, where the impact of uncertain observation and deception attack probabilities on estimation accuracy is evaluated.",8,5,2024
Crystal structure identification with 3D convolutional neural networks with application to high-pressure phase transitions in SiO$_2$,"Efficient, reliable and easy-to-use structure recognition of atomic environments is essential for the analysis of atomic scale computer simulations. In this work, we train two neuronal network (NN) architectures, namely PointNet and dynamic graph convolutional NN (DG-CNN) using different hyperparameters and training regimes to assess their performance in structure identification tasks of atomistic structure data. We show benchmarks on simple crystal structures, where we can compare against established methods. The approach is subsequently extended to structurally more complex SiO$_2$ phases. By making use of this structure recognition tool, we are able to achieve a deeper understanding of the crystallization process in amorphous SiO$_2$ under shock compression. Lastly, we show how the NN based structure identification workflows can be integrated into OVITO using its python interface.",8,5,2024
An efficient truncation scheme for Eulerian and total Lagrangian SPH methods,"In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency. In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency. Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy. In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h. This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation. To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \cite{wang2023fourth}. Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support. A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency.",8,5,2024
The Potential and Implications of Generative AI on HCI Education,"Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines. As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI. In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module. We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions. Our insights are based on replies to a survey sent out to the students after completing the module. Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps. We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools. Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice. We end with a discussion of our findings in relation to the TPACK framework in HCI.",8,5,2024
Divided Powers and Derived De Rham Cohomology,"We develop the formalism of derived divided power algebras, and revisit the theory of derived De Rham and crystalline cohomology in this framework. We characterize derived De Rham cohomology of a derived commutative ring $A$, together with the Hodge filtration on it, in terms of a universal property as the largest filtered divided power thickening of $A$. We show that our approach agrees with A.Raksit's. Along the way, we develop some fundamentals of square-zero extensions and derivations in derived algebraic geometry in connection with derived De Rham cohomology.",8,5,2024
On equivalence of the Mellin-Barnes and the Givental integral representations of the Whittaker functions,We construct an integral transformation intertwining the Gelfand-Tsetlin and the (modified) Gauss-Givental realizations of principle series representations of gl(3). This provides a direct identification of the corresponding integral representations for the gl(3)-Whittaker function. The construction essentially uses integral identities due to Barnes and Gustafson thus providing a basis for their representation theory interpretation. The result of this paper might be useful for constructing the explicit analytic realization of the mirror symmetry map in the case of the flag manifold GL(3)/B.,8,5,2024
Simulating Spin Dynamics of Supersolid States in a Quantum Ising Magnet,"Motivated by the recent experimental study on a quantum Ising magnet $\text{K}_2\text{Co}(\text{SeO}_3)_2$ where spectroscopic evidence of zero-field supersolidity is presented [arXiv:2402.15869], we simulate the excitation spectrum of the corresponding microscopic $XXZ$ model for the compound, using the recently developed excitation ansatz of infinite projected entangled-pair states (iPEPS). We map out the ground state phase diagram and compute the dynamical spin structure factors across a range of magnetic field strengths, focusing especially on the two supersolid phases found near zero and saturation fields. Our simulated excitation spectra for the zero-field supersolid ""Y"" phase are in excellent agreement with the experimental data -- recovering the low-energy branches and integer quantized excited energy levels $\omega_n=nJ_{zz}$. Furthermore, we demonstrate the nonlocal multi-spin-flip features for modes at $\omega_2$, indicative of their multi-magnon nature. Additionally, we identify characteristics of the high-field supersolid ""V"" phase in the simulated spectra, to be compared with future experimental results.",8,5,2024
Principal Component Analysis for Spatial Phase Reconstruction in Atom Interferometry,"Atom interferometers are sensitive to a wide range of forces by encoding their signals in interference patterns of matter waves. To estimate the magnitude of these forces, the underlying phase shifts they imprint on the atoms must be extracted. Up until now, extraction algorithms typically rely on a fixed model of the patterns' spatial structure, which if inaccurate can lead to systematic errors caused by, for example, wavefront aberrations of the used lasers. In this paper we employ an algorithm based on Principal Component Analysis, which is capable of characterizing the spatial phase structure and per image phase offsets of an atom interferometer from a set of images. The algorithm does so without any prior knowledge about the specific spatial pattern as long as this pattern is the same for all images in the set. On simulated images with atom projection noise we show the algorithm's reconstruction performance follows distinct scaling laws, i.e., it is inversely-proportional to the square-root of the number atoms or the number of images respectively, which allows a projection of its performance for experiments. We also successfully extract the spatial phase patterns of two experimental data sets from an atom gravimeter. This algorithm is a first step towards a better understanding and complex spatial phase patterns, e.g., caused by inhomogeneous laser fields in atom interferometry.",8,5,2024
Local Theory of Yang-Mills-Higgs-Schrödinger Flow,"In this article, we study two Hamiltonian type flows: Yang-Mills-Higgs-Schrödinger flow and $A$-Schrödinger flow. For the first one, we only obtain local existence. However, the uniqueness follows from classical tricks for the second one.",8,5,2024
Boundary symmetry breaking of flocking systems,"We consider a flocking system confined transversally between two infinite reflecting parallel walls separated by a distance $L_\perp$. Infinite or periodic boundary conditions are assumed longitudinally to the direction of collective motion, defining a ring geometry typical of experimental realizations with flocking active colloids. Such a confinement selects a flocking state with its mean direction aligned parallel to the wall, thus breaking explicitly the rotational symmetry locally by a boundary effect. Finite size scaling analysis and numerical simulations show that confinement induces an effective mass term ${M_c} \sim L_\perp^{-\zeta}$ (with positive $\zeta$ being the dynamical scaling exponent of the free theory) suppressing scale free correlations at small wave-numbers. However, due to the finite system size in the transversal direction, this effect can only be detected for large enough longitudinal system sizes (i.e. narrow ring geometries). Furthermore, in the longitudinal direction, density correlations are characterized by an anomalous effective mass term. The effective mass term also enhances the global scalar order parameter and suppresses fluctuations of the mean flocking direction. These results suggest an equivalence between transversal confinement and driving by an homogeneous external field, which breaks the rotational symmetry at the global level.",8,5,2024
Early and elongated epochs of planetesimal dynamo generation,"Accreting in the first few Ma after Solar System formation, planetesimals record conditions in the protoplanetary disc and are the remnants of planetary formation processes. The meteorite paleomagnetic record carries key insights into the thermal history of planetesimals and their extent of differentiation. The current paradigm splits the paleomagnetic record into three magnetic field generation epochs: an early nebula field (<5Ma after CAI formation), followed by thermal dynamos (5-34 Ma after CAI formation), then a gap in dynamo generation, before the onset of core solidification and compositional dynamos. The split between these epochs has been defined using thermal evolution and dynamo generation models of planetesimals. Here we demonstrate these epochs are not as distinct as previously thought based on our refined thermal evolution model that includes more realistic parametrisations for mantle convection, non-eutectic core solidification and radiogenic $^{60}Fe$ in the core. Inclusion of $^{60}$ in the core brings forward the onset of dynamo generation to 1-2 Ma after CAI formation, which overlaps with the existence of the nebula field. The second epoch of dynamo generation begins prior to the onset of core solidification, suggesting this epoch is not purely compositionally driven. Planetesimal radius is the dominant control on dynamo generation, and the choice of reference viscosity can widen the gap between epochs of dynamo generation from 0-200 Ma. Overall, timings of different planetesimal magnetic field generation mechanisms are more variable. This alters the information we can glean from the meteorite paleomagnetic record about the early Solar System. Evidence for the nebula field requires more careful interpretation and young paleomagnetic remanences, for example in the pallasites, may not be evidence for planetesimal core solidification.",8,5,2024
Hybrid Convolutional Neural Networks with Reliability Guarantee,"Making AI safe and dependable requires the generation of dependable models and dependable execution of those models. We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model. This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties. Typical redundancy techniques incur at least double or triple the computational expense of the original. We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary. We describe the design, implementation and some preliminary results of a hybrid CNN.",8,5,2024
Multivariate group sequential tests for global summary statistics,"We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses. We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic. An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints. We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation. In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places. Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model. Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements.",8,5,2024
Direct measurement of bulk currents in the quantized Hall regime,"The integer quantized Hall effect reveals a state of scattering-free carrier transport and quantized resistance in a two-dimensional conductor exposed to a perpendicular magnetic field. The quantized resistance is observed for the Hall bar geometry, i.e., if the current carrying contacts are connected by sample edges. A widely accepted model is the Landauer-Buttiker picture, which assumes an incompressible, i.e., electrically insulating bulk state surrounded by one-dimensional edge channels giving rise to quantized resistance. This model is challenged by the screening theory, which takes into account electron-electron interaction and predicts for the quantized Hall plateaus current flow in incompressible strips, which gradually shifts from the sample edges into the bulk with increasing magnetic field. We present direct proof of the predicted scattering-free bulk transport by exploring a Hall bar augmented with an additional contact placed in its center away from the Hall bar edges. Our result supports the screening theory.",8,5,2024
Dynamic Size Counting in the Population Protocol Model,"The population protocol model describes collections of distributed agents that interact in pairs to solve a common task. We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time. In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols.Our contributions in this paper are three-fold. Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps. Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model. Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice.",8,5,2024
Identifying every building's function in large-scale urban areas with multi-modality remote-sensing data,"Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones. Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions. In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data. In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings. Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples. Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy. Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government. The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China. This study has the potential to support large-scale urban management and sustainable urban development. All collected data and produced maps are open access atthis https URL.",8,5,2024
Low-Distortion Clustering in Bounded Growth Graphs,"The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms. One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\log n)$ distortion factor and rounding issues. Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications.We prove that there exist graphs for which an $\Omega((\log n)^{1/3})$ distortion factor is necessary for any clustering. We also consider a class of nice graphs which we call uniformly bounded independence graphs. These include, for example, paths, lattice graphs, and ""dense"" unit disk graphs. For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them. This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph.Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems. We also investigate related lower bounds.",8,5,2024
DenserRadar: A 4D millimeter-wave radar point cloud detector based on dense LiDAR point clouds,"The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application. In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds. Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar. The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset.",8,5,2024
Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection,"Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available atthis https URL.",8,5,2024
Web Intelligence Journal in perspective: an analysis of its two decades trajectory,"The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs. The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities. Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors. To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community. Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis. Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence. The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals.",8,5,2024
Degree of the Grassmannian as an affine variety,"The degree of the Grassmannian with respect to the Plücker embedding is well-known. However, the Plücker embedding, while ubiquitous in pure mathematics, is almost never used in applied mathematics. In applied mathematics, the Grassmannian is usually embedded as projection matrices $\operatorname{Gr}(k,\mathbb{R}^n) \cong \{P \in \mathbb{R}^{n \times n} : P^{\scriptscriptstyle\mathsf{T}} = P = P^2,\; \operatorname{tr}(P) = k\}$ or as involution matrices $\operatorname{Gr}(k,\mathbb{R}^n) \cong \{X \in \mathbb{R}^{n \times n} : X^{\scriptscriptstyle\mathsf{T}} = X,\; X^2 = I,\; \operatorname{tr}(X)=2k - n\}$. We will determine an explicit expression for the degree of the Grassmannian with respect to these embeddings. In so doing, we resolved a conjecture of Devriendt--Friedman--Sturmfels about the degree $\operatorname{Gr}(2, \mathbb{R}^n)$ and in fact generalized it to $\operatorname{Gr}(k, \mathbb{R}^n)$. We also proved a set theoretic variant of another conjecture of Devriendt--Friedman--Sturmfels about the limit of $\operatorname{Gr}(k,\mathbb{R}^n)$ in the sense of Gröbner degneration.",8,5,2024
Evolution of Spin in the Intermediate Polar CC Sculptoris,"We report on spin variations in the intermediate polar and cataclysmic variable CC Scl, as seen by the Transiting Exoplanet Survey Satellite (TESS). By studying both the spin period and its harmonic, we find that the spin has varied since it was first observed in 2011. We find the latest spin value for the source to be 389.473(6)s, equivalent to 0.00450779(7) days, 0.02s shorter than the first value measured. A linear fit to these and intermediate data give a rate of change of spin ~-4.26(2.66)e10^-11 and a characteristic timescale tau~2.90e10^5 years, in line with other known intermediate polars with varying spin. The spin profile of this source also matches theoretical spin profiles of high-inclination intermediate polars, and furthermore, appears to have changed in shape over a period of three years. Such `spin-up' in an intermediate polar is considered to be from mass accretion onto the white dwarf (the primary), and we note the presence of dwarf nova eruptions in this source as being a possible catalyst of the variations.",8,5,2024
Exploring Speech Pattern Disorders in Autism using Machine Learning,"Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech patterns from examiner-patient dialogues presents significant challenges due to the subtle and diverse manifestations of speech-related symptoms in affected individuals. This study presents a comprehensive approach to identify distinctive speech patterns through the analysis of examiner-patient dialogues. Utilizing a dataset of recorded dialogues, we extracted 40 speech-related features, categorized into frequency, zero-crossing rate, energy, spectral characteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance. These features encompass various aspects of speech such as intonation, volume, rhythm, and speech rate, reflecting the complex nature of communicative behaviors in ASD. We employed machine learning for both classification and regression tasks to analyze these speech features. The classification model aimed to differentiate between ASD and non-ASD cases, achieving an accuracy of 87.75%. Regression models were developed to predict speech pattern related variables and a composite score from all variables, facilitating a deeper understanding of the speech dynamics associated with ASD. The effectiveness of machine learning in interpreting intricate speech patterns and the high classification accuracy underscore the potential of computational methods in supporting the diagnostic processes for ASD. This approach not only aids in early detection but also contributes to personalized treatment planning by providing insights into the speech and communication profiles of individuals with ASD.",3,5,2024
Correlation and Autocorrelation of Data on Complex Networks,"Networks where each node has one or more associated numerical values are common in applications. This work studies how summary statistics used for the analysis of spatial data can be applied to non-spatial networks for the purposes of exploratory data analysis. We focus primarily on Moran-type statistics and discuss measures of global autocorrelation, local autocorrelation and global correlation. We introduce null models based on fixing edges and permuting the data or fixing the data and permuting the edges. We demonstrate the use of these statistics on real and synthetic node-valued networks.",8,5,2024
A Gauss-Newton Method for ODE Optimal Tracking Control,"This paper introduces and analyses a continuous optimization approach to solve optimal control problems involving ordinary differential equations (ODEs) and tracking type objectives. Our aim is to determine control or input functions, and potentially uncertain model parameters, for a dynamical system described by an ODE. We establish the mathematical framework and define the optimal control problem with a tracking functional, incorporating regularization terms and box-constraints for model parameters and input functions. Treating the problem as an infinite-dimensional optimization problem, we employ a Gauss-Newton method within a suitable function space framework. This leads to an iterative process where, at each step, we solve a linearization of the problem by considering a linear surrogate model around the current solution estimate. The resulting linear auxiliary problem resembles a linear-quadratic ODE optimal tracking control problem, which we tackle using either a gradient descent method in function spaces or a Riccati-based approach. Finally, we present and analyze the efficacy of our method through numerical experiments.",8,5,2024
An Imprecise Maxwell's Demon with Feedback Delay: An Exactly Solvable Information Engine Model,"A finite cycle time information engine based on a two-level system in contact with a thermal reservoir is studied analytically. The model for the engine incorporates an error in measuring the system's state and time delay between the measurement and the feedback process. The efficiency and power of the engine in steady state are derived as a function of level spacing, feedback delay time, engine cycle time, and measurement error. For a fixed value of level spacing and feedback delay, there is an upper bound on measurement error such that the engine can extract positive work. This threshold value of error is found to be independent of the cycle time. For a range of values of level spacing and feedback delay time, efficiency has a non-monotonic dependence on the measurement error, implying that there is an optimal measurement error for the information engine to operate efficiently. At high temperatures and with precise measurement, the engine's ability to extract positive work is extended over a larger range of feedback delay time.",8,5,2024
Knowledge Gaps and Research Needs for Modeling CO2 Mineralization in the Basalt-CO2-Water System: A Review of Laboratory Experiments,"Carbon capture and storage in basalt is being actively investigated as a scalable climate change mitigation option. Accurate geochemical modeling prediction of the extent and rate of CO2 mineralization is a critical component in assessing the local and global feasibility and efficacy of this strategy. In this study, we review basalt-CO2-water interaction experimental studies conducted during the last two decades to determine whether they provide useable information for geochemical modeling. Most of the cited experiments generate data on the temporal evolution of water composition, and a few provide identification of secondary precipitates and their compositions, offering empirical and semi-quantitative information about the reactivity of basalts and the likelihood of secondary carbonate mineralization at various temperatures, pHs, and pCO2 conditions. However, most experiments provide insufficient information on the properties and quantity of secondary minerals formed, prohibiting accurate mass balance calculations and hence more quantitative geochemical modeling studies. Primary Ca, Mg, and Fe-bearing minerals in basalt control the availability of major ions released into aqueous solution for carbonate precipitation, and many secondary minerals, i.e., smectites, Ca-Mg-Fe carbonates, and zeolites, provide sinks for the same major ions, some of which are difficult to quantify experimentally. Thus, we have a multi-source and multi-sink inverse mass balance problem with insufficient constraints on the bulk system in which the temporal evolution of major ions does not provide sufficient information on which mineral(s) dissolve or the sequence of dissolution and precipitation reactions. Going forward, we propose that future experimental work should focus on trace elements and multiple isotopic tracers and better characterize the solid reaction products with modern analytical instruments.",8,5,2024
A goodness-of-fit diagnostic for count data derived from half-normal plots with a simulated envelope,"Traditional methods of model diagnostics may include a plethora of graphical techniques based on residual analysis, as well as formal tests (e.g. Shapiro-Wilk test for normality and Bartlett test for homogeneity of variance). In this paper we derive a new distance metric based on the half-normal plot with a simulation envelope, a graphical model evaluation method, and investigate its properties through simulation studies. The proposed metric can help to assess the fit of a given model, and also act as a model selection criterion by being comparable across models, whether based or not on a true likelihood. More specifically, it quantitatively encompasses the model evaluation principles and removes the subjective bias when closely related models are involved. We validate the technique by means of an extensive simulation study carried out using count data, and illustrate with two case studies in ecology and fisheries research.",8,5,2024
GRB afterglows with energy injections in AGN accretion disks,"Active galactic nucleus (AGN) disks are widely considered potential hosts for various high-energy transients, including gamma-ray bursts (GRBs). The reactivation of GRB central engines can provide additional energy to shocks formed during the interaction of the initially ejected GRB jets with the circumburst material, commonly referred to as energy injections. In this paper, we study GRBs occurring in AGN disks within the context of energy injections. We adopt the standard external forward shock (EFS) model and consider both short- and long-duration GRB scenarios. Light curves for two types of radiation, namely the radiation from the heated disk material (RHDM) and GRB afterglows, are computed. We find that the energy injection facilitates the EFS to break out from the photosphere of the low-density AGN disk at relativistic velocity. Moreover, the energy injection almost does not affect the RHDM but significantly enhances the peak flux of the GRB afterglows.",8,5,2024
Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference,"Estimating causal effects under interference is pertinent to many real-world settings. However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information. A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information. However, their use of polynomial extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population. We analyze the bias and variance of an interpolation-style estimator under this experimental design. Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator. Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small.",8,5,2024
Full Version: (De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms,"We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of ""Multi-Dimensional Homomorphisms (MDHs)"". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.",8,5,2024
(In)Security of Mobile Apps in Developing Countries: A Systematic Literature Review,"In developing countries, several key sectors, including education, finance, agriculture, and healthcare, mainly deliver their services via mobile app technology on handheld devices. As a result, mobile app security has emerged as a paramount issue in developing countries. In this paper, we investigate the state of research on mobile app security, focusing on developing countries. More specifically, we performed a systematic literature review exploring the research directions taken by existing works, the different security concerns addressed, and the techniques used by researchers to highlight or address app security issues. Our main findings are: (1) the literature includes only a few studies on mobile app security in the context of developing countries ; (2) among the different security concerns that researchers study, vulnerability detection appears to be the leading research topic; (3) FinTech apps are revealed as the main target in the relevant literature. Overall, our work highlights that there is largely room for developing further specialized techniques addressing mobile app security in the context of developing countries.",7,5,2024
XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples,"Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data. In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning. Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages. Our code is available atthis https URL.",8,5,2024
Likelihood and appearance of life beyond the Earth: An astronomical perspective,"As of 2023, over 5500 planets are known to orbit stars other than our Sun. We can measure their sizes and orbital periods, infer their masses and temperatures, and constrain their compositions. Based on these data, about 1% of extrasolar planets are potentially habitable for life as we know it, implying that of the billions of planets in our Galaxy, some may actually be inhabited, at least by microbes. However, recognizing signs of alien life forms is a major challenge for current technology, because of the wide range of conditions on extrasolar planets, and because of the wide range of forms that life may take. This chapter reviews observations of exoplanets and discusses astrobiological definitions of habitability and the likelihood of finding life beyond the Earth, both within and outside the Solar system.",8,5,2024
Universality of opinions disappearing in sociophysical models of opinion dynamics: From initial multitude of opinions to ultimate consensus,"Possibility of reaching a consensus in social systems with strong initial fragmentation is one of the most interesting issues in sociopysics. It is also intriguing what the dynamics of such processes is. To address those problems, we performed computer simulations using well-established models of social opinion formation, namely the voter, Sznajd, and Latané models. We investigated opinion dynamics in cases where the initial number of opinions is very large, equal to the number of actors (the voter and Latané models) or when every second actor has their own opinion (Sznajd model), with some variations on the update schemes, lattice topologies, effective ranges of interaction, and information noise levels. For all considered models, the number of opinions assumed by the actors is finally almost always reduced to only one. However, while the voter and Latané models exhibit a power-law time decrease in the number of opinions, the Sznajd model follows a complex three-stage behavior. We also demonstrated that the mean/median time of reaching the consensus scales with system size according to a power law for voter and Sznajd models, while for the Latané model this increase is even faster. Our results show that in the studied models the consensus is possible, provided that a long enough and model-dependent time to reach this state is available.",8,5,2024
Anharmonic phonons with Gaussian processes,"We provide a method for calculating anharmonic lattice dynamics, by building a surrogate model based on Gaussian Processes (GPs). Due to the underlying Gaussian form of a GP, the model is infinitely differentiable. This allows us to train the model trained directly on forces (the derivative of PESs) reducing the evaluations required for a given accuracy. We can extend this differentiation to directly calculate second and third order force-constants using automatic differentiation (AD). For the five model materials we study, we find that the force-constants are in close agreement with a standard finite-displacement approach. Our method appears to be linear scaling in the number of atoms at predicting both second and third-order (anharmonic) force-constants.",8,5,2024
High Voltage Determination and Stabilization for Collinear Laser Spectroscopy Applications,"Fast beam collinear laser spectroscopy is the established method to investigate nuclear ground state properties such as the spin, the electromagnetic moments, and the charge radius of exotic nuclei. These are extracted with high precision from atomic observables, i.e., the hyperfine splitting and its the isotope shift, which becomes possible due to a large reduction of the Doppler broadening by compressing the velocity width of the ion beam through electrostatic acceleration. With the advancement of the experimental methods and applied devices, e.g., to measure and stabilize the laser frequency, the acceleration potential became the dominant systematic uncertainty contribution. To overcome this, we present a custom-built high-voltage divider, which was developed and tested at the German metrology institute (PTB), and a feedback loop that enabled collinear laser spectroscopy to be performed at a 100-kHz level. Furthermore, we describe the impact of field penetration into the laser-ion-interaction region. This strongly affects the determined isotope shifts and hyperfine splittings, if Doppler tuning is applied, i.e., the ion beam energy is altered instead of scanning the laser frequency. Using different laser frequencies that were referenced to a frequency comb, the field penetration was extracted laser spectroscopically. This allowed us to define an effective scanning potential to still apply the faster and easier Doppler tuning without introducing systematic deviations.",8,5,2024
JWST FRESCO: a comprehensive census of H$β$+[OIII] emitters at 6.8<z<9.0 in the GOODS fields,"We present the census of H$\beta$+[O III] $4960,5008$ Åemitters at $6.8<z<9.0$ from the JWST FRESCO survey over 124 arcmin$^2$ in the GOODS-North and GOODS-South fields. Our unbiased spectroscopic search results in 137 spectroscopically-confirmed galaxies at $6.8<z<9.0$ with observed [O III] fluxes $f_{[O III]}\gtrsim 1\times 10^{-18}\ \rm{erg}\ \rm{s}^{-1} \ \rm{cm}^{-2}$. The rest-frame optical line ratios of the median stacked spectrum indicate negligible dust attenuation, low metallicity ($12+\log(\rm{O/H})= 7.2-7.7$) and a high ionisation parameter $\log_{10}U \simeq -2.5$ at a median UV magnitude $M_{\rm{UV}}=-19.65^{+0.59}_{-1.05}$. We find a factor $\times\ 1.3$ difference in the number density of $6.8<z<9.0$ galaxies between GOODS-South and GOODS-North, which is caused by single overdensity at $7.0<z<7.2$ in GOODS-North. The bright end of the UV luminosity function of spectroscopically-confirmed [O III] emitters is in good agreement with that from pre-JWST dropout-selected samples. Discrepancies between the observed [O III] LF, [O III] /UV ratio and [O III] equivalent widths distribution and that predicted by theoretical models suggest burstier star-formation histories and/or more heterogeneous metallicity and ionising conditions in $z>7$ galaxies. We report a rapid decline of the [O III] luminosity density at $z\gtrsim 6-7$ which cannot be explained solely by the evolution of the cosmic star-formation rate density. Finally, we find that FRESCO, in only $2$h, captures star-forming galaxies likely accounting for $\sim 10-20\%$ of the ionising budget at $z=7$ and $z=8$, raising the prospect of detecting directly all the sources of reionisation with JWST.",8,5,2024
Uncertainty quantification in metric spaces,"This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space. The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used. Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees. To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fréchet model) in various clinical applications related to precision and digital medicine. The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions.",8,5,2024
QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs,"Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries. However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries. In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization. Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs. To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables. Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches. Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization.",8,5,2024
"Spin-lattice-coupled helical magnetic order in breathing pyrochlore magnets, CuAlCr$_{4}$S$_{8}$ and CuGaCr$_{4}$S$_{8}$","We report low-temperature powder X-ray and neutron diffraction studies on breathing pyrochlore magnets Cu$M$Cr$_{4}$S$_{8}$ ($M$ = Al, Ga), which undergo a magnetic transition at $T_{\rm N} \approx$ 21 and 31 K for {$M$ = Al and Ga, respectively. X-ray diffraction reveals that the magnetic transition accompanies a structural transition from cubic $F{\overline 4}3m$ to polar orthorhombic $Imm2$ symmetry for both the compounds, with larger distortion observed for $M$ = Ga at low temperatures. Neutron diffraction reveals incommensurate magnetic modulation ${\mathbf Q} = (q_{\rm IC}, 0.5, 0)$ in the orthorhombic setting, where $q_{\rm IC} \approx$ 0.39 and 0.31 for $M$ = Al and Ga, respectively. Our magnetic-structure analysis suggests cycloid-type magnetic order but not proper-screw type for both the compounds. We find strong correlation between the local spin configuration and Cr-Cr bond lengths, indicating that the spin-lattice coupling as well as the magnetic frustration play an important role in determining the ground state. Cu$M$Cr$_{4}$S$_{8}$ potentially offers a platform to explore magnetoelectric effects arising from the helimagnet driven electric polarity.",8,5,2024
Leveraging AES Padding: dBs for Nothing and FEC for Free in IoT Systems,"The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices. This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities. Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems. Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process. The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process. This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER). Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications. This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications.",8,5,2024
A note on the $ Π$-property of some subgroups of finite groups,"Let $ H $ be a subgroup of a finite group $ G $. We say that $ H $ satisfies the $ \Pi $-property in $ G $ if for any chief factor $ L / K $ of $ G $, $ |G/K : N_{G/K}(HK/K\cap L/K )| $ is a $ \pi (HK/K\cap L/K) $-number. In this paper, we obtain some criteria for the $ p $-supersolubility or $ p $-nilpotency of a finite group and extend some known results by concerning some subgroups that satisfy the $ \Pi $-property.",8,5,2024
Algebraic symmetries of the observables on the sky: Variable emitters and observers,"In this paper we prove a number of exact relations between optical observables, such as trigonometric parallax, position drift and the proper motion of a luminous source in addition to the variations of redshift and the viewing angle. These relations are valid in general relativity for any spacetime and they are of potential interest for astrometry and precise cosmology. They generalize the well-known Etherington's reciprocity relation between the angular diameter distance and the luminosity distance. Similar to the Etherington's relation, they hold independently of the spacetime metric, the positions and the motions of a light source or an observer. We show that those relations follow from the symplectic property of the bi-local geodesic operator, i.e., the geometric object that describes the light propagation between two distant regions of a spacetime. The set of relations we present is complete in the sense that no other relations between those observables should hold in general. In the meantime, we develop the mathematical machinery of the bi-local approach to light propagation in general relativity and its corresponding Hamiltonian formalism.",8,5,2024
Dark photon constraints from CMB temperature anisotropies,"The resonant conversion, within the inter-galactic medium, of regular photons into dark photons amplifies the anisotropy observed in the CMB, thereby imposing stringent constraints on the existence of light dark photons. In this study, we investigate the impact of light dark photons, with masses in the range $3\times 10^{-15} ~\rm{eV} < m_{A'} < 3\times 10^{-12}~\rm{eV}$ on the power spectrum of temperature anisotropies within the cosmic microwave background (CMB) radiation utilizing the state-of-the-art large-volume FLAMINGO cosmological simulations. Our results show that using full Planck data, one can expect the existing constraints on the dark photon mixing parameter in this mass range to improve by an order of magnitude.",8,5,2024
Multistability of Bi-Reaction Networks,"We provide a sufficient and necessary condition in terms of the stoichiometric coefficients for a bi-reaction network to admit multistability. Also, this result completely characterizes the bi-reaction networks according to if they admit multistability.",8,5,2024
The Harmonic Descent Chain,"The decreasing Markov chain on \{1,2,3, \ldots\} with transition probabilities $p(j,j-i) \propto 1/i$ arises as a key component of the analysis of the beta-splitting random tree model. We give a direct and almost self-contained ""probability"" treatment of its occupation probabilities, as a counterpart to a more sophisticated but perhaps opaque derivation using a limit continuum tree structure and Mellin transforms.",8,5,2024
Inflation Models with Correlation and Skew,"We formulate a forward inflation index model with multi-factor volatility structure featuring a parametric form that allows calibration to correlations between indices of different tenors observed in the market. Assuming the nominal interest rate follows a single factor Gaussian short rate model, we present analytical prices for zero-coupon and year-on-year swaps, caps, and floors. The same method applies to any interest rate model for which one can compute the zero-coupon bond prices and measure shifts. We extend the multi-factor model with leverage functions to capture the entire market volatility skew with a single process. The time-consuming calibration step of this model can be avoided in the simplified model that we further propose. We demonstrate the leveraged and the simplified models with market data.",8,5,2024
Fundamental Limits for Jammer-Resilient Communication in Finite-Resolution MIMO,"Spatial filtering based on multiple-input multiple-output (MIMO) processing is a powerful method for jammer mitigation. In principle, a MIMO receiver can null the interference of a single-antenna jammer at the cost of only one degree of freedom - if the number of receive antennas is large, communication performance is barely affected. In this paper, we show that the potential for MIMO jammer mitigation based on the digital outputs of finite-resolution analog-to-digital converters (ADCs) is fundamentally worse: Strong jammers will either cause the ADCs to saturate (when the ADCs' quantization range is small) or drown legitimate communication signals in quantization noise (when the ADCs' quantization range is large). We provide a fundamental bound on the mutual information between the quantized receive signal and the legitimate transmit signal. Our bound shows that, for any fixed ADC resolution, the mutual information tends to zero as the jammer power tends to infinity. Our bound also confirms the intuition that for every 6.02 dB increase in jamming power, the ADC resolution must be increased by 1 bit in order to prevent the mutual information from vanishing.",8,5,2024
Phase-induced vortex pinning in rotating supersolid dipolar systems,"We analyze the pinning of vortices for a stationary rotating dipolar supersolid along the low-density paths between droplets as a function of the rotation frequency. We restrict ourselves to the stationary configurations of vortices with the same symmetry as that of the array of droplets. Our approach exploits the fact that the wave function of each droplet acquires a linear phase on the coordinates, and hence the relative phases between neighboring droplets allows us to predict the position of the vortices. For a confined system, the estimate accurately reproduces the Gross-Pitaevskii results in the spatial regions where the neighboring droplets are well defined.",8,5,2024
Energy stable gradient flow schemes for shape and topology optimization in Navier-Stokes flows,We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model. Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem. Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces. Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed.,8,5,2024
Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks,"Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In contrast, for biological neurons e.g. ""it is not uncommon for axonal propagation of action potentials to happen in both directions"" \cite{axon} - suggesting they are optimized to continuously operate in multidirectional way. Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments. Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\rho(x|y,z)$ or $\rho(y,z|x)$ by substituting to $\rho(x,y,z)$ and normalizing. There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\rho(x,y,z)=\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition. Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities.",8,5,2024
Rapid Co-design of Task-Specialized Whegged Robots for Ad-Hoc Needs,"In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks. Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters. Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware.",8,5,2024
Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators,"This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods.",8,5,2024
Mass function of stellar black holes as revealed by the LIGO-Virgo-KAGRA observations,"Ninety gravitational wave events have been detected by the LIGO-Virgo-KAGRA network and are released in the Gravitational-Wave Transient Catalog. Among these events, 83 cases are definitely binary black hole mergers since the masses of all the objects involved significantly exceed the upper limit of neutron stars. The black holes in these merger events naturally form two interesting samples, a pre-merger sample that includes all the black holes before the mergers and a post-merger sample that consists of the black holes generated during the merging processes. The former represents black holes that once existed in the Universe, while the latter represents newly born black holes. Here we present a statistical analysis on these two samples. The non-parametric $\tau$ statistic method is adopted to correct for the observational selection effect. The Lynden-Bell's $C^{-}$ method is further applied to derive the mass distribution and density function of black holes. It is found that the mass distribution can be expressed as a broken power-law function. More interestingly, the power-law index in the high mass region is comparable for the two samples. The number density of black holes is found to depend on redshift as $\rho(z) \propto z^{-2.06}$-$z^{-2.12}$ based on the two samples. Implications of these findings on the origin of black holes are discussed.",8,5,2024
A Hierarchical Approach to Quantum Many-Body Systems in Structured Environments,"Cavity quantum materials combine the rich many-body physics of condensed matter systems with strong coupling to the surrounding electromagnetic field, which presents both novel prospects and intricate challenges. One is often interested in the properties of one specific aspect of the material, e.g. the electronic many-body dynamics, subject to a structured bath of phononic and photonic modes. Open quantum systems featuring non-Markovian dynamics are routinely solved using techniques such as the Hierarchical Equations of Motion (HEOM) but their usage of the system density-matrix renders them intractable for many-body systems. Here, we combine the HEOM with the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy to reach a consistent and rigorous description of open many-body systems and their quantum dynamics. We demonstrate first the strength and limitations of this stacked hierarchy for superradiant emission and spin-squeezing of established quantum optical models before presenting its full potential for quantum many-body systems. In particular, we explicitly simulate the impact of charge noise on the dynamic of the Fermi-Hubbard model subject to a structured bath comprising cavity and vibro-phononic environment. Strong optical coupling not only modifies the dynamic of the many-body system but serves furthermore as measurement channel providing information about the correlated motion imprinted by charge noise. Our work establishes an accessible, yet rigorous, route between condensed matter and quantum optics, fostering the growth of a new domain at their interface.",8,5,2024
Understanding solid nitrogen through machine learning simulation,"We construct a fast, transferable, general purpose, machine-learning interatomic potential suitable for large-scale simulations of $N_2$. The potential is trained only on high quality quantum chemical molecule-molecule interactions, no condensed phase information is used. The potential reproduces the experimental phase diagram including the melt curve and the molecular solid phases of nitrogen up to 10 GPa. This demonstrates that many-molecule interactions are unnecessary to explain the condensed phases of $N_2$. With increased pressure, transitions are observed from cubic ($\alpha-N_2$), which optimises quadrupole-quadrupole interactions, through tetragonal ($\gamma-N_2$) which allows more efficient packing, through to monoclinic ($\lambda-N_2$) which packs still more efficiently. On heating, we obtain the hcp 3D rotor phase ($\beta-N_2$) and, at pressure, the cubic $\delta-N_2$ phase which contains both 3D and 2D rotors, tetragonal $\delta^\star-N_2$ phase with 2D rotors and the rhombohedral $\epsilon-N_2$. Molecular dynamics demonstrates where these phases are indeed rotors, rather than frustrated order. The model does not support the existence of the wide range of bondlengths reported for the complex $\iota-N_2$ phase. The thermodynamic transitions involve both shifts of molecular centres and rotations of molecules. We simulate these phase transitions between finding that the onset of rotation is rapid whereas motion of molecular centres is inhibited and the cause of the observed sluggishness of transitions. Routine density functional theory calculations give a similar picture to the potential.",8,5,2024
What doesn't kill Gaia makes her stronger,"Life on Earth has experienced numerous upheavals over its approximately 4 billion year history. In previous work we have discussed how interruptions to stability lead, on average, to increases in habitability over time, a tendency we called Entropic Gaia. Here we continue this exploration, working with the Tangled Nature Model of co-evolution, to understand how the evolutionary history of life is shaped by periods of acute environmental stress. We find that while these periods of stress pose a risk of complete extinction, they also create opportunities for evolutionary exploration which would otherwise be impossible, leading to more populous and stable states among the survivors than in alternative histories without a stress period. We also study how the duration, repetition and number of refugia into which life escapes during the perturbation affects the final outcome. The model results are discussed in relation to both Earth history and the search for alien life.",8,5,2024
Spontaneous Crystal Thermal Hall Effect in Insulating Altermagnets,"We show that magnetic insulators with a collinear and compensated order can exhibit a thermal Hall effect even at zero magnetic field if they have altermagnetic symmetries. We predict a finite thermal Hall conductivity vector $\boldsymbol{\kappa}_\text{H}$ for a rutile-inspired effective spin model with Dzyaloshinskii-Moriya interaction. Within the linear spin-wave theory, we identify two magnon branches that carry identical Berry curvature and give rise to a finite $\boldsymbol{\kappa}_\text{H}$, which can be controlled by the Néel vector orientation and by strain. The thermal Hall response is further complemented with a spin Nernst response to contrast spin and heat transport in altermagnetic insulators with those in ferromagnets and antiferromagnets. Our results establish the crystal thermal Hall effect of magnons and we discuss material candidates for experimental realization, such as MnF$_2$, CoF$_2$, and NiF$_2$.",8,5,2024
Longitudinal spin polarization in a thermal model with dissipative corrections,"In this work, we address the problem of longitudinal spin polarization of the $\Lambda$ hyperons produced in relativistic heavy-ion collisions. We combine a relativistic kinetic-theory framework that includes spin degrees of freedom treated in a classical way with the freeze-out parametrization used in previous investigations. The use of the kinetic theory allows us to incorporate dissipative corrections (due to the thermal shear and gradients of thermal vorticity) into the Pauli-Lubanski vector that determines spin polarization and can be directly compared with the experimental data. As in earlier similar studies, it turns out that a successful description of data can only be achieved with additional assumptions -- in our case, they involve the use of projected thermal vorticity and a suitably adjusted time for spin relaxation ($\tau_s$). From our analysis, we find that $\tau_s \sim 5$ fm/$c$, which is comparable with other estimates.",8,5,2024
Local Tameness and Ends of Groups,We investigate connections between local tameness of a group and a number of its ends.,8,5,2024
Electroweak Multiplets as Dark Matter candidates: A brief review,"I provide a thorough review of the theoretical and experimental status of ElectroWeak multiplets as Dark Matter candidates, serving as the prototype of Weakly Interacting Massive Particles (WIMPs) Dark Matter. Specifically, the examination includes both real SU(2) representations with zero hypercharge and complex ones with $Y\neq 0$. For the first time, all calculable thermal masses for scalar and fermionic WIMPs are computed, incorporating significant non-perturbative non-relativistic effects such as Sommerfeld enhancement and the formation of WIMP bound states. WIMP masses of few hundred TeV are shown to be compatible both with $s$-wave unitarity of the annihilation cross-section, and perturbativity. Additionally, a strategy is outlined for probing these scenarios in the next generation of experiments.",8,5,2024
"Cryptocurrency Risk, Trust, and Acceptance in Thailand: A Comparative Study with Switzerland","The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era. Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region. Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland. Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies. With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries.",8,5,2024
Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling Biases of Equal Shares in Participatory Budgeting,"Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation. So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares. However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments. This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods. This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics. We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented. However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture. We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation.",8,5,2024
"Transition frequencies, isotope shifts, and hyperfine structure in $4s \rightarrow 4p$ transitions of Ti$^+$ ions","We have measured transition frequencies, isotope shifts and hyperfine structure splittings in the $3 d^{2}\left({ }^{3\!}F\right) 4 s\,{ }^{4} F_J\rightarrow 3 d^{2}\left({ }^{3\!} F\right) 4 p \,^{4} G_{J+1}$ transitions in Ti$^+$ ions for $J=\frac{3}{2},\, \frac{5}{2},\, \frac{7}{2}$ using collinear laser spectroscopy. Ions were generated by laser ablation in a buffer gas atmosphere and extracted into vacuum through a nozzle and a pair of radiofrequency (RF) funnels. The obtained results are of interest as reference values for on-line measurements of short-lived titanium isotopes and for astrophysical searches for temporal or spatial variations of the fine structure constant $\alpha$ using quasar absorption spectra.",8,5,2024
Committee Elections with Candidate Attribute Constraints,"In many real-world applications of committee elections, the candidates are associated with certain attributes and the chosen committee is required to satisfy some constraints posed on the candidate attributes. For instance, when dress collocation, it is generally acknowledged that when wearing a tie, you'd better wear a shirt, and wearing a suit, you'd better wear leather shoes. Here, dresses are categorized by upper garment, lower garment, shoesthis http URL, and upper garment is with the attribute tie and shirt, lower garment is with the attribute suit, and shoes is with the attribute leather. And two constraints ""tie infers shirt"" and ""suit infers leather shoes"" are proposed. We study this variant of committee elections from the computational complexity viewpoint. Given a set of candidates, each with some attributes and a profit, and a set of constraints, given as propositional logical expressions of the attributes, the task is to compute a set of k candidates, whose attributes satisfy all constraints and whose total profit achieves a given bound. We achieve a dichotomy concerning classical complexity with no length limit on constraints: the problem is polynomial-time solvable, if the following two conditions are fulfilled: 1) each candidate has only one attribute and 2) each attribute occurs at most once in the constraints. It becomes NP-hard if one of the two conditions is violated. Moreover, we examine its parameterized complexity. The parameterization with the number of constraints, the size of the committee, or the total profit bound as parameter leads to para-NP-hardness or W[1]-hardness, while with the number of attributes or the number of candidates as parameter, the problem turns out to be fixed-parameter tractable.",8,5,2024
On linear-combinatorial problems associated with subspaces spanned by $\{\pm 1\}$-vectors,"A complete answer to the question about subspaces generated by $\{\pm 1\}$-vectors, which arose in the work of I.Kanter and H.Sompolinsky on associative memories, is given. More precisely, let vectors $v_1, \ldots , v_p,$ $p\leq n-1,$ be chosen at random uniformly and independently from $\{\pm 1\}^n \subset {\bf R}^n.$ Then the probability ${\mathbb P}(p, n)$ that $$span \ \langle v_1, \ldots , v_p \rangle \cap \left\{ \{\pm 1\}^n \setminus \{\pm v_1, \ldots , \pm v_p\}\right\} \ne \emptyset \ $$ is shown to be $$4{p \choose 3}\left(\frac{3}{4}\right)^n + O\left(\left(\frac{5}{8} + o_n(1)\right)^n\right) \quad \mbox{as} \quad n\to \infty,$$ where the constant implied by the $O$-notation does not depend on $p$. The main term in this estimate is the probability that some 3 vectors $v_{j_1}, v_{j_2}, v_{j_3}$ of $v_j$, $j= 1, \ldots , p,$ have a linear combination that is a $\{\pm 1\}$-vector different from $\pm v_{j_1}, \pm v_{j_2}, \pm v_{j_3}. $",8,5,2024
Robust deep learning from weakly dependent data,"Recent developments on deep learning established some theoretical properties of deep neural networks estimators. However, most of the existing works on this topic are restricted to bounded loss functions or (sub)-Gaussian or bounded input. This paper considers robust deep learning from weakly dependent observations, with unbounded loss function and unbounded input/output. It is only assumed that the output variable has a finite $r$ order moment, with $r >1$. Non asymptotic bounds for the expected excess risk of the deep neural network estimator are established under strong mixing, and $\psi$-weak dependence assumptions on the observations. We derive a relationship between these bounds and $r$, and when the data have moments of any order (that is $r=\infty$), the convergence rate is close to some well-known results. When the target predictor belongs to the class of Hölder smooth functions with sufficiently large smoothness index, the rate of the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained with i.i.d. samples. Application to robust nonparametric regression and robust nonparametric autoregression are considered. The simulation study for models with heavy-tailed errors shows that, robust estimators with absolute loss and Huber loss function outperform the least squares method.",8,5,2024
Concerns on Bias in Large Language Models when Creating Synthetic Personae,"This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs). These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them. The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research.",8,5,2024
Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment,"Initialization-free bundle adjustment (BA) remains largely uncharted. While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization. In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization. Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem. We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series. Importantly, we link the power series expansion to Riemannian manifold optimization. This projective framework is crucial to solve large-scale bundle adjustment problem without initialization. Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy. In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion.",8,5,2024
Observation of $t\bar{t}$ production in the lepton+jets and dilepton channels in $p$+Pb collisions at $\sqrt{s_\mathrm{NN}}=8.16$ TeV with the ATLAS detector,"This paper reports the observation of top-quark pair production in proton-lead collisions in the ATLAS experiment at the Large Hadron Collider. The measurement is performed using 165 nb$^{-1}$ of $p$+Pb data collected at $\sqrt{s_\mathrm{NN}}=8.16$ TeV in 2016. Events are categorised in two analysis channels, consisting of either events with exactly one lepton (electron or muon) and at least four jets, or events with two opposite-charge leptons and at least two jets. In both channels at least one $b$-tagged jet is also required. Top-quark pair production is observed with a significance over five standard deviations in each channel. The top-quark pair production cross-section is measured to be $\sigma_{t\bar{t}}= 58.1\pm 2.0\;\mathrm{(stat.)\;^{+4.8}_{-4.4} \;\mathrm{(syst.)}}\;\mathrm{nb}$, with a total uncertainty of 9%. In addition, the nuclear modification factor is measured to be $R_{p\mathrm{A}} = 1.090\pm0.039\;(\mathrm{stat.})\;^{+0.094}_{-0.087}\;(\mathrm{syst.})$. The measurements are found to be in good agreement with theory predictions involving nuclear parton distribution functions.",8,5,2024
"Analyzing the Influence of Geometrical Deformation on Photon Sphere and Shadow Radius: A New Analytical Approach -Stationary, and Axisymmetric Spacetime",Black hole shadows and photon spheres offer valuable tools for investigating black hole properties. Recent observations by the Event Horizon Telescope Collaboration have confirmed the existence of rotating black holes. Black hole parameters influence the observed shadow size. This paper explores the impact of geometrical deformations on black hole shadow size using gravitational decoupling applied to axially-symmetric spacetime. We find the results are more complex than the spherically-symmetric case. We compare shadows in well-known models with those of an Kerr black hole. Our approach suggests that the influence of an accretion disc on the observed shadow shape can be accurately described despite negligible impact on the black hole geometry itself.,8,5,2024
Subsystem Information Capacity in Random Circuits and Hamiltonian Dynamics,"In this study, we explore the information capacity of open quantum systems, focusing on the effective channels formed by the subsystem of random quantum circuits and quantum Hamiltonian evolution. By analyzing the subsystem information capacity, which is closely linked to quantum coherent information of these effective quantum channels, we uncover a diverse range of dynamical and steady behaviors depending on the types of evolution. Therefore, the subsystem information capacity serves as a valuable tool for studying the intrinsic nature of various dynamical phases, such as integrable, localized, thermalized, and topological systems. We also reveal the impact of different initial information encoding schemes on information dynamics including one-to-one, one-to-many, and many-to-many. To support our findings, we provide representative examples for numerical simulations, including random quantum circuits with or without mid-circuit measurements, random Clifford Floquet circuits, free and interacting Aubry-André models, and Su-Schrieffer-Heeger models. Those numerical results are further quantitatively explained using the effective statistical model mapping and the quasiparticle picture in the cases of random circuits and non-interacting Hamiltonian dynamics, respectively.",8,5,2024
Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations,"This work studies sparse adversarial perturbations bounded by $l_0$ norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available atthis https URL.",8,5,2024
K3 surfaces associated to a cubic fourfold,"Let $X\subset ¶^5$ be a smooth cubic fourfold. A well known conjecture asserts that $X$ is rational if and only if there an Hodge theoretically associated K3 surface $S$. The surface $S$ can be associated to $X$ in two other different ways. If there is an equivalence of categories $\sA_X \simeq D^b(S,\alpha)$ where $\sA_X$ is the Kuznetsov component of $D^b(X)$ and $\alpha$ is a Brauer class, or if there is an isomorphism between the transcendental motive $t(X)$ and the (twisted ) transcendental motive of a K3 surface$S$. In this note we consider families of cubic fourfolds with a finite group of automorphisms and describe the cases where there is an associated K3 surface in one of the above senses.",8,5,2024
gasmodel: An R Package for Generalized Autoregressive Score Models,"Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution. GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions. This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models. The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables. Model estimation utilizes the maximum likelihood method.",8,5,2024
Novel Actor-Critic Algorithm for Robust Decision Making of CAV under Delays and Loss of V2X Data,"Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles. However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle. This issue should be considered when designing control strategies for connected and autonomous vehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data. The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values. To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge. Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data. We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches. The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms. Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels.",8,5,2024
Search for production of a single vector-like quark decaying to tH or tZ in the all-hadronic final state in pp collisions at $\sqrt{s}$ = 13 TeV,"A search for electroweak production of a single vector-like T quark in association with a bottom (b) quark in the all-hadronic decay channel is presented. This search uses proton-proton collision data at $\sqrt{s}$ = 13 TeV collected by the CMS experiment at the CERN LHC during 2016-2018, corresponding to an integrated luminosity of 138 fb$^{-1}$ The T quark is assumed to have charge 2/3 and decay to a top (t) quark and a Higgs (H) or Z boson. Event kinematics and the presence of jets containing b hadrons are used to reconstruct the hadronic decays of the t quark and H or Z boson. No significant deviation from the standard model prediction is observed in the data. The 95% confidence level upper limits on the product of the production cross section and branching fraction of a T quark produced in association with a b quark and decaying via tH or tZ range from 1260 to 68 fb for T quark masses of 600-1200 GeV.",8,5,2024
Equivalence analysis between Quasi-coarse-grained and Atomistic Simulations,"In recent years, simulation methods based on the scaling of atomic potential functions, such as quasi-coarse-grained dynamics and coarse-grained dynamics, have shown promising results for modeling crystalline systems at multiple scales. However, this letter presents evidence suggesting that the spatiotemporal trajectories of coarse-grained systems generated by such simulation methods exhibit a complete correspondence with those of specific molecular dynamics systems. In essence, current coarse-grained simulation methods involve a direct amplification of the results obtained from molecular dynamics simulations across spatial and temporal scales, yet they may lack the capability to adequately capture authentic scale effects. Consequently, the findings of related studies warrant careful re-evaluation. Furthermore, this study underscores the importance of not only verifying the consistency of mesoscale simulation methods with microscopic simulations but also meticulously assessing their capability to accurately forecast mesoscale physical phenomena.",8,5,2024
Improved Decoy-state and Flag-state Squashing Methods,"In this work, we present an improved analysis for decoy-state methods, enhancing both achievable key rates and recovering analytical results for the single intensity scenario. Our primary focus is improving the shortcomings observed in current decoy-state methods, particularly recovering results when employing no decoy intensities. Our methods enable the continuous interpolation across varying numbers of intensity settings. Additionally, we extend decoy-state techniques to encompass scenarios where intensities vary depending on the signal state, thereby relaxing the constraints on experimental implementations. Our findings demonstrate that a minimum of two intensities are sufficient for high asymptotic secret key rates, thereby further softening experimental requirements. Additionally, we address inherent imperfections within detection setups like imperfect beamsplitters. We derive provable secure lower bounds on the subspace population estimation, which is required for certain squashing methods such as the flag-state squasher. These analytical bounds allow us to encompass arbitrary passive linear optical setups, and together with intensities varying with each signal state, lets us include a broad class of experimental setups.",8,5,2024
Chemistry Beyond Exact Solutions on a Quantum-Centric Supercomputer,"A universal quantum computer can be used as a simulator capable of predicting properties of diverse quantum systems. Electronic structure problems in chemistry offer practical use cases around the hundred-qubit mark. This appears promising since current quantum processors have reached these sizes. However, mapping these use cases onto quantum computers yields deep circuits, and for for pre-fault-tolerant quantum processors, the large number of measurements to estimate molecular energies leads to prohibitive runtimes. As a result, realistic chemistry is out of reach of current quantum computers in isolation. A natural question is whether classical distributed computation can relieve quantum processors from parsing all but a core, intrinsically quantum component of a chemistry workflow. Here, we incorporate quantum computations of chemistry in a quantum-centric supercomputing architecture, using up to 6400 nodes of the supercomputer Fugaku to assist a Heron superconducting quantum processor. We simulate the N$_2$ triple bond breaking in a correlation-consistent cc-pVDZ basis set, and the active-space electronic structure of [2Fe-2S] and [4Fe-4S] clusters, using 58, 45 and 77 qubits respectively, with quantum circuits of up to 10570 (3590 2-qubit) quantum gates. We obtain our results using a class of quantum circuits that approximates molecular eigenstates, and a hybrid estimator. The estimator processes quantum samples, produces upper bounds to the ground-state energy and wavefunctions supported on a polynomial number of states. This guarantees an unconditional quality metric for quantum advantage, certifiable by classical computers at polynomial cost. For current error rates, our results show that classical distributed computing coupled to quantum processors can produce good approximate solutions for practical problems beyond sizes amenable to exact diagonalization.",8,5,2024
Computing Chebyshev polynomials using the complex Remez algorithm,"We employ the generalized Remez algorithm, initially suggested by P. T. P. Tang, to perform an experimental study of Chebyshev polynomials in the complex plane. Our focus lies particularly on the examination of their norms and zeros. What sets our study apart is the breadth of examples considered, coupled with the fact that the degrees under investigation are substantially higher than those in previous studies where other methods have been applied. These computations of Chebyshev polynomials of high degrees reveal discernible patterns which allow for conjectures to be formulated based on abundant experimental evidence. The use of Tang's algorithm allows for computations executed with precision, maintaining accuracy within quantifiable margins of error. Additionally, as a result of our experimental study, we propose what we believe to be a fundamental relationship between Chebyshev and Faber polynomials associated with a compact set.",8,5,2024
Designing Skill-Compatible AI: Methodologies and Frameworks in Chess,"Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility.",8,5,2024
Quasi-Banach Schatten-von Neumann properties in Weyl-Hörmander calculus,"We study structural properties of Wiener-Lebesgue spaces with respect to a slowly varying metrics and certain Lebesgue parameters. For $p\in (0,1]$, we deduce Schatten-$p$ properties for pseudo-differential operators whose symbols, together with their derivatives, obey suitable Wiener-Lebesgue-boundedness conditions. Especially, we perform such investigations for the Weyl-Hörmander calculus. Finally, we apply our results to global-type SG and Shubin pseudo-differential operators.",8,5,2024
Implication of the velocity dispersion scalings on high-mass star formation in molecular clouds,"This paper is aimed at exploring implications of velocity dispersion scalings on high-mass star formation in molecular clouds, including the scalings of Larson's linewidth--size ($\sigma$--$R$) and ratio--mass surface density ($\cal{L}$--$\Sigma$; here $\cal{L}$$=\sigma/R^{0.5}$). We have systematically analyzed the $\sigma$ parameter of well-selected 221 massive clumps, complemented with published samples of other hierarchical density structures of molecular clouds over spatial scales of 0.01--10 pc. Those massive clumps are classified into four phases: quiescent, protostellar, HII region, and PDR clumps in an evolutionary sequence. The velocity dispersion of clumps increases overall with the evolutionary sequence, reflecting enhanced stellar feedback in more evolved phases. The relations of $\sigma$--$R$ and $\cal{L}$--$\Sigma$ are weak with the clump sample alone, but become evident when combined with others spanning a much wider spatial scales. For $\sigma$--$R$, its tight relation indicates a kinematic connection between hierarchical density structures, supporting theoretical models of multiscale high-mass star formation. From the $\cal{L}$--$\Sigma$ relation, cloud structures can be found to transition from over-virial state ($\alpha_\mathrm{vir} > 2$) to sub-virial state ($\alpha_\mathrm{vir} < 2$) as they become smaller and denser, indicating a possible shift in the governing force from turbulence to gravity. This implies that the multiscale physical process of high-mass star formation hinges on the self-gravity of sub-virial molecular clouds. However, the influence of turbulence may not be dismissed until large-scale clouds attain a sub-virial state. This is pending confirmation from future multiscale kinematic observations of molecular clouds with uniform observing settings.",8,5,2024
"What role of gravity, turbulence and magnetic fields in high-mass star formation clouds?","To explore the potential role of gravity, turbulence and magnetic fields in high-mass star formation in molecular clouds, this study revisits the velocity dispersion--size ($\sigma$--$L$) and density--size ($\rho$--$L$) scalings and the associated turbulent energy spectrum using an extensive data sample. The sample includes various hierarchical density structures in high-mass star formation clouds, across scales of 0.01 to 100 pc. We observe $\sigma \propto L^{0.26}$ and $\rho \propto L^{-1.54}$ scalings, converging toward a virial equilibrium state. A nearly flat virial parameter--mass ($\alpha_{\rm vir}-M$) distribution is seen across all density scales, with $\alpha_{\rm vir}$ values centered around unity, suggesting a global equilibrium maintained by the interplay between gravity and turbulence across multiple scales. Our turbulent energy spectrum ($E(k)$) analysis, based on the $\sigma$--$L$ and $\rho$--$L$ scalings, yields a characteristic $E(k) \propto k^{-1.52}$. These findings indicate the potential significance of gravity, turbulence, and possibly magnetic fields all in regulating dynamics of molecular clouds and high-mass star formation therein.",8,5,2024
Controlling Borda Elections by Adding or Deleting either Votes or Candidates: Complete and Top-Truncated Votes,"An election is defined as a pair of a set of candidates C=\{c_1,\cdots,c_m\} and a multiset of votes V=\{v_1,\cdots,v_n\}, where each vote is a linear order of the candidates. The Borda election rule is characterized by a vector \langle m-1,m-2,\cdots,0\rangle, which means that the candidate ranked at the i-th position of a vote v receives a score m-i from v, and the candidate receiving the most score from all votes wins the election. Here, we consider the control problems of a Borda election, where the chair of the election attempts to influence the election outcome by adding or deleting either votes or candidates with the intention to make a special candidate win (constructive control) or lose (destructive control) the election. Control problems have been extensively studied for Borda elections from both classical and parameterized complexity viewpoints. We complete the parameterized complexity picture for Borda control problems by showing W[2]-hardness with the number of additions/deletions as parameter for constructive control by deleting votes, adding candidates, or deleting candidates. The hardness result for deleting votes settles an open problem posed by Liu and Zhu. Following the suggestion by Menon and Larson, we also investigate the impact of introducing top-truncated votes, where each voter ranks only t out of the given m candidates, on the classical and parameterized complexity of Borda control problems. Constructive Borda control problems remain NP-hard even with t being a small constant. Moreover, we prove that in the top-truncated case, constructive control by adding/deleting votes problems are FPT with the number \ell of additions/deletions and t as parameters, while for every constant t\geq 2, constructive control by adding/deleting candidates problems are W[2]-hard with respect to \ell.",8,5,2024
Impact of Tone-Aware Explanations in Recommender Systems,"In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.",8,5,2024
Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models,"Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems. In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals. The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model. We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task. Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design.",8,5,2024
G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric Information,"Localization in already mapped environments is a critical component in many robotics and automotive applications, where previously acquired information can be exploited along with sensor fusion to provide robust and accurate localization estimates. In this work, we offer a new perspective on map-based localization by reusing prior topological and metric information. Thus, we reformulate this long-studied problem to go beyond the mere use of metric maps. Our framework seamlessly integrates LiDAR, iner\-tial and GNSS measurements, and scan-to-map registrations in a sliding window graph fashion, which allows to accommodate the uncertainty of each observation. The modularity of our framework allows it to work with different sensor configurations (\textit{e.g.,} LiDAR resolutions, GNSS denial) and environmental conditions (\textit{e.g.,} map-less regions, large environments). We have conducted different validation experiments, including deployment in a real-world automotive application, demonstrating the accuracy, efficiency, and versatility of our system in online localization.",8,5,2024
Quality assurance of actuators for the Medium-Sized Telescopes of the Cherenkov Telescope Array,"The Cherenkov Telescope Array (CTA) is a future ground-based observatory for gamma-ray astronomy providing unparalleled sensitivity in the energy range from 20 GeV up to 300 TeV. CTA will consist of telescopes with three different sizes. The Medium-Sized Telescopes (MSTs) will have 12 m reflectors with a tessellated mirror design of 86 mirror facets each. Each mirror facet is mounted on the mirror support structure with two actuators that are adjustable in length to align the mirrors, and a freely rotating fixpoint. Image resolution and pointing accuracy constraints impose limits on the backlash and deformation of the actuators and the fixpoint under various weight and wind loads. In this contribution, the test stand to measure the backlash and deformation behaviour of actuators and fixpoints is described and the measurement procedure is explained.",8,5,2024
Real-Time Motion Detection Using Dynamic Mode Decomposition,"Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system. In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation. A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays. In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD. Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video. We apply the method to a database of test videos which emulate security footage under varying realistic conditions. Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement.",8,5,2024
Large N limit of fuzzy geometries coupled to fermions,"In this paper we present an analysis of the large N limit of a family of quartic Dirac ensembles based on (0, 1) fuzzy geometries that are coupled to fermions. These Dirac ensembles are examples of single-matrix, multi-trace matrix ensembles. Additionally, they serve as examples of integer-valued $\beta$-ensembles. Convergence of the spectral density in the large N limit for a large class of such matrix ensembles is proven, improving on existing results. The main results of this paper are the addition of the fermionic contribution in the matrix ensemble and the investigation of spectral estimators for finite dimensional spectral triples",8,5,2024
"More than 200 Globular Clusters in the Milky Way, and still no super-Solar metallicity ones","Many globular clusters (GCs) in the Milky Way (MW) have been studied in recent years, especially in hidden regions such as those of the Galactic bulge. Our main goal is to understand what we can learn if we include these new objects into the MWGC system that we know today. We catalogue 37 recently discovered GCs. We use different distributions for investigating the MWGC system: metallicity distribution (MD), luminosity function (LF), and age distribution. We first treat separately the new GCs sample from the known and well-characterised GCs. We merge these two samples, upgrading the MWGC system. We performed a comparison between our clusters sample and field star (FS) population. We find a double peaked distribution for the LF, which shows an elongated faint end tail. Considering the ""merged"" sample, the LF and the MDs display a bimodality trend. We construct the MD for the FS sample, and comparing this with that one of the GCs, we learn that a high percentage of FS show [Fe/H]$>0$, whereas we do not detect any GCs in the same metallicity range. In order to understand this inconsistency, we construct the age-metallicity diagram for both samples, noting that the old and metal-poor population (age$\geq8$ Gyr and [Fe/H]$\leq -1.0$) is represented by GCs, while the young and metal-rich population (age$<8$ Gyr and [Fe/H]$>-1.0$) corresponds to FS. From the analysis of the GC LF and MD, we can conclude that many GCs, probably those very faint, have survived strong dynamical processes, typical of the Bulge regions. We cannot exclude the possibility that some of them have been accreted during past merging events, especially the metal-poor component, whereas the metal-rich population may be related to the formation of the bulge and/or disk. Finally, the difference that we notice between the GC and FS samples should be sought in the evolutionary difference between these two stellar populations.",8,5,2024
Observation of an unconventional giant negative exchange bias effect in La$_{0.5}$Sr$_{0.5}$Co$_{0.85}$Nb$_{0.15}$O$_3$,"We find an unconventional giant negative exchange bias (EB) of $H_{\rm EB}$ = --14.1~kOe at 2~K (cooling field of 50~kOe) in the cluster spin-glass (CSG) La$_{0.5}$Sr$_{0.5}$Co$_{0.85}$Nb$_{0.15}$O$_3$ perovsikte cobaltites. The magnetic memory effect, aging measurements, and nonlineraity in specific heat capacity reveal the glassy magnetic state at low temperatures. Further, the detailed analysis of {\it ac-}magnetic susceptibility confirms the glassy state below $\sim$58~K and the obtained characteristic spin-relaxation time-scale of $\tau_0$ = 8.4$\times$10$^{-10}$~s indicates the presence of CSG. Moreover, the analysis of magnetic training effect using the classical EB relaxation model reveals that the frozen spins relax slowly as compared to the rotatable spins at the interface of antiferromagnetic/ferromagnetic (AFM/FM) regions in CSG. Interestingly, the dependence of EB parameters is found to be unconventional for cooling field $>$50~kOe as the $H\rm_{EB}$ and $M\rm_{EB}$ show decreasing trend instead of expected saturation at higher fields. This unusual nature emerges due to large negative values of intrinsic interface exchange coupling ($J_i$), i.e., --10.24$\pm$0.22~meV and --12.55$\pm$0.49~meV for the measuring fields of $\pm$50~kOe and $\pm$90~kOe, respectively, whereas the number of spins in the FM cluster ($N_{\rm FM}$) are found to be small in the range of 2.4--3.1. These obtained values of $J_i $ and $N_{\rm FM}$ indicate the dominant AFM interactions and the presence of FM clusters in the AFM matrix, respectively, which correlate well with the observed unconventional behavior of giant negative exchange bias in the present sample.",8,5,2024
Ab initio computations of strongly deformed nuclei around $^{80}$Zr,"Nuclei around $N\approx Z\approx 40$ are strongly deformed and exhibit coexistence of shapes. These phenomena have challenged nuclear models. Here we perform ab initio coupled-cluster computations of low-lying collective states and electromagnetic quadrupole transitions of the even-even nuclei $^{72}$Kr, $^{76,78}$Sr, $^{78,80}$Zr and $^{84}$Mo starting from chiral nucleon-nucleon and three-nucleon forces. Our calculations reproduce the coexistence of oblate and prolate shapes in these nuclei, yield rotational bands and strong electromagnetic transitions, but are not accurate for some observables and nuclei. These results highlight the advances and challenges of ab initio computations of heavy deformed nuclei.",8,5,2024
Variational simulation of $d$-level systems on qubit-based quantum simulators,"Current quantum simulators are primarily qubit-based, making them naturally suitable for simulating 2-level quantum systems. However, many systems in nature are inherently $d$-level, including higher spins, bosons, vibrational modes, and itinerant electrons. To simulate $d$-level systems on qubit-based quantum simulators, an encoding method is required to map the $d$-level system onto a qubit basis. Such mapping may introduce illegitimate states in the Hilbert space which makes the simulation more sophisticated. In this paper, we develop a systematic method to address the illegitimate states. In addition, we compare two different mappings, namely binary and symmetry encoding methods, and compare their performance through variational simulation of the ground state and time evolution of various many-body systems. While binary encoding is very efficient with respect to the number of qubits it cannot easily incorporate the symmetries of the original Hamiltonian in its circuit design. On the other hand, the symmetry encoding facilitates the implementation of symmetries in the circuit design, though it comes with an overhead for the number of qubits. Our analysis shows that the symmetry encoding significantly outperforms the binary encoding, despite requiring extra qubits. Their advantage is indicated by requiring fewer two-qubit gates, converging faster, and being far more resilient to Barren plateaus. We have performed variational ground state simulations of spin-1, spin-3/2, and bosonic systems as well as variational time evolution of spin-1 systems. Our proposal can be implemented on existing quantum simulators and its potential is extendable to a broad class of physical models.",8,5,2024
On the Euler characteristic of $S$-arithmetic groups,"We show that the sign of the Euler characteristic of an $S$-arithmetic subgroup of a simple $k$-group over a number field $k$ depends on the $S$-congruence completion only. Consequently, the sign is a profinite invariant for such $S$-arithmetic groups with the congruence subgroup property. This generalizes previous work of the first author with Kionke-Raimbault-Sauer.",8,5,2024
Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources,"Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases. Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited.Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl. The study analyzed the context in which various diseases are discussed alongside markers of race and gender. Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize. We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation.Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts. gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated. We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed. Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions.Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets. Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare.",8,5,2024
