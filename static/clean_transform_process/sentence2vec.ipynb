{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:33:16.983709Z","iopub.status.busy":"2024-05-10T04:33:16.983113Z","iopub.status.idle":"2024-05-10T04:33:31.112818Z","shell.execute_reply":"2024-05-10T04:33:31.111645Z","shell.execute_reply.started":"2024-05-10T04:33:16.983680Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentence-transformers\n","  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: sentence-transformers\n","Successfully installed sentence-transformers-2.7.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:33:31.115106Z","iopub.status.busy":"2024-05-10T04:33:31.114813Z","iopub.status.idle":"2024-05-10T04:34:10.951016Z","shell.execute_reply":"2024-05-10T04:34:10.949785Z","shell.execute_reply.started":"2024-05-10T04:33:31.115078Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pygraphviz\n","  Downloading pygraphviz-1.13.tar.gz (104 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: pygraphviz\n","  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pygraphviz: filename=pygraphviz-1.13-cp310-cp310-linux_x86_64.whl size=97929 sha256=c7934e83450c10c4e2225c6e68c9ebec14da59b0c925c5a4bbf422a01bd73944\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/10/6c25add1fffc368b1927252bf73b63fcb938de8f4486e23691\n","Successfully built pygraphviz\n","Installing collected packages: pygraphviz\n","Successfully installed pygraphviz-1.13\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pygraphviz"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:10.952670Z","iopub.status.busy":"2024-05-10T04:34:10.952382Z","iopub.status.idle":"2024-05-10T04:34:18.345236Z","shell.execute_reply":"2024-05-10T04:34:18.344393Z","shell.execute_reply.started":"2024-05-10T04:34:10.952642Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import numpy as np\n","from sentence_transformers import SentenceTransformer\n","import pandas as pd\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer\n","nltk.download(\"stopwords\")\n","from sklearn.metrics.pairwise import cosine_similarity\n","import networkx as nx\n","import community as community_louvain\n","import pickle \n","import os\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:18.348036Z","iopub.status.busy":"2024-05-10T04:34:18.347539Z","iopub.status.idle":"2024-05-10T04:34:29.185572Z","shell.execute_reply":"2024-05-10T04:34:29.184693Z","shell.execute_reply.started":"2024-05-10T04:34:18.348008Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"122e61551085480195b0c03aed7327f6","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83c9c6a8bd5d492b8227ec36d085e52e","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca8f4eac9835426398fc57b0634195c5","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/65.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a224859bf23241488dafba24f073991e","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e79f3f0129c345b3957cf8921b639d81","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ea7774b1e1e42c38dd81cc6ff1f84df","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f8ff9cfb737462eb8157eb892471f92","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12c8e5b4f38e4de09811dff74771caa2","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45cceb865e23410990fbe56031b97741","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ceb454894a134f048f35aaa86aea7b41","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e2fdfb3553e42f1b28f6adb476e9be4","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = SentenceTransformer('WhereIsAI/UAE-Large-V1')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:29.187130Z","iopub.status.busy":"2024-05-10T04:34:29.186634Z","iopub.status.idle":"2024-05-10T04:34:31.555593Z","shell.execute_reply":"2024-05-10T04:34:31.554465Z","shell.execute_reply.started":"2024-05-10T04:34:29.187104Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"../data/main_file.csv\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:31.591460Z","iopub.status.busy":"2024-05-10T04:34:31.591034Z","iopub.status.idle":"2024-05-10T04:34:31.659217Z","shell.execute_reply":"2024-05-10T04:34:31.658411Z","shell.execute_reply.started":"2024-05-10T04:34:31.591424Z"},"trusted":true},"outputs":[],"source":["df_2018 = df.loc[df.year == 2018].dropna(subset=['abstract'])\n","df_2019 = df.loc[df.year == 2019].dropna(subset=['abstract'])\n","df_2020 = df.loc[df.year == 2020].dropna(subset=['abstract'])\n","df_2021 = df.loc[df.year == 2021].dropna(subset=['abstract'])\n","df_2022 = df.loc[df.year == 2022].dropna(subset=['abstract'])\n","df_2023 = df.loc[df.year == 2023].dropna(subset=['abstract'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:31.660740Z","iopub.status.busy":"2024-05-10T04:34:31.660400Z","iopub.status.idle":"2024-05-10T04:34:31.669231Z","shell.execute_reply":"2024-05-10T04:34:31.668176Z","shell.execute_reply.started":"2024-05-10T04:34:31.660713Z"},"trusted":true},"outputs":[],"source":["def text_preprocessing(s):\n","    \"\"\"\n","    - Lowercase the sentence\n","    - Change \"'t\" to \"not\"\n","    - Remove \"@name\"\n","    - Isolate and remove punctuations except \"?\"\n","    - Remove other special characters\n","    - Remove stop words except \"not\" and \"can\"\n","    - Remove trailing whitespace\n","    \"\"\"\n","    s = s.lower()\n","    # Change 't to 'not'\n","    s = re.sub(r\"\\'t\", \" not\", s)\n","    # Remove @name\n","    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n","    # Isolate and remove punctuations except '?'\n","    s = re.sub(r'([\\'\\\"\\!\\?\\\\/\\,])', r' \\1 ', s)\n","    s = re.sub(r'[^\\w\\s\\?\\.\\']', ' ', s)\n","    # Remove number\n","    s = re.sub(r'[0-9]', '', s)\n","    # Remove some special characters\n","    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n","    # Remove stopwords except 'not' and 'can'\n","    s = \" \".join([word for word in s.split()\n","                  if word not in stopwords.words('english')\n","                  or word in ['not', 'can']])\n","    # Remove trailing whitespace\n","    s = re.sub(r'\\s+', ' ', s).strip()\n","\n","    return s"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:31.670891Z","iopub.status.busy":"2024-05-10T04:34:31.670631Z","iopub.status.idle":"2024-05-10T04:34:31.683368Z","shell.execute_reply":"2024-05-10T04:34:31.682442Z","shell.execute_reply.started":"2024-05-10T04:34:31.670869Z"},"trusted":true},"outputs":[],"source":["clean_cols = ['abstract']\n","\n","txt_pipe = Pipeline([('clean_text', FunctionTransformer(lambda x: x.applymap(text_preprocessing)))])\n","\n","col_trans = ColumnTransformer(transformers=[\n","    ('txt_pipe', txt_pipe, clean_cols),\n","    ], )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T04:34:31.686862Z","iopub.status.busy":"2024-05-10T04:34:31.686321Z","iopub.status.idle":"2024-05-10T04:54:39.386639Z","shell.execute_reply":"2024-05-10T04:54:39.385588Z","shell.execute_reply.started":"2024-05-10T04:34:31.686837Z"},"trusted":true},"outputs":[],"source":["os.makedirs('graphs_info', exist_ok=True)\n","for y in ['2018', '2019', '2020', '2021', '2022', '2023']:\n","    folder_name = f'../../data/{y}'\n","    os.makedirs(folder_name, exist_ok=True)\n","    \n","    year_df = eval(f'df_{y}')\n","    year_df[clean_cols] = col_trans.fit_transform(year_df)\n","    words = year_df.abstract.values\n","    vectors = model.encode(words)\n","    year_df['abs_vector'] = vectors.tolist()\n","    \n","    year_df.to_csv(os.path.join(folder_name, f'{y}_paper_info.csv'))\n","    \n","    for i in range(1,13):\n","        rows_month_i = year_df.loc[year_df.month==i]\n","        vector = list(rows_month_i['abs_vector'].values)\n","        index_vector = rows_month_i.index\n","        if (len(vector) == 0 ): break\n","        sims = cosine_similarity(vector, vector)\n","        for j in range(len(vector)):\n","            for k in range(len(vector)):\n","                if j<=k:\n","                    sims[j, k] = False\n","        indices = np.argwhere(sims > 0.65)\n","        print(f'indices shape = {indices.shape}')\n","        tmp_df = pd.DataFrame(columns=['target', 'source', 'weight'])\n","        G = nx.Graph()\n","\n","        for index in indices:\n","            target = index_vector[index[0]]\n","            source = index_vector[index[1]]\n","            weight = sims[index[0], index[1]]\n","            app_df = pd.DataFrame({'target': [target], 'source': [source], 'weight': [weight]})\n","            tmp_df = pd.concat([tmp_df, app_df])\n","            G.add_edge(target, source , weight=weight)\n","\n","        pos = nx.spring_layout(G,dim=3, seed=123)\n","        partition = community_louvain.best_partition(G)\n","        \n","        set_partition = set(partition.values())\n","        key_list = np.array(words[list(partition.keys())])\n","        val_list = np.array(list(partition.values()))\n","        partition_nodes = {pid: key_list[np.where(val_list ==pid)] for pid in set_partition}\n","        keywords_group = dict()\n","        tf_idf = TfidfVectorizer()\n","        top_n = 5\n","        for pid in set_partition :\n","            output = tf_idf.fit_transform(partition_nodes[pid])\n","            feature_names = tf_idf.get_feature_names_out()\n","            tfidf_scores = output.max(0).toarray()[0]\n","            important_words = {word: score for word, score in zip(feature_names, tfidf_scores)}\n","            important_words_sorted = dict(sorted(important_words.items(), key=lambda x: x[1], reverse=True))\n","            annotation_text = \"/ \".join(list(important_words_sorted.keys())[:top_n])\n","            keywords_group[pid] = annotation_text\n","        \n","        dc = nx.degree_centrality(G)\n","        bc = nx.betweenness_centrality(G)\n","\n","        folder_name = f'../../data/{y}/{i}_month'\n","        os.makedirs(folder_name, exist_ok=True)\n","        with open(os.path.join(folder_name, f'topics.pkl'), 'wb') as f:\n","            pickle.dump(keywords_group, f)        \n","        with open(os.path.join(folder_name, f'pos.pkl'), 'wb') as f:\n","            pickle.dump(pos, f)\n","        with open(os.path.join(folder_name, f'partition.pkl'), 'wb') as f:\n","            pickle.dump(partition, f)\n","        with open(os.path.join(folder_name, f'degree_centrality.pkl'), 'wb') as f:\n","            pickle.dump(dc, f)\n","        with open(os.path.join(folder_name, f'between_centrality.pkl'), 'wb') as f:\n","            pickle.dump(bc, f)\n","        tmp_df.to_csv(os.path.join(folder_name, f'graph.csv'))\n","    print(f'Done year {y}')\n","    \n","print(f'All Done!!')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4948246,"sourceId":8369243,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
