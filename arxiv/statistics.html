<div id="dlpage">
    <h1>Statistics </h1>
    <h2>New submissions</h2>
    <div class="list-dateline">Submissions received from Fri 3 May 24 to Mon 6 May 24, announced Tue, 7 May 24</div>
    <ul>
        <li><a href="/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
        <li><a href="#item39">Cross-lists</a></li>
        <li><a href="#item61">Replacements</a></li>
    </ul>
    <small>[ total of 110 entries: <b>1-110</b> ]</small><br>
    <small>[ showing up to 2000 entries per page: <a href="/list/stat/new?skip=0&amp;show=1000">fewer</a> | <font
            color="#999999">more</font> ]</small><br>
    <h3>New submissions for Tue, 7 May 24</h3>
    <dl>
        <dt><a name="item1">[1]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02315"
                    title="Abstract">arXiv:2405.02315</a> [<a href="/pdf/2405.02315" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02315" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Regime Identification for Improving Causal Analysis in
                    Non-stationary Timeseries
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Ahmad%2C+W">Wasim Ahmad</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Shadaydeh%2C+M">Maha Shadaydeh</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Denzler%2C+J">Joachim Denzler</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
                <p class="mathjax">Time series data from real-world systems often display non-stationary
                    behavior, indicating varying statistical characteristics over time. This
                    inherent variability poses significant challenges in deciphering the underlying
                    structural relationships within the data, particularly in correlation and
                    causality analyses, model stability, etc. Recognizing distinct segments or
                    regimes within multivariate time series data, characterized by relatively
                    stable behavior and consistent statistical properties over extended periods,
                    becomes crucial. In this study, we apply the regime identification (RegID)
                    technique to multivariate time series, fundamentally designed to unveil locally
                    stationary segments within data. The distinguishing features between regimes
                    are identified using covariance matrices in a Riemannian space. We aim to
                    highlight how regime identification contributes to improving the discovery of
                    causal structures from multivariate non-stationary time series data. Our
                    experiments, encompassing both synthetic and real-world datasets, highlight the
                    effectiveness of regime-wise time series causal analysis. We validate our
                    approach by first demonstrating improved causal structure discovery using
                    synthetic data where the ground truth causal relationships are known.
                    Subsequently, we apply this methodology to climate-ecosystem dataset,
                    showcasing its applicability in real-world scenarios.
                </p>
            </div>
        </dd>
        <dt><a name="item2">[2]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02319"
                    title="Abstract">arXiv:2405.02319</a> [<a href="/pdf/2405.02319" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02319" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Bayesian Inference for Estimating Heat Sources through
                    Temperature Assimilation
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Mousavi%2C+H">Hanieh Mousavi</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Eldredge%2C+J+D">Jeff D. Eldredge</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">This paper introduces a Bayesian inference framework for two-dimensional
                    steady-state heat conduction, focusing on the estimation of unknown distributed
                    heat sources in a thermally-conducting medium with uniform conductivity. The
                    goal is to infer heater locations, strengths, and shapes using temperature
                    assimilation in the Euclidean space, employing a Fourier series to represent
                    each heater's shape. The Markov Chain Monte Carlo (MCMC) method, incorporating
                    the random-walk Metropolis-Hasting algorithm and parallel tempering, is
                    utilized for posterior distribution exploration in both unbounded and
                    wall-bounded domains. Strong correlations between heat strength and heater area
                    prompt caution against simultaneously estimating these two quantities. It is
                    found that multiple solutions arise in cases where the number of temperature
                    sensors is less than the number of unknown states. Moreover, smaller heaters
                    introduce greater uncertainty in estimated strength. The diffusive nature of
                    heat conduction smooths out any deformations in the temperature contours,
                    especially in the presence of multiple heaters positioned near each other,
                    impacting convergence. In wall-bounded domains with Neumann boundary
                    conditions, the inference of heater parameters tends to be more accurate than
                    in unbounded domains.
                </p>
            </div>
        </dd>
        <dt><a name="item3">[3]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02322"
                    title="Abstract">arXiv:2405.02322</a> [<a href="/pdf/2405.02322" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02322" title="Download PostScript">ps</a>, <a href="/format/2405.02322"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Towards Causal Interpretation of Sexual Orientation in
                    Regression Analysis: Applications and Challenges
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Lu%2C+J">Junjie Lu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Guo%2C+Z">Zhongyi Guo</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Rehkopf%2C+D+H">David H. Rehkopf</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
                <p class="mathjax">This study presents an approach to analyze health disparities in Sexual and
                    Gender Minority (SGM) populations, with a focus on the role of social support
                    levels as an example to allow causal interpretations of regression models. We
                    advocate for precisely defining the exposure variable and incorporating
                    mediators into analyses, to address the limitations of comparing counterfactual
                    outcomes solely between SGM and heterosexual populations. We define sexual
                    orientation into domains (attraction, behavior, and identity), and emphasize a
                    consideration of these elements either separately or together, depending on the
                    research question. We also introduce social support measured before and after
                    the disclosure of sexual orientation to facilitate inference. We illustrate
                    this approach by examining the association between SGM status and depression
                    diagnosis with data from the 2020 and 2021 National Health Interview Survey. We
                    find a direct effect of SGM status on depression (OR: 3.07, 95% CI: 2.64 -
                    3.58) and no indirect effect through social support (OR: 1.07, 95% CI:
                    0.87-1.31). Our research emphasizes the necessity of the comprehensive
                    measurement of sexual orientation and a focus on intervenable variables like
                    social support in order to empower SGM communities and address SGM related
                    health inequalities.
                </p>
            </div>
        </dd>
        <dt><a name="item4">[4]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02340"
                    title="Abstract">arXiv:2405.02340</a> [<a href="/pdf/2405.02340" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02340" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A Comprehensive Approach to Carbon Dioxide Emission Analysis
                    in High Human Development Index Countries using Statistical and Machine Learning Techniques
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Khosravi%2C+H">Hamed Khosravi</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Raihan%2C+A+S">Ahmed Shoyeb Raihan</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Islam%2C+F">Farzana Islam</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Nimbarte%2C+A">Ashish Nimbarte</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Ahmed%2C+I">Imtiaz Ahmed</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>; Machine Learning (cs.LG)

                </div>
                <p class="mathjax">Reducing Carbon dioxide (CO2) emission is vital at both global and national
                    levels, given their significant role in exacerbating climate change. CO2
                    emission, stemming from a variety of industrial and economic activities, are
                    major contributors to the greenhouse effect and global warming, posing
                    substantial obstacles in addressing climate issues. It's imperative to forecast
                    CO2 emission trends and classify countries based on their emission patterns to
                    effectively mitigate worldwide carbon emission. This paper presents an in-depth
                    comparative study on the determinants of CO2 emission in twenty countries with
                    high Human Development Index (HDI), exploring factors related to economy,
                    environment, energy use, and renewable resources over a span of 25 years. The
                    study unfolds in two distinct phases: initially, statistical techniques such as
                    Ordinary Least Squares (OLS), fixed effects, and random effects models are
                    applied to pinpoint significant determinants of CO2 emission. Following this,
                    the study leverages supervised and unsupervised machine learning (ML) methods
                    to further scrutinize and understand the factors influencing CO2 emission.
                    Seasonal AutoRegressive Integrated Moving Average with eXogenous variables
                    (SARIMAX), a supervised ML model, is first used to predict emission trends from
                    historical data, offering practical insights for policy formulation.
                    Subsequently, Dynamic Time Warping (DTW), an unsupervised learning approach, is
                    used to group countries by similar emission patterns. The dual-phase approach
                    utilized in this study significantly improves the accuracy of CO2 emission
                    predictions while also providing a deeper insight into global emission trends.
                    By adopting this thorough analytical framework, nations can develop more
                    focused and effective carbon reduction policies, playing a vital role in the
                    global initiative to combat climate change.
                </p>
            </div>
        </dd>
        <dt><a name="item5">[5]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02343"
                    title="Abstract">arXiv:2405.02343</a> [<a href="/pdf/2405.02343" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02343" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Rejoinder on "Marked spatial point processes: current state
                    and extensions to point processes on linear networks"
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Eckardt%2C+M">Matthias Eckardt</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Moradi%2C+M">Mehdi Moradi</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Response to commentary on <a
                        href="/abs/2309.01511">arXiv:2309.01511</a>
                </div>
                <div class="list-journal-ref">
                    <span class="descriptor">Journal-ref:</span> Eckardt, M. and Moradi, M. (2024). Rejoinder on `Marked
                    Spatial
                    Point Processes: Current State and Extensions to Point Processes on Linear
                    Networks. Journal of Agricultural, Biological and Environmental Statistics
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Applications (stat.AP)

                </div>
                <p class="mathjax">We are grateful to all discussants for their invaluable comments,
                    suggestions, questions, and contributions to our article. We have attentively
                    reviewed all discussions with keen interest. In this rejoinder, our objective
                    is to address and engage with all points raised by the discussants in a
                    comprehensive and considerate manner. Consistently, we identify the
                    discussants, in alphabetical order, as follows: CJK for Cronie, Jansson, and
                    Konstantinou, DS for Stoyan, GP for Grabarnik and Pommerening, MRS for
                    Myllym\"aki, Rajala, and S\"arkk\"a, and MCvL for van Lieshout throughout this
                    rejoinder.
                </p>
            </div>
        </dd>
        <dt><a name="item6">[6]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02372"
                    title="Abstract">arXiv:2405.02372</a> [<a href="/pdf/2405.02372" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02372" title="Download PostScript">ps</a>, <a href="/format/2405.02372"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Triadic-OCD: Asynchronous Online Change Detection with
                    Provable Robustness, Optimality, and Convergence
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Huang%2C+Y">Yancheng Huang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Yang%2C+K">Kai Yang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhu%2C+Z">Zelin Zhu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Chen%2C+L">Leian Chen</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted at ICML2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

                </div>
                <p class="mathjax">The primary goal of online change detection (OCD) is to promptly identify
                    changes in the data stream. OCD problem find a wide variety of applications in
                    diverse areas, e.g., security detection in smart grids and intrusion detection
                    in communication networks. Prior research usually assumes precise knowledge of
                    the parameters linked to the data stream. Nevertheless, this presumption often
                    proves unattainable in practical scenarios due to factors such as estimation
                    errors, system updates, etc. This paper aims to take the first attempt to
                    develop a triadic-OCD framework with certifiable robustness, provable
                    optimality, and guaranteed convergence. In addition, the proposed triadic-OCD
                    algorithm can be realized in a fully asynchronous distributed manner, easing
                    the necessity of transmitting the data to a single server. This asynchronous
                    mechanism also could mitigate the straggler issue that faced by traditional
                    synchronous algorithm. We then analyze the non-asymptotic convergence property
                    of triadic-OCD and derive its iteration complexity to achieve an
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-1-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-1"
                                style="width: 0.524em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.408em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.334em, 1000.41em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3"
                                                style="font-family: MathJax_Math-italic;">ϵ</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-1">\epsilon</script>-optimal point. Finally, extensive
                    experiments have been conducted to
                    elucidate the effectiveness of the proposed method.
                </p>
            </div>
        </dd>
        <dt><a name="item7">[7]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02383"
                    title="Abstract">arXiv:2405.02383</a> [<a href="/pdf/2405.02383" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02383" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A Fresh Look at Sanity Checks for Saliency Maps
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Hedstr%C3%B6m%2C+A">Anna Hedström</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Weber%2C+L">Leander Weber</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=H%C3%B6hne%2C+M">Marina Höhne</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a
                        href="/abs/2401.06465">arXiv:2401.06465</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition
                    (cs.CV); Machine Learning (cs.LG)

                </div>
                <p class="mathjax">The Model Parameter Randomisation Test (MPRT) is highly recognised in the
                    eXplainable Artificial Intelligence (XAI) community due to its fundamental
                    evaluative criterion: explanations should be sensitive to the parameters of the
                    model they seek to explain. However, recent studies have raised several
                    methodological concerns for the empirical interpretation of MPRT. In response,
                    we propose two modifications to the original test: Smooth MPRT and Efficient
                    MPRT. The former reduces the impact of noise on evaluation outcomes via
                    sampling, while the latter avoids the need for biased similarity measurements
                    by re-interpreting the test through the increase in explanation complexity
                    after full model randomisation. Our experiments show that these modifications
                    enhance the metric reliability, facilitating a more trustworthy deployment of
                    explanation methods.
                </p>
            </div>
        </dd>
        <dt><a name="item8">[8]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02449"
                    title="Abstract">arXiv:2405.02449</a> [<a href="/pdf/2405.02449" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02449" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Quality-Weighted Vendi Scores And Their Application To
                    Diverse Experimental Design
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Nguyen%2C+Q">Quan Nguyen</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Dieng%2C+A+B">Adji Bousso Dieng</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Published in International Conference on Machine Learning,
                    ICML 2024. Code can be found in the Vertaix GitHub: <a
                        href="https://github.com/vertaix/Quality-Weighted-Vendi-Score.">this https URL</a> Paper
                    dedicated to Kwame Nkrumah
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Biomolecules
                    (q-bio.BM)

                </div>
                <p class="mathjax">Experimental design techniques such as active search and Bayesian
                    optimization are widely used in the natural sciences for data collection and
                    discovery. However, existing techniques tend to favor exploitation over
                    exploration of the search space, which causes them to get stuck in local
                    optima. This ``collapse" problem prevents experimental design algorithms from
                    yielding diverse high-quality data. In this paper, we extend the Vendi scores
                    -- a family of interpretable similarity-based diversity metrics -- to account
                    for quality. We then leverage these quality-weighted Vendi scores to tackle
                    experimental design problems across various applications, including drug
                    discovery, materials discovery, and reinforcement learning. We found that
                    quality-weighted Vendi scores allow us to construct policies for experimental
                    design that flexibly balance quality and diversity, and ultimately assemble
                    rich and diverse sets of high-performing data points. Our algorithms led to a
                    70%-170% increase in the number of effective discoveries compared to baselines.
                </p>
            </div>
        </dd>
        <dt><a name="item9">[9]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02462"
                    title="Abstract">arXiv:2405.02462</a> [<a href="/pdf/2405.02462" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02462" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Finite Sample Analysis and Bounds of Generalization Error of
                    Gradient Descent in In-Context Linear Regression
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Duraisamy%2C+K">Karthik Duraisamy</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Numerical Analysis (math.NA); Probability (math.PR)

                </div>
                <p class="mathjax">Recent studies show that transformer-based architectures emulate gradient
                    descent during a forward pass, contributing to in-context learning capabilities
                    - an ability where the model adapts to new tasks based on a sequence of prompt
                    examples without being explicitly trained or fine tuned to do so. This work
                    investigates the generalization properties of a single step of gradient descent
                    in the context of linear regression with well-specified models. A random design
                    setting is considered and analytical expressions are derived for the
                    statistical properties of generalization error in a non-asymptotic (finite
                    sample) setting. These expressions are notable for avoiding arbitrary
                    constants, and thus offer robust quantitative information and scaling
                    relationships. These results are contrasted with those from classical least
                    squares regression (for which analogous finite sample bounds are also derived),
                    shedding light on systematic and noise components, as well as optimal step
                    sizes. Additionally, identities involving high-order products of Gaussian
                    random matrices are presented as a byproduct of the analysis.
                </p>
            </div>
        </dd>
        <dt><a name="item10">[10]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02488"
                    title="Abstract">arXiv:2405.02488</a> [<a href="/pdf/2405.02488" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02488" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Modelling Sampling Distributions of Test Statistics with
                    Autograd
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Kadhim%2C+A+A">Ali Al Kadhim</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Prosper%2C+H+B">Harrison B. Prosper</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex);
                    Computation (stat.CO)

                </div>
                <p class="mathjax">Simulation-based inference methods that feature correct conditional coverage
                    of confidence sets based on observations that have been compressed to a scalar
                    test statistic require accurate modelling of either the p-value function or the
                    cumulative distribution function (cdf) of the test statistic. If the model of
                    the cdf, which is typically a deep neural network, is a function of the test
                    statistic then the derivative of the neural network with respect to the test
                    statistic furnishes an approximation of the sampling distribution of the test
                    statistic. We explore whether this approach to modelling conditional
                    1-dimensional sampling distributions is a viable alternative to the probability
                    density-ratio method, also known as the likelihood-ratio trick. Relatively
                    simple, yet effective, neural network models are used whose predictive
                    uncertainty is quantified through a variety of methods.
                </p>
            </div>
        </dd>
        <dt><a name="item11">[11]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02498"
                    title="Abstract">arXiv:2405.02498</a> [<a href="/pdf/2405.02498" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02498" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Multimatrix variate distributions
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=D%C3%ADaz-Garc%C3%ADa%2C+J+A">José A.
                        Díaz-García</a>,
                    <a href="/search/math?searchtype=author&amp;query=Caro-Lopera%2C+F+J">Francisco J. Caro-Lopera</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 14 pages
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>

                </div>
                <p class="mathjax">A new family of distributions indexed by the class of matrix variate
                    contoured elliptically distribution is proposed as an extension of some
                    bimatrix variate distributions. The termed \emph{multimatrix variate
                    distributions} open new perspectives for the classical distribution theory,
                    usually based on probabilistic independent models and preferred untested
                    fitting laws. Most of the multimatrix models here derived are invariant under
                    the spherical family, a fact that solves the testing and prior knowledge of the
                    underlying distributions and elucidates the statistical methodology in
                    contrasts with some weakness of current studies as copulas. The paper also
                    includes a number of diverse special cases, properties and generalisations. The
                    new joint distributions allows several unthinkable combinations for copulas,
                    such as scalars, vectors and matrices, all of them adjustable to the required
                    models of the experts. The proposed joint distributions are also easily
                    computable, then several applications are plausible. In particular, an
                    exhaustive example in molecular docking on SARS-CoV-2 presents the results on
                    matrix dependent samples.
                </p>
            </div>
        </dd>
        <dt><a name="item12">[12]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02529"
                    title="Abstract">arXiv:2405.02529</a> [<a href="/pdf/2405.02529" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02529" title="Download PostScript">ps</a>, <a href="/format/2405.02529"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Chauhan Weighted Trajectory Analysis reduces sample size
                    requirements and expedites time-to-efficacy signals in advanced cancer clinical trials
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Chauhan%2C+U">Utkarsh Chauhan</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Mackey%2C+D">Daylen Mackey</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Mackey%2C+J+R">John R. Mackey</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints,
                    most advanced cancer randomized clinical trials (RCTs) are powered for either
                    progression free survival (PFS) or overall survival (OS). This discards
                    efficacy information carried by partial responses, complete responses, and
                    stable disease that frequently precede progressive disease and death. Chauhan
                    Weighted Trajectory Analysis (CWTA) is a generalization of KM that
                    simultaneously assesses multiple rank-ordered endpoints. We hypothesized that
                    CWTA could use this efficacy information to reduce sample size requirements and
                    expedite efficacy signals in advanced cancer trials. We performed 100-fold and
                    1000-fold simulations of solid tumour systemic therapy RCTs with health
                    statuses rank ordered from complete response (Stage 0) to death (Stage 4). At
                    increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA
                    for (i) sample size requirements to achieve a power of 0.8 and (ii)
                    time-to-first significant efficacy signal. CWTA consistently demonstrated
                    greater power, and reduced sample size requirements by 18% to 35% compared to
                    KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy
                    signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer
                    treatment trajectory, provides clinically relevant reduction in required sample
                    size and meaningfully expedites the efficacy signals of cancer treatments
                    compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial
                    outcome has the potential to meaningfully reduce the numbers of patients, trial
                    duration, and costs to evaluate therapies in advanced cancer.
                </p>
            </div>
        </dd>
        <dt><a name="item13">[13]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02539"
                    title="Abstract">arXiv:2405.02539</a> [<a href="/pdf/2405.02539" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02539" title="Download PostScript">ps</a>, <a href="/format/2405.02539"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Distributed Iterative Hard Thresholding for Variable
                    Selection in Tobit Models
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Yang%2C+C">Changxin Yang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhu%2C+Z">Zhongyi Zhu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Lian%2C+H">Heng Lian</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">While extensive research has been conducted on high-dimensional data and on
                    regression with left-censored responses, simultaneously addressing these
                    complexities remains challenging, with only a few proposed methods available.
                    In this paper, we utilize the Iterative Hard Thresholding (IHT) algorithm on
                    the Tobit model in such a setting. Theoretical analysis demonstrates that our
                    estimator converges with a near-optimal minimax rate. Additionally, we extend
                    the method to a distributed setting, requiring only a few rounds of
                    communication while retaining the estimation rate of the centralized version.
                    Simulation results show that the IHT algorithm for the Tobit model achieves
                    superior accuracy in predictions and subset selection, with the distributed
                    estimator closely matching that of the centralized estimator. When applied to
                    high-dimensional left-censored HIV viral load data, our method also exhibits
                    similar superiority.
                </p>
            </div>
        </dd>
        <dt><a name="item14">[14]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02551"
                    title="Abstract">arXiv:2405.02551</a> [<a href="/pdf/2405.02551" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02551" title="Download PostScript">ps</a>, <a href="/format/2405.02551"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Power-Enhanced Two-Sample Mean Tests for High-Dimensional
                    Compositional Data with Application to Microbiome Data Analysis
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Li%2C+D">Danning Li</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Xue%2C+L">Lingzhou Xue</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Yang%2C+H">Haoyi Yang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Yu%2C+X">Xiufan Yu</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 25 pages
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Statistics Theory (math.ST); Applications (stat.AP)

                </div>
                <p class="mathjax">Testing differences in mean vectors is a fundamental task in the analysis of
                    high-dimensional compositional data. Existing methods may suffer from low power
                    if the underlying signal pattern is in a situation that does not favor the
                    deployed test. In this work, we develop two-sample power-enhanced mean tests
                    for high-dimensional compositional data based on the combination of <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-4"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.52em, 2.26em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-2">p</script>-values,
                    which integrates strengths from two popular types of tests: the maximum-type
                    test and the quadratic-type test. We provide rigorous theoretical guarantees on
                    the proposed tests, showing accurate Type-I error rate control and enhanced
                    testing power. Our method boosts the testing power towards a broader
                    alternative space, which yields robust performance across a wide range of
                    signal pattern settings. Our theory also contributes to the literature on power
                    enhancement and Gaussian approximation for high-dimensional hypothesis testing.
                    We demonstrate the performance of our method on both simulated data and
                    real-world microbiome data, showing that our proposed approach improves the
                    testing power substantially compared to existing methods.
                </p>
            </div>
        </dd>
        <dt><a name="item15">[15]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02666"
                    title="Abstract">arXiv:2405.02666</a> [<a href="/pdf/2405.02666" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02666" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> The Analysis of Criminal Recidivism: A Hierarchical
                    Model-Based Approach for the Analysis of Zero-Inflated, Spatially Correlated recurrent events Data
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Silva%2C+A+C+C">Alisson C. C. Silva</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Demarqui%2C+F+N">Fábio N. Demarqui</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Silva%2C+B+F">Bráulio F. Silva</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Prates%2C+M+O">Marcos O. Prates</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 23 pages, 12 figuras and 4 tables
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">The life course perspective in criminology has become prominent last years,
                    offering valuable insights into various patterns of criminal offending and
                    pathways. The study of criminal trajectories aims to understand the beginning,
                    persistence and desistence in crime, providing intriguing explanations about
                    these moments in life. Central to this analysis is the identification of
                    patterns in the frequency of criminal victimization and recidivism, along with
                    the factors that contribute to them. Specifically, this work introduces a new
                    class of models that overcome limitations in traditional methods used to
                    analyze criminal recidivism. These models are designed for recurrent events
                    data characterized by excess of zeros and spatial correlation. They extend the
                    Non-Homogeneous Poisson Process, incorporating spatial dependence in the model
                    through random effects, enabling the analysis of associations among individuals
                    within the same spatial stratum. To deal with the excess of zeros in the data,
                    a zero-inflated Poisson mixed model was incorporated. In addition to parametric
                    models following the Power Law process for baseline intensity functions, we
                    propose flexible semi-parametric versions approximating the intensity function
                    using Bernstein Polynomials. The Bayesian approach offers advantages such as
                    incorporating external evidence and modeling specific correlations between
                    random effects and observed data. The performance of these models was evaluated
                    in a simulation study with various scenarios, and we applied them to analyze
                    criminal recidivism data in the Metropolitan Region of Belo Horizonte, Brazil.
                    The results provide a detailed analysis of high-risk areas for recurrent crimes
                    and the behavior of recidivism rates over time. This research significantly
                    enhances our understanding of criminal trajectories, paving the way for more
                    effective strategies in combating criminal recidivism.
                </p>
            </div>
        </dd>
        <dt><a name="item16">[16]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02715"
                    title="Abstract">arXiv:2405.02715</a> [<a href="/pdf/2405.02715" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02715" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Grouping predictors via network-wide metrics
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Park%2C+B+W">Brandon Woosuk Park</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Vidyashankar%2C+A+N">Anand N. Vidyashankar</a>,
                    <a href="/search/stat?searchtype=author&amp;query=McElroy%2C+T+S">Tucker S. McElroy</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">When multitudes of features can plausibly be associated with a response, both
                    privacy considerations and model parsimony suggest grouping them to increase
                    the predictive power of a regression model. Specifically, the identification of
                    groups of predictors significantly associated with the response variable eases
                    further downstream analysis and decision-making. This paper proposes a new data
                    analysis methodology that utilizes the high-dimensional predictor space to
                    construct an implicit network with weighted edges %and weights on the edges to
                    identify significant associations between the response and the predictors.
                    Using a population model for groups of predictors defined via network-wide
                    metrics, a new supervised grouping algorithm is proposed to determine the
                    correct group, with probability tending to one as the sample size diverges to
                    infinity. For this reason, we establish several theoretical properties of the
                    estimates of network-wide metrics. A novel model-assisted bootstrap procedure
                    that substantially decreases computational complexity is developed,
                    facilitating the assessment of uncertainty in the estimates of network-wide
                    metrics. The proposed methods account for several challenges that arise in the
                    high-dimensional data setting, including (i) a large number of predictors, (ii)
                    uncertainty regarding the true statistical model, and (iii) model selection
                    variability. The performance of the proposed methods is demonstrated through
                    numerical experiments, data from sports analytics, and breast cancer data.
                </p>
            </div>
        </dd>
        <dt><a name="item17">[17]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02779"
                    title="Abstract">arXiv:2405.02779</a> [<a href="/pdf/2405.02779" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02779" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Estimating Complier Average Causal Effects with Mixtures of
                    Experts
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Grolleau%2C+F">François Grolleau</a>,
                    <a href="/search/stat?searchtype=author&amp;query=B%C3%A9ji%2C+C">Céline Béji</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Petit%2C+F">François Petit</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Porcher%2C+R">Raphaël Porcher</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">Understanding the causal impact of medical interventions is essential in
                    healthcare research, especially through randomized controlled trials (RCTs).
                    Despite their prominence, challenges arise due to discrepancies between
                    treatment allocation and actual intake, influenced by various factors like
                    patient non-adherence or procedural errors. This paper focuses on the Complier
                    Average Causal Effect (CACE), crucial for evaluating treatment efficacy among
                    compliant patients. Existing methodologies often rely on assumptions such as
                    exclusion restriction and monotonicity, which can be problematic in practice.
                    We propose a novel approach, leveraging supervised learning architectures, to
                    estimate CACE without depending on these assumptions. Our method involves a
                    two-step process: first estimating compliance probabilities for patients, then
                    using these probabilities to estimate two nuisance components relevant to CACE
                    calculation. Building upon the principal ignorability assumption, we introduce
                    four root-n consistent, asymptotically normal, CACE estimators, and prove that
                    the underlying mixtures of experts' nuisance components are identifiable. Our
                    causal framework allows our estimation procedures to enjoy reduced mean squared
                    errors when exclusion restriction or monotonicity assumptions hold. Through
                    simulations and application to a breastfeeding promotion RCT, we demonstrate
                    the method's performance and applicability.
                </p>
            </div>
        </dd>
        <dt><a name="item18">[18]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02783"
                    title="Abstract">arXiv:2405.02783</a> [<a href="/pdf/2405.02783" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02783" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Linear Noise Approximation Assisted Bayesian Inference on
                    Mechanistic Model of Partially Observed Stochastic Reaction Network
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Xu%2C+W">Wandi Xu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Xie%2C+W">Wei Xie</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 11 pages, 2 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
                <p class="mathjax">To support mechanism online learning and facilitate digital twin development
                    for biomanufacturing processes, this paper develops an efficient Bayesian
                    inference approach for partially observed enzymatic stochastic reaction network
                    (SRN), a fundamental building block of multi-scale bioprocess mechanistic
                    model. To tackle the critical challenges brought by the nonlinear stochastic
                    differential equations (SDEs)-based mechanistic model with partially observed
                    state and having measurement error, an interpretable Bayesian updating linear
                    noise approximation (LNA) metamodel, incorporating the structure information of
                    the mechanistic model, is proposed to approximate the likelihood of
                    observations. Then, an efficient posterior sampling approach is developed by
                    utilizing the gradients of the derived likelihood to speed up the convergence
                    of MCMC. The empirical study demonstrates that the proposed approach has a
                    promising performance.
                </p>
            </div>
        </dd>
        <dt><a name="item19">[19]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02802"
                    title="Abstract">arXiv:2405.02802</a> [<a href="/pdf/2405.02802" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.02802" title="Download PostScript">ps</a>, <a href="/format/2405.02802"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Permutation time irreversibility in sleep
                    electroencephalograms: Dependence on sleep stage and the effect of equal values
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Yao%2C+W">Wenpo Yao</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 13 pages, 9 figures
                </div>
                <div class="list-journal-ref">
                    <span class="descriptor">Journal-ref:</span> Yao, WP. Permutation time irreversibility in sleep
                    electroencephalograms: Dependence on sleep stage and the effect of equal
                    values. Physical Review E, 2024, 109(5): 054104
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>

                </div>
                <p class="mathjax">Time irreversibility (TIR) refers to the manifestation of nonequilibrium
                    brain activity influenced by various physiological conditions; however, the
                    influence of sleep on electroencephalogram (EEG) TIR has not been sufficiently
                    investigated. In this paper, a comprehensive study on permutation TIR (pTIR) of
                    EEG data under different sleep stages is conducted. Two basic ordinal patterns
                    (i.e., the original and amplitude permutations) are distinguished to simplify
                    sleep EEGs, and then the influences of equal values and forbidden permutation
                    on pTIR are elucidated. To detect pTIR of brain electric signals, 5 groups of
                    EEGs in the awake, stages I, II, III, and rapid eye movement (REM) stages are
                    collected from the public Polysomnographic Database in PhysioNet. Test results
                    suggested that the pTIR of sleep EEGs significantly decreases as the sleep
                    stage increases (p&lt;0.001), with the awake and REM EEGs, demonstrating greater
                    differences than others. Comparative analysis and numerical simulations support
                    the importance of equal values. Distribution of equal states, a simple
                    quantification of amplitude fluctuations, significantly increases with the
                    sleep stage (p&lt;0.001). If these equalities are ignored, incorrect probabilistic
                    differences may arise in the forward-backward and symmetric permutations of
                    TIR, leading to contradictory results; moreover, the ascending and descending
                    orders for symmetric permutations also lead different outcomes in sleep EEGs.
                    Overall, pTIR in sleep EEGs contributes to our understanding of quantitative
                    TIR and classification of sleep EEGs.
                </p>
            </div>
        </dd>
        <dt><a name="item20">[20]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02871"
                    title="Abstract">arXiv:2405.02871</a> [<a href="/pdf/2405.02871" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02871" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Modeling frequency distribution above a priority in presence
                    of IBNR
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Baradel%2C+N">Nicolas Baradel</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Probability (math.PR)

                </div>
                <p class="mathjax">In reinsurance, Poisson and Negative binomial distributions are employed for
                    modeling frequency. However, the incomplete data regarding reported incurred
                    claims above a priority level presents challenges in estimation. This paper
                    focuses on frequency estimation using Schnieper's framework for claim
                    numbering. We demonstrate that Schnieper's model is consistent with a Poisson
                    distribution for the total number of claims above a priority at each year of
                    development, providing a robust basis for parameter estimation. Additionally,
                    we explain how to build an alternative assumption based on a Negative binomial
                    distribution, which yields similar results. The study includes a bootstrap
                    procedure to manage uncertainty in parameter estimation and a case study
                    comparing assumptions and evaluating the impact of the bootstrap approach.
                </p>
            </div>
        </dd>
        <dt><a name="item21">[21]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02905"
                    title="Abstract">arXiv:2405.02905</a> [<a href="/pdf/2405.02905" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02905" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Mixture of partially linear experts
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Hwang%2C+Y">Yeongsan Hwang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Seo%2C+B">Byungtae Seo</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Oh%2C+S">Sangkon Oh</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">In the mixture of experts model, a common assumption is the linearity between
                    a response variable and covariates. While this assumption has theoretical and
                    computational benefits, it may lead to suboptimal estimates by overlooking
                    potential nonlinear relationships among the variables. To address this
                    limitation, we propose a partially linear structure that incorporates
                    unspecified functions to capture nonlinear relationships. We establish the
                    identifiability of the proposed model under mild conditions and introduce a
                    practical estimation algorithm. We present the performance of our approach
                    through numerical studies, including simulations and real data analysis.
                </p>
            </div>
        </dd>
        <dt><a name="item22">[22]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02983"
                    title="Abstract">arXiv:2405.02983</a> [<a href="/pdf/2405.02983" title="Download PDF">pdf</a>, <a
                    href="/format/2405.02983" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> CVXSADes: a stochastic algorithm for constructing optimal
                    exact regression designs with single or multiple objectives
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Yeh%2C+C">Chi-Kuang Yeh</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhou%2C+J">Julie Zhou</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Applications (stat.AP); Computation (stat.CO)

                </div>
                <p class="mathjax">We propose an algorithm to construct optimal exact designs (EDs). Most of the
                    work in the optimal regression design literature focuses on the approximate
                    design (AD) paradigm due to its desired properties, including the optimality
                    verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced
                    weights, and practitioners may have difficulty implementing them with a
                    designated run size <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-7"
                                style="width: 0.697em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.582em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.58em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9"
                                                style="font-family: MathJax_Math-italic;">n</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-3">n</script>. Some EDs are constructed using rounding
                    methods to get
                    an integer number of runs at each support point of an AD, but this approach may
                    not yield optimal results. To construct EDs, one may need to perform new
                    combinatorial constructions for each <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-10"
                                style="width: 0.697em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.582em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.58em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12"
                                                style="font-family: MathJax_Math-italic;">n</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-4">n</script>, and there is no unified approach to
                    construct them. Therefore, we develop a systematic way to construct EDs for any
                    given <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-5-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-13"
                                style="width: 0.697em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.582em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.58em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15"
                                                style="font-family: MathJax_Math-italic;">n</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-5">n</script>. Our method can transform ADs into EDs
                    while retaining high
                    statistical efficiency in two steps. The first step involves constructing an AD
                    by utilizing the convex nature of many design criteria. The second step employs
                    a simulated annealing algorithm to search for the ED stochastically. Through
                    several applications, we demonstrate the utility of our method for various
                    design problems. Additionally, we show that the design efficiency approaches
                    unity as the number of design points increases.
                </p>
            </div>
        </dd>
        <dt><a name="item23">[23]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03041"
                    title="Abstract">arXiv:2405.03041</a> [<a href="/pdf/2405.03041" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03041" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Bayesian Functional Graphical Models with Change-Point
                    Detection
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Liu%2C+C">Chunshan Liu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Kowal%2C+D+R">Daniel R. Kowal</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Doss-Gollin%2C+J">James Doss-Gollin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Vannucci%2C+M">Marina Vannucci</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Revised for Computational Statistics and Data Analysis
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">Functional data analysis, which models data as realizations of random
                    functions over a continuum, has emerged as a useful tool for time series data.
                    Often, the goal is to infer the dynamic connections (or time-varying
                    conditional dependencies) among multiple functions or time series. For this
                    task, we propose a dynamic and Bayesian functional graphical model. Our
                    modeling approach prioritizes the careful definition of an appropriate graph to
                    identify both time-invariant and time-varying connectivity patterns. We
                    introduce a novel block-structured sparsity prior paired with a finite basis
                    expansion, which together yield effective shrinkage and graph selection with
                    efficient computations via a Gibbs sampling algorithm. Crucially, the model
                    includes (one or more) graph changepoints, which are learned jointly with all
                    model parameters and incorporate graph dynamics. Simulation studies demonstrate
                    excellent graph selection capabilities, with significant improvements over
                    competing methods. We apply the proposed approach to study of dynamic
                    connectivity patterns of sea surface temperatures in the Pacific Ocean and
                    discovers meaningful edges.
                </p>
            </div>
        </dd>
        <dt><a name="item24">[24]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03042"
                    title="Abstract">arXiv:2405.03042</a> [<a href="/pdf/2405.03042" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03042" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Functional Post-Clustering Selective Inference with
                    Applications to EHR Data Analysis
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Zhu%2C+Z">Zihan Zhu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Gai%2C+X">Xin Gai</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+A+R">Anru R. Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Applications (stat.AP); Computation (stat.CO)

                </div>
                <p class="mathjax">In electronic health records (EHR) analysis, clustering patients according to
                    patterns in their data is crucial for uncovering new subtypes of diseases.
                    Existing medical literature often relies on classical hypothesis testing
                    methods to test for differences in means between these clusters. Due to
                    selection bias induced by clustering algorithms, the implementation of these
                    classical methods on post-clustering data often leads to an inflated type-I
                    error. In this paper, we introduce a new statistical approach that adjusts for
                    this bias when analyzing data collected over time. Our method extends classical
                    selective inference methods for cross-sectional data to longitudinal data. We
                    provide theoretical guarantees for our approach with upper bounds on the
                    selective type-I and type-II errors. We apply the method to simulated data and
                    real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the
                    advantages of our approach.
                </p>
            </div>
        </dd>
        <dt><a name="item25">[25]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03063"
                    title="Abstract">arXiv:2405.03063</a> [<a href="/pdf/2405.03063" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03063" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Stability of a Generalized Debiased Lasso with Applications
                    to Resampling-Based Variable Selection
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Liu%2C+J">Jingbo Liu</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Methodology (stat.ME);
                    Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Suppose that we first apply the Lasso to a design matrix, and then update one
                    of its columns. In general, the signs of the Lasso coefficients may change, and
                    there is no closed-form expression for updating the Lasso solution exactly. In
                    this work, we propose an approximate formula for updating a debiased Lasso
                    coefficient. We provide general nonasymptotic error bounds in terms of the
                    norms and correlations of a given design matrix's columns, and then prove
                    asymptotic convergence results for the case of a random design matrix with
                    i.i.d.\ sub-Gaussian row vectors and i.i.d.\ Gaussian noise. Notably, the
                    approximate formula is asymptotically correct for most coordinates in the
                    proportional growth regime, under the mild assumption that each row of the
                    design matrix is sub-Gaussian with a covariance matrix having a bounded
                    condition number. Our proof only requires certain concentration and
                    anti-concentration properties to control various error terms and the number of
                    sign changes. In contrast, rigorously establishing distributional limit
                    properties (e.g.\ Gaussian limits for the debiased Lasso) under similarly
                    general assumptions has been considered open problem in the universality
                    theory. As applications, we show that the approximate formula allows us to
                    reduce the computation complexity of variable selection algorithms that require
                    solving multiple Lasso problems, such as the conditional randomization test and
                    a variant of the knockoff filter.
                </p>
            </div>
        </dd>
        <dt><a name="item26">[26]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03083"
                    title="Abstract">arXiv:2405.03083</a> [<a href="/pdf/2405.03083" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03083" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Causal K-Means Clustering
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Kim%2C+K">Kwangho Kim</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Kim%2C+J">Jisu Kim</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Kennedy%2C+E+H">Edward H. Kennedy</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Causal effects are often characterized with population summaries. These might
                    provide an incomplete picture when there are heterogeneous treatment effects
                    across subgroups. Since the subgroup structure is typically unknown, it is more
                    challenging to identify and evaluate subgroup effects than population effects.
                    We propose a new solution to this problem: Causal k-Means Clustering, which
                    harnesses the widely-used k-means clustering algorithm to uncover the unknown
                    subgroup structure. Our problem differs significantly from the conventional
                    clustering setup since the variables to be clustered are unknown counterfactual
                    functions. We present a plug-in estimator which is simple and readily
                    implementable using off-the-shelf algorithms, and study its rate of
                    convergence. We also develop a new bias-corrected estimator based on
                    nonparametric efficiency theory and double machine learning, and show that this
                    estimator achieves fast root-n rates and asymptotic normality in large
                    nonparametric models. Our proposed methods are especially useful for modern
                    outcome-wide studies with multiple treatment levels. Further, our framework is
                    extensible to clustering with generic pseudo-outcomes, such as partially
                    observed outcomes or otherwise unknown functions. Finally, we explore finite
                    sample properties via simulation, and illustrate the proposed methods in a
                    study of treatment programs for adolescent substance abuse.
                </p>
            </div>
        </dd>
        <dt><a name="item27">[27]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03096"
                    title="Abstract">arXiv:2405.03096</a> [<a href="/pdf/2405.03096" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03096" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Exact Sampling of Spanning Trees via Fast-forwarded Random
                    Walks
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Tam%2C+E">Edric Tam</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Dunson%2C+D+B">David B. Dunson</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Duan%2C+L+L">Leo L. Duan</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">Tree graphs are routinely used in statistics. When estimating a Bayesian
                    model with a tree component, sampling the posterior remains a core difficulty.
                    Existing Markov chain Monte Carlo methods tend to rely on local moves, often
                    leading to poor mixing. A promising approach is to instead directly sample
                    spanning trees on an auxiliary graph. Current spanning tree samplers, such as
                    the celebrated Aldous--Broder algorithm, predominantly rely on simulating
                    random walks that are required to visit all the nodes of the graph. Such
                    algorithms are prone to getting stuck in certain sub-graphs. We formalize this
                    phenomenon using the bottlenecks in the random walk's transition probability
                    matrix. We then propose a novel fast-forwarded cover algorithm that can break
                    free from bottlenecks. The core idea is a marginalization argument that leads
                    to a closed-form expression which allows for fast-forwarding to the event of
                    visiting a new node. Unlike many existing approximation algorithms, our
                    algorithm yields exact samples. We demonstrate the enhanced efficiency of the
                    fast-forwarded cover algorithm, and illustrate its application in fitting a
                    Bayesian dendrogram model on a Massachusetts crimes and communities dataset.
                </p>
            </div>
        </dd>
        <dt><a name="item28">[28]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03130"
                    title="Abstract">arXiv:2405.03130</a> [<a href="/pdf/2405.03130" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03130" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Deep Learning for Causal Inference: A Comparison of
                    Architectures for Heterogeneous Treatment Effect Estimation
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Papakostas%2C+D">Demetrios Papakostas</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Herren%2C+A">Andrew Herren</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Hahn%2C+P+R">P. Richard Hahn</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Castillo%2C+F">Francisco Castillo</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
                <p class="mathjax">Causal inference has gained much popularity in recent years, with interests
                    ranging from academic, to industrial, to educational, and all in between.
                    Concurrently, the study and usage of neural networks has also grown profoundly
                    (albeit at a far faster rate). What we aim to do in this blog write-up is
                    demonstrate a Neural Network causal inference architecture. We develop a fully
                    connected neural network implementation of the popular Bayesian Causal Forest
                    algorithm, a state of the art tree based method for estimating heterogeneous
                    treatment effects. We compare our implementation to existing neural network
                    causal inference methodologies, showing improvements in performance in
                    simulation settings. We apply our method to a dataset examining the effect of
                    stress on sleep.
                </p>
            </div>
        </dd>
        <dt><a name="item29">[29]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03180"
                    title="Abstract">arXiv:2405.03180</a> [<a href="/pdf/2405.03180" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03180" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Braced Fourier Continuation and Regression for Anomaly
                    Detection
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Sabuda%2C+J">Josef Sabuda</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 16 pages, 9 figures, associated Github link: <a
                        href="https://github.com/j4sabuda/Braced-Fourier-Continuation-and-Regression">this https URL</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

                </div>
                <p class="mathjax">In this work, the concept of Braced Fourier Continuation and Regression
                    (BFCR) is introduced. BFCR is a novel and computationally efficient means of
                    finding nonlinear regressions or trend lines in arbitrary one-dimensional data
                    sets. The Braced Fourier Continuation (BFC) and BFCR algorithms are first
                    outlined, followed by a discussion of the properties of BFCR as well as
                    demonstrations of how BFCR trend lines may be used effectively for anomaly
                    detection both within and at the edges of arbitrary one-dimensional data sets.
                    Finally, potential issues which may arise while using BFCR for anomaly
                    detection as well as possible mitigation techniques are outlined and discussed.
                    All source code and example data sets are either referenced or available via
                    GitHub, and all associated code is written entirely in Python.
                </p>
            </div>
        </dd>
        <dt><a name="item30">[30]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03198"
                    title="Abstract">arXiv:2405.03198</a> [<a href="/pdf/2405.03198" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03198" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Stability Evaluation via Distributional Perturbation Analysis
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Blanchet%2C+J">Jose Blanchet</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Cui%2C+P">Peng Cui</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Li%2C+J">Jiajin Li</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Liu%2C+J">Jiashuo Liu</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted by ICML 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

                </div>
                <p class="mathjax">The performance of learning models often deteriorates when deployed in
                    out-of-sample environments. To ensure reliable deployment, we propose a
                    stability evaluation criterion based on distributional perturbations.
                    Conceptually, our stability evaluation criterion is defined as the minimal
                    perturbation required on our observed dataset to induce a prescribed
                    deterioration in risk evaluation. In this paper, we utilize the optimal
                    transport (OT) discrepancy with moment constraints on the \textit{(sample,
                    density)} space to quantify this perturbation. Therefore, our stability
                    evaluation criterion can address both \emph{data corruptions} and
                    \emph{sub-population shifts} -- the two most common types of distribution
                    shifts in real-world scenarios. To further realize practical benefits, we
                    present a series of tractable convex formulations and computational methods
                    tailored to different classes of loss functions. The key technical tool to
                    achieve this is the strong duality theorem provided in this paper. Empirically,
                    we validate the practical utility of our stability evaluation criterion across
                    a host of real-world applications. These empirical studies showcase the
                    criterion's ability not only to compare the stability of different learning
                    models and features but also to provide valuable guidelines and strategies to
                    further improve models.
                </p>
            </div>
        </dd>
        <dt><a name="item31">[31]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03225"
                    title="Abstract">arXiv:2405.03225</a> [<a href="/pdf/2405.03225" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03225" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Consistent response prediction for multilayer networks on
                    unknown manifolds
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Acharyya%2C+A">Aranyak Acharyya</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Reli%C3%B3n%2C+J+A">Jesús Arroyo Relión</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Clayton%2C+M">Michael Clayton</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zlatic%2C+M">Marta Zlatic</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Park%2C+Y">Youngser Park</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Priebe%2C+C+E">Carey E. Priebe</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
                <p class="mathjax">Our paper deals with a collection of networks on a common set of nodes, where
                    some of the networks are associated with responses. Assuming that the networks
                    correspond to points on a one-dimensional manifold in a higher dimensional
                    ambient space, we propose an algorithm to consistently predict the response at
                    an unlabeled network. Our model involves a specific multiple random network
                    model, namely the common subspace independent edge model, where the networks
                    share a common invariant subspace, and the heterogeneity amongst the networks
                    is captured by a set of low dimensional matrices. Our algorithm estimates these
                    low dimensional matrices that capture the heterogeneity of the networks, learns
                    the underlying manifold by isomap, and consistently predicts the response at an
                    unlabeled network. We provide theoretical justifications for the use of our
                    algorithm, validated by numerical simulations. Finally, we demonstrate the use
                    of our algorithm on larval Drosophila connectome data.
                </p>
            </div>
        </dd>
        <dt><a name="item32">[32]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03468"
                    title="Abstract">arXiv:2405.03468</a> [<a href="/pdf/2405.03468" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03468" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Hierarchic Flows to Estimate and Sample High-dimensional
                    Probabilities
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Lempereur%2C+E">Etienne Lempereur</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Mallat%2C+S">Stéphane Mallat</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)

                </div>
                <p class="mathjax">Finding low-dimensional interpretable models of complex physical fields such
                    as turbulence remains an open question, 80 years after the pioneer work of
                    Kolmogorov. Estimating high-dimensional probability distributions from data
                    samples suffers from an optimization and an approximation curse of
                    dimensionality. It may be avoided by following a hierarchic probability flow
                    from coarse to fine scales. This inverse renormalization group is defined by
                    conditional probabilities across scales, renormalized in a wavelet basis. For a
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-6-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-16"
                                style="width: 1.334em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 1.102em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1001.1em, 1.392em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-17"><span class="msubsup"
                                                id="MathJax-Span-18"><span
                                                    style="display: inline-block; position: relative; width: 1.102em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.359em, 1000.64em, 4.401em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-19"
                                                            style="font-family: MathJax_Math-italic;">φ</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.639em;"><span
                                                            class="mn" id="MathJax-Span-20"
                                                            style="font-size: 70.7%; font-family: MathJax_Main;">4</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.392em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-6">\varphi^4</script> scalar potential, sampling these
                    hierarchic models avoids the
                    critical slowing down at the phase transition. An outstanding issue is to also
                    approximate non-Gaussian fields having long-range interactions in space and
                    across scales. We introduce low-dimensional models with robust multiscale
                    approximations of high order polynomial energies. They are calculated with a
                    second wavelet transform, which defines interactions over two hierarchies of
                    scales. We estimate and sample these wavelet scattering models to generate 2D
                    vorticity fields of turbulence, and images of dark matter densities.
                </p>
            </div>
        </dd>
        <dt><a name="item33">[33]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03487"
                    title="Abstract">arXiv:2405.03487</a> [<a href="/pdf/2405.03487" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03487" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Precision-based designs for sequential randomized experiments
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Nordin%2C+M">Mattias Nordin</a>,
                    <a href="/search/math?searchtype=author&amp;query=Schultzberg%2C+M">Mårten Schultzberg</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>

                </div>
                <p class="mathjax">In this paper, we consider an experimental setting where units enter the
                    experiment sequentially. Our goal is to form stopping rules which lead to
                    estimators of treatment effects with a given precision. We propose a
                    fixed-width confidence interval design (FWCID) where the experiment terminates
                    once a pre-specified confidence interval width is achieved. We show that under
                    this design, the difference-in-means estimator is a consistent estimator of the
                    average treatment effect and standard confidence intervals have asymptotic
                    guarantees of coverage and efficiency for several versions of the design. In
                    addition, we propose a version of the design that we call fixed power design
                    (FPD) where a given power is asymptotically guaranteed for a given treatment
                    effect, without the need to specify the variances of the outcomes under
                    treatment or control. In addition, this design also gives a consistent
                    difference-in-means estimator with correct coverage of the corresponding
                    standard confidence interval. We complement our theoretical findings with Monte
                    Carlo simulations where we compare our proposed designs with standard designs
                    in the sequential experiments literature, showing that our designs outperform
                    these designs in several important aspects. We believe our results to be
                    relevant for many experimental settings where units enter sequentially, such as
                    in clinical trials, as well as in online A/B tests used by the tech and
                    e-commerce industry.
                </p>
            </div>
        </dd>
        <dt><a name="item34">[34]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03538"
                    title="Abstract">arXiv:2405.03538</a> [<a href="/pdf/2405.03538" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03538" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Adolescent sports participation and health in early
                    adulthood: An observational study
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Kokandakar%2C+A+H">Ajinkya H. Kokandakar</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Lin%2C+Y">Yuzhou Lin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Jin%2C+S">Steven Jin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Weiss%2C+J">Jordan Weiss</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Rabinowitz%2C+A+R">Amanda R. Rabinowitz</a>,
                    <a href="/search/stat?searchtype=author&amp;query=May%2C+R+A+B">Reuben A. Buford May</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Small%2C+D">Dylan Small</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Deshpande%2C+S+K">Sameer K. Deshpande</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> The pre-analysis protocol for this study is available at
                    <a href="/abs/2211.02104">arXiv:2211.02104</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
                <p class="mathjax">We study the impact of teenage sports participation on early-adulthood health
                    using longitudinal data from the National Study of Youth and Religion. We focus
                    on two primary outcomes measured at ages 23--28 -- self-rated health and total
                    score on the PHQ9 Patient Depression Questionnaire -- and control for several
                    potential confounders related to demographics and family socioeconomic status.
                    To probe the possibility that certain types of sports participation may have
                    larger effects on health than others, we conduct a matched observational study
                    at each level within a hierarchy of exposures. Our hierarchy ranges from
                    broadly defined exposures (e.g., participation in any organized after-school
                    activity) to narrow (e.g., participation in collision sports). We deployed an
                    ordered testing approach that exploits the hierarchical relationships between
                    our exposure definitions to perform our analyses while maintaining a fixed
                    family-wise error rate. Compared to teenagers who did not participate in any
                    after-school activities, those who participated in sports had statistically
                    significantly better self-rated and mental health outcomes in early adulthood.
                </p>
            </div>
        </dd>
        <dt><a name="item35">[35]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03549"
                    title="Abstract">arXiv:2405.03549</a> [<a href="/pdf/2405.03549" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03549" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Bridging discrete and continuous state spaces: Exploring the
                    Ehrenfest process in time-continuous diffusion models
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Winkler%2C+L">Ludwig Winkler</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Richter%2C+L">Lorenz Richter</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Opper%2C+M">Manfred Opper</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS); Probability (math.PR)

                </div>
                <p class="mathjax">Generative modeling via stochastic processes has led to remarkable empirical
                    results as well as to recent advances in their theoretical understanding. In
                    principle, both space and time of the processes can be discrete or continuous.
                    In this work, we study time-continuous Markov jump processes on discrete state
                    spaces and investigate their correspondence to state-continuous diffusion
                    processes given by SDEs. In particular, we revisit the <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-21"
                                style="width: 9.146em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 7.584em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.102em, 1007.58em, 2.376em, -999.997em); top: -1.965em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-22"><span class="texatom"
                                                id="MathJax-Span-23"><span class="mrow" id="MathJax-Span-24"><span
                                                        class="mtext" id="MathJax-Span-25"
                                                        style="font-family: MathJax_Main-italic;">Ehrenfest
                                                        process</span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.97em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-7">\textit{Ehrenfest
    process}</script>, which converges to an Ornstein-Uhlenbeck process in the infinite
                    state space limit. Likewise, we can show that the time-reversal of the
                    Ehrenfest process converges to the time-reversed Ornstein-Uhlenbeck process.
                    This observation bridges discrete and continuous state spaces and allows to
                    carry over methods from one to the respective other setting. Additionally, we
                    suggest an algorithm for training the time-reversal of Markov jump processes
                    which relies on conditional expectations and can thus be directly related to
                    denoising score matching. We demonstrate our methods in multiple convincing
                    numerical experiments.
                </p>
            </div>
        </dd>
        <dt><a name="item36">[36]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03579"
                    title="Abstract">arXiv:2405.03579</a> [<a href="/pdf/2405.03579" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03579" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Some Statistical and Data Challenges When Building
                    Early-Stage Digital Experimentation and Measurement Capabilities
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Liu%2C+C+H+B">C. H. Bryan Liu</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> PhD thesis. Imperial College London. Official library
                    version available on: <a href="https://spiral.imperial.ac.uk/handle/10044/1/110307">this https
                        URL</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>; Databases (cs.DB); Methodology (stat.ME)

                </div>
                <p class="mathjax">Digital experimentation and measurement (DEM) capabilities -- the knowledge
                    and tools necessary to run experiments with digital products, services, or
                    experiences and measure their impact -- are fast becoming part of the standard
                    toolkit of digital/data-driven organisations in guiding business decisions.
                    Many large technology companies report having mature DEM capabilities, and
                    several businesses have been established purely to manage experiments for
                    others. Given the growing evidence that data-driven organisations tend to
                    outperform their non-data-driven counterparts, there has never been a greater
                    need for organisations to build/acquire DEM capabilities to thrive in the
                    current digital era.
                    <br>This thesis presents several novel approaches to statistical and data
                    challenges for organisations building DEM capabilities. We focus on the
                    fundamentals associated with building DEM capabilities, which lead to a richer
                    understanding of the underlying assumptions and thus enable us to develop more
                    appropriate capabilities. We address why one should engage in DEM by
                    quantifying the benefits and risks of acquiring DEM capabilities. This is done
                    using a ranking under lower uncertainty model, enabling one to construct a
                    business case. We also examine what ingredients are necessary to run digital
                    experiments. In addition to clarifying the existing literature around
                    statistical tests, datasets, and methods in experimental design and causal
                    inference, we construct an additional dataset and detailed case studies on
                    applying state-of-the-art methods. Finally, we investigate when a digital
                    experiment design would outperform another, leading to an evaluation framework
                    that compares competing designs' data efficiency.
                </p>
            </div>
        </dd>
        <dt><a name="item37">[37]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03603"
                    title="Abstract">arXiv:2405.03603</a> [<a href="/pdf/2405.03603" title="Download PDF">pdf</a>, <a
                    href="/ps/2405.03603" title="Download PostScript">ps</a>, <a href="/format/2405.03603"
                    title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Copas-Heckman-type sensitivity analysis for publication bias
                    in rare-event meta-analysis under the framework of the generalized linear mixed model
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Zhou%2C+Y">Yi Zhou</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Hu%2C+T">Taojun Hu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhou%2C+X">Xiao-Hua Zhou</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Hattori%2C+S">Satoshi Hattori</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Applications (stat.AP)

                </div>
                <p class="mathjax">Publication bias (PB) is one of the serious issues in meta-analysis. Many
                    existing methods dealing with PB are based on the normal-normal (NN)
                    random-effects model assuming normal models in both the within-study and the
                    between-study levels. For rare-event meta-analysis where the data contain rare
                    occurrences of event, the standard NN random-effects model may perform poorly.
                    Instead, the generalized linear mixed effects model (GLMM) using the exact
                    within-study model is recommended. However, no method has been proposed for
                    dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we
                    propose sensitivity analysis methods for evaluating the impact of PB on the
                    GLMM based on the famous Copas-Heckman-type selection model. The proposed
                    methods can be easily implemented with the standard software coring the
                    nonlinear mixed-effects model. We use a real-world example to show how the
                    usefulness of the proposed methods in evaluating the potential impact of PB in
                    meta-analysis of the log-transformed odds ratio based on the GLMM using the
                    non-central hypergeometric or binomial distribution as the within-study model.
                    An extension of the proposed method is also introduced for evaluating PB in
                    meta-analysis of proportion based on the GLMM with the binomial within-study
                    model.
                </p>
            </div>
        </dd>
        <dt><a name="item38">[38]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03606"
                    title="Abstract">arXiv:2405.03606</a> [<a href="/pdf/2405.03606" title="Download PDF">pdf</a>, <a
                    href="/format/2405.03606" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Strang Splitting for Parametric Inference in Second-order
                    Stochastic Differential Equations
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Pilipovic%2C+P">Predrag Pilipovic</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Samson%2C+A">Adeline Samson</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Ditlevsen%2C+S">Susanne Ditlevsen</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">We address parameter estimation in second-order stochastic differential
                    equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE
                    is converted to a first-order system by introducing an auxiliary velocity
                    variable raising two main challenges. First, the system is hypoelliptic since
                    the noise affects only the velocity, making the Euler-Maruyama estimator
                    ill-conditioned. To overcome that, we propose an estimator based on the Strang
                    splitting scheme. Second, since the velocity is rarely observed we adjust the
                    estimator for partial observations. We present four estimators for complete and
                    partial observations, using full likelihood or only velocity marginal
                    likelihood. These estimators are intuitive, easy to implement, and
                    computationally fast, and we prove their consistency and asymptotic normality.
                    Our analysis demonstrates that using full likelihood with complete observations
                    reduces the asymptotic variance of the diffusion estimator. With partial
                    observations, the asymptotic variance increases due to information loss but
                    remains unaffected by the likelihood choice. However, a numerical study on the
                    Kramers oscillator reveals that using marginal likelihood for partial
                    observations yields less biased estimators. We apply our approach to
                    paleoclimate data from the Greenland ice core and fit it to the Kramers
                    oscillator model, capturing transitions between metastable states reflecting
                    observed climatic conditions during glacial eras.
                </p>
            </div>
        </dd>
    </dl>
    <h3>Cross-lists for Tue, 7 May 24</h3>
    <dl>
        <dt><a name="item39">[39]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02373"
                    title="Abstract">arXiv:2405.02373</a> (cross-list from math.OC) [<a href="/pdf/2405.02373"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02373" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Exponentially Weighted Algorithm for Online Network Resource
                    Allocation with Long-Term Constraints
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Sid-Ali%2C+A">Ahmed Sid-Ali</a>,
                    <a href="/search/math?searchtype=author&amp;query=Lambadaris%2C+I">Ioannis Lambadaris</a>,
                    <a href="/search/math?searchtype=author&amp;query=Zhao%2C+Y+Q">Yiqiang Q. Zhao</a>,
                    <a href="/search/math?searchtype=author&amp;query=Shaikhet%2C+G">Gennady Shaikhet</a>,
                    <a href="/search/math?searchtype=author&amp;query=Asgharnia%2C+A">Amirhossein Asgharnia</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a
                        href="/abs/2305.15558">arXiv:2305.15558</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control
                        (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

                </div>
                <p class="mathjax">This paper studies an online optimal resource reservation problem in
                    communication networks with job transfers where the goal is to minimize the
                    reservation cost while maintaining the blocking cost under a certain budget
                    limit. To tackle this problem, we propose a novel algorithm based on a
                    randomized exponentially weighted method that encompasses long-term
                    constraints. We then analyze the performance of our algorithm by establishing
                    an upper bound for the associated regret and the cumulative constraint
                    violations. Finally, we present numerical experiments where we compare the
                    performance of our algorithm with those of reinforcement learning where we show
                    that our algorithm surpasses it.
                </p>
            </div>
        </dd>
        <dt><a name="item40">[40]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02441"
                    title="Abstract">arXiv:2405.02441</a> (cross-list from cs.LG) [<a href="/pdf/2405.02441"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.02441" title="Download PostScript">ps</a>, <a
                    href="/format/2405.02441" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Learning minimal volume uncertainty ellipsoids
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Alon%2C+I">Itai Alon</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Arnon%2C+D">David Arnon</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Wiesel%2C+A">Ami Wiesel</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">We consider the problem of learning uncertainty regions for parameter
                    estimation problems. The regions are ellipsoids that minimize the average
                    volumes subject to a prescribed coverage probability. As expected, under the
                    assumption of jointly Gaussian data, we prove that the optimal ellipsoid is
                    centered around the conditional mean and shaped as the conditional covariance
                    matrix. In more practical cases, we propose a differentiable optimization
                    approach for approximately computing the optimal ellipsoids using a neural
                    network with proper calibration. Compared to existing methods, our network
                    requires less storage and less computations in inference time, leading to
                    accurate yet smaller ellipsoids. We demonstrate these advantages on four
                    real-world localization datasets.
                </p>
            </div>
        </dd>
        <dt><a name="item41">[41]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02475"
                    title="Abstract">arXiv:2405.02475</a> (cross-list from cs.LG) [<a href="/pdf/2405.02475"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02475" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Generalizing Orthogonalization for Models with
                    Non-linearities
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=R%C3%BCgamer%2C+D">David Rügamer</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Kolb%2C+C">Chris Kolb</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Weber%2C+T">Tobias Weber</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Kook%2C+L">Lucas Kook</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Nagler%2C+T">Thomas Nagler</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO); Methodology (stat.ME)

                </div>
                <p class="mathjax">The complexity of black-box algorithms can lead to various challenges,
                    including the introduction of biases. These biases present immediate risks in
                    the algorithms' application. It was, for instance, shown that neural networks
                    can deduce racial information solely from a patient's X-ray scan, a task beyond
                    the capability of medical experts. If this fact is not known to the medical
                    expert, automatic decision-making based on this algorithm could lead to
                    prescribing a treatment (purely) based on racial information. While current
                    methodologies allow for the "orthogonalization" or "normalization" of neural
                    networks with respect to such information, existing approaches are grounded in
                    linear models. Our paper advances the discourse by introducing corrections for
                    non-linearities such as ReLU activations. Our approach also encompasses scalar
                    and tensor-valued predictions, facilitating its integration into neural network
                    architectures. Through extensive experiments, we validate our method's
                    effectiveness in safeguarding sensitive data in generalized linear models,
                    normalizing convolutional neural networks for metadata, and rectifying
                    pre-existing embeddings for undesired attributes.
                </p>
            </div>
        </dd>
        <dt><a name="item42">[42]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02594"
                    title="Abstract">arXiv:2405.02594</a> (cross-list from cs.LG) [<a href="/pdf/2405.02594"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02594" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Leveraging (Biased) Information: Multi-armed Bandits with
                    Offline Data
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Cheung%2C+W+C">Wang Chi Cheung</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+L">Lixing Lyu</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 24 pages, 5 figures. Accepted to ICML 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">We leverage offline data to facilitate online learning in stochastic
                    multi-armed bandits. The probability distributions that govern the offline data
                    and the online rewards can be different. Without any non-trivial upper bound on
                    their difference, we show that no non-anticipatory policy can outperform the
                    UCB policy by (Auer et al. 2002), even in the presence of offline data. In
                    complement, we propose an online policy MIN-UCB, which outperforms UCB when a
                    non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the
                    offline data when they are deemed informative, and to ignore them otherwise.
                    MIN-UCB is shown to be tight in terms of both instance independent and
                    dependent regret bounds. Finally, we corroborate the theoretical results with
                    numerical experiments.
                </p>
            </div>
        </dd>
        <dt><a name="item43">[43]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02612"
                    title="Abstract">arXiv:2405.02612</a> (cross-list from cs.LG) [<a href="/pdf/2405.02612"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02612" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Learning Linear Utility Functions From Pairwise Comparison
                    Queries
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Ge%2C+L">Luise Ge</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Juba%2C+B">Brendan Juba</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Submitted to ECAI for review
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning
                    (stat.ML)

                </div>
                <p class="mathjax">We study learnability of linear utility functions from pairwise comparison
                    queries. In particular, we consider two learning objectives. The first
                    objective is to predict out-of-sample responses to pairwise comparisons,
                    whereas the second is to approximately recover the true parameters of the
                    utility function. We show that in the passive learning setting, linear
                    utilities are efficiently learnable with respect to the first objective, both
                    when query responses are uncorrupted by noise, and under Tsybakov noise when
                    the distributions are sufficiently "nice". In contrast, we show that utility
                    parameters are not learnable for a large set of data distributions without
                    strong modeling assumptions, even when query responses are noise-free. Next, we
                    proceed to analyze the learning problem in an active learning setting. In this
                    case, we show that even the second objective is efficiently learnable, and
                    present algorithms for both the noise-free and noisy query response settings.
                    Our results thus exhibit a qualitative learnability gap between passive and
                    active learning from pairwise preference queries, demonstrating the value of
                    the ability to select pairwise queries for utility learning.
                </p>
            </div>
        </dd>
        <dt><a name="item44">[44]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02753"
                    title="Abstract">arXiv:2405.02753</a> (cross-list from math.OC) [<a href="/pdf/2405.02753"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.02753" title="Download PostScript">ps</a>, <a
                    href="/format/2405.02753" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Unscented Trajectory Optimization
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Ross%2C+I+M">I. M. Ross</a>,
                    <a href="/search/math?searchtype=author&amp;query=Proulx%2C+R+J">R. J. Proulx</a>,
                    <a href="/search/math?searchtype=author&amp;query=Karpenko%2C+M">M. Karpenko</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 21 pages, 11 figures 2023 AAS/AIAA Astrodynamics
                    Specialist Conference, Big Sky, MT, Aug 13-17, 2023
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control
                        (math.OC)</span>; Systems and Control (eess.SY); Statistics Theory (math.ST); Computation
                    (stat.CO)

                </div>
                <p class="mathjax">In a nutshell, unscented trajectory optimization is the generation of optimal
                    trajectories through the use of an unscented transform. Although unscented
                    trajectory optimization was introduced by the authors about a decade ago, it is
                    reintroduced in this paper as a special instantiation of tychastic optimal
                    control theory. Tychastic optimal control theory (from \textit{Tyche}, the
                    Greek goddess of chance) avoids the use of a Brownian motion and the resulting
                    It\^{o} calculus even though it uses random variables across the entire
                    spectrum of a problem formulation. This approach circumvents the enormous
                    technical and numerical challenges associated with stochastic trajectory
                    optimization. Furthermore, it is shown how a tychastic optimal control problem
                    that involves nonlinear transformations of the expectation operator can be
                    quickly instantiated using an unscented transform. These nonlinear
                    transformations are particularly useful in managing trajectory dispersions be
                    it associated with path constraints or targeted values of final-time
                    conditions. This paper also presents a systematic and rapid process for
                    formulating and computing the most desirable tychastic trajectory using an
                    unscented transform. Numerical examples are used to illustrate how unscented
                    trajectory optimization may be used for risk reduction and mission recovery
                    caused by uncertainties and failures.
                </p>
            </div>
        </dd>
        <dt><a name="item45">[45]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02810"
                    title="Abstract">arXiv:2405.02810</a> (cross-list from math.NA) [<a href="/pdf/2405.02810"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02810" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Adaptive deep density approximation for stochastic dynamical
                    systems
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=He%2C+J">Junjie He</a>,
                    <a href="/search/math?searchtype=author&amp;query=Liao%2C+Q">Qifeng Liao</a>,
                    <a href="/search/math?searchtype=author&amp;query=Wan%2C+X">Xiaoliang Wan</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 24 pages, 13 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis
                        (math.NA)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">In this paper we consider adaptive deep neural network approximation for
                    stochastic dynamical systems. Based on the Liouville equation associated with
                    the stochastic dynamical systems, a new temporal KRnet (tKRnet) is proposed to
                    approximate the probability density functions (PDFs) of the state variables.
                    The tKRnet gives an explicit density model for the solution of the Liouville
                    equation, which alleviates the curse of dimensionality issue that limits the
                    application of traditional grid based numerical methods. To efficiently train
                    the tKRnet, an adaptive procedure is developed to generate collocation points
                    for the corresponding residual loss function, where samples are generated
                    iteratively using the approximate density function at each iteration. A
                    temporal decomposition technique is also employed to improve the long-time
                    integration. Theoretical analysis of our proposed method is provided, and
                    numerical examples are presented to demonstrate its performance.
                </p>
            </div>
        </dd>
        <dt><a name="item46">[46]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02833"
                    title="Abstract">arXiv:2405.02833</a> (cross-list from math.PR) [<a href="/pdf/2405.02833"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02833" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Limiting Behavior of Maxima under Dependence
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Herrmann%2C+K">Klaus Herrmann</a>,
                    <a href="/search/math?searchtype=author&amp;query=Hofert%2C+M">Marius Hofert</a>,
                    <a href="/search/math?searchtype=author&amp;query=Neslehova%2C+J+G">Johanna G. Neslehova</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Probability
                        (math.PR)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">Weak convergence of maxima of dependent sequences of identically distributed
                    continuous random variables is studied under normalizing sequences arising as
                    subsequences of the normalizing sequences from an associated iid sequence. This
                    general framework allows one to derive several generalizations of the
                    well-known Fisher-Tippett-Gnedenko theorem under conditions on the univariate
                    marginal distribution and the dependence structure of the sequence. The
                    limiting distributions are shown to be compositions of a generalized extreme
                    value distribution and a distortion function which reflects the limiting
                    behavior of the diagonal of the underlying copula. Uniform convergence rates
                    for the weak convergence to the limiting distribution are also derived.
                    Examples covering well-known dependence structures are provided. Several
                    existing results, e.g. for exchangeable sequences or stationary time series,
                    are embedded in the proposed framework.
                </p>
            </div>
        </dd>
        <dt><a name="item47">[47]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02881"
                    title="Abstract">arXiv:2405.02881</a> (cross-list from cs.LG) [<a href="/pdf/2405.02881"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02881" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> FedConPE: Efficient Federated Conversational Bandits with
                    Heterogeneous Clients
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhuohua Li</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Maoli Liu</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Lui%2C+J+C+S">John C.S. Lui</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted in the 33rd International Joint Conference on
                    Artificial Intelligence (IJCAI), 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Conversational recommender systems have emerged as a potent solution for
                    efficiently eliciting user preferences. These systems interactively present
                    queries associated with "key terms" to users and leverage user feedback to
                    estimate user preferences more efficiently. Nonetheless, most existing
                    algorithms adopt a centralized approach. In this paper, we introduce FedConPE,
                    a phase elimination-based federated conversational bandit algorithm, where <span
                        class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-8-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-26"
                                style="width: 1.276em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 1.045em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1001.04em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28"
                                                style="font-family: MathJax_Math-italic;">M<span
                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.061em;"></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-8">M</script>
                    agents collaboratively solve a global contextual linear bandit problem with the
                    help of a central server while ensuring secure data management. To effectively
                    coordinate all the clients and aggregate their collected data, FedConPE uses an
                    adaptive approach to construct key terms that minimize uncertainty across all
                    dimensions in the feature space. Furthermore, compared with existing federated
                    linear bandit algorithms, FedConPE offers improved computational and
                    communication efficiency as well as enhanced privacy protections. Our
                    theoretical analysis shows that FedConPE is minimax near-optimal in terms of
                    cumulative regret. We also establish upper bounds for communication costs and
                    conversation frequency. Comprehensive evaluations demonstrate that FedConPE
                    outperforms existing conversational bandit algorithms while using fewer
                    conversations.
                </p>
            </div>
        </dd>
        <dt><a name="item48">[48]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.02928"
                    title="Abstract">arXiv:2405.02928</a> (cross-list from math.PR) [<a href="/pdf/2405.02928"
                    title="Download PDF">pdf</a>, <a href="/format/2405.02928" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Probabilistic cellular automata with local transition
                    matrices: synchronization, ergodicity, and inference
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Bayrakta%2C+E">Erhan Bayrakta</a>,
                    <a href="/search/math?searchtype=author&amp;query=Lu%2C+F">Fei Lu</a>,
                    <a href="/search/math?searchtype=author&amp;query=Maggioni%2C+M">Mauro Maggioni</a>,
                    <a href="/search/math?searchtype=author&amp;query=Wu%2C+R">Ruoyu Wu</a>,
                    <a href="/search/math?searchtype=author&amp;query=Yang%2C+S">Sichen Yang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Probability
                        (math.PR)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">We introduce a new class of probabilistic cellular automata that are capable
                    of exhibiting rich dynamics such as synchronization and ergodicity and can be
                    easily inferred from data. The system is a finite-state locally interacting
                    Markov chain on a circular graph. Each site's subsequent state is random, with
                    a distribution determined by its neighborhood's empirical distribution
                    multiplied by a local transition matrix. We establish sufficient and necessary
                    conditions on the local transition matrix for synchronization and ergodicity.
                    Also, we introduce novel least squares estimators for inferring the local
                    transition matrix from various types of data, which may consist of either
                    multiple trajectories, a long trajectory, or ensemble sequences without
                    trajectory information. Under suitable identifiability conditions, we show the
                    asymptotic normality of these estimators and provide non-asymptotic bounds for
                    their accuracy.
                </p>
            </div>
        </dd>
        <dt><a name="item49">[49]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03021"
                    title="Abstract">arXiv:2405.03021</a> (cross-list from econ.EM) [<a href="/pdf/2405.03021"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.03021" title="Download PostScript">ps</a>, <a
                    href="/format/2405.03021" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Tuning parameter selection in econometrics
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/econ?searchtype=author&amp;query=Chetverikov%2C+D">Denis Chetverikov</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 41 pages, 1 table
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics
                        (econ.EM)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">I review some of the main methods for selecting tuning parameters in
                    nonparametric and <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-9-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-29"
                                style="width: 1.045em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(0.119em, 1000.87em, 1.334em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-30"><span class="msubsup"
                                                id="MathJax-Span-31"><span
                                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.41em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-32"
                                                            style="font-family: MathJax_Main;">ℓ</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -3.817em; left: 0.408em;"><span
                                                            class="mn" id="MathJax-Span-33"
                                                            style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 1.184em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-9">\ell_1</script>-penalized estimation. For the
                    nonparametric
                    estimation, I consider the methods of Mallows, Stein, Lepski, cross-validation,
                    penalization, and aggregation in the context of series estimation. For the
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-10-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-34"
                                style="width: 1.045em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(0.119em, 1000.87em, 1.334em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-35"><span class="msubsup"
                                                id="MathJax-Span-36"><span
                                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.41em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-37"
                                                            style="font-family: MathJax_Main;">ℓ</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -3.817em; left: 0.408em;"><span
                                                            class="mn" id="MathJax-Span-38"
                                                            style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 1.184em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-10">\ell_1</script>-penalized estimation, I consider the
                    methods based on the theory of
                    self-normalized moderate deviations, bootstrap, Stein's unbiased risk
                    estimation, and cross-validation in the context of Lasso estimation. I explain
                    the intuition behind each of the methods and discuss their comparative
                    advantages. I also give some extensions.
                </p>
            </div>
        </dd>
        <dt><a name="item50">[50]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03043"
                    title="Abstract">arXiv:2405.03043</a> (cross-list from quant-ph) [<a href="/pdf/2405.03043"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.03043" title="Download PostScript">ps</a>, <a
                    href="/format/2405.03043" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Negative Probability
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/quant-ph?searchtype=author&amp;query=Polson%2C+N">Nick Polson</a>,
                    <a href="/search/quant-ph?searchtype=author&amp;query=Sokolov%2C+V">Vadim Sokolov</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics
                        (quant-ph)</span>; Statistics Theory (math.ST)

                </div>
                <p class="mathjax">Negative probabilities arise primarily in quantum theory and computing.
                    Bartlett provides a definition based on characteristic functions and
                    extraordinary random variables. As Bartlett observes, negative probabilities
                    must always be combined with positive probabilities to yield a valid
                    probability distribution before any physical interpretation is admissible.
                    Negative probabilities arise as mixing distributions of unobserved latent
                    variables in Bayesian modeling. Our goal is to provide a link with dual
                    densities and the class of scale mixtures of normal distributions. We provide
                    an analysis of the classic half coin distribution and Feynman's negative
                    probability examples. A number of examples of dual densities with negative
                    mixing measures including the linnik distribution, Wigner distribution and the
                    stable distribution are provided. Finally, we conclude with directions for
                    future research.
                </p>
            </div>
        </dd>
        <dt><a name="item51">[51]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03059"
                    title="Abstract">arXiv:2405.03059</a> (cross-list from cs.LG) [<a href="/pdf/2405.03059"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03059" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Active Preference Learning for Ordering Items In- and
                    Out-of-sample
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Bergstr%C3%B6m%2C+H">Herman Bergström</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Carlsson%2C+E">Emil Carlsson</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Dubhashi%2C+D">Devdatt Dubhashi</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Johansson%2C+F+D">Fredrik D. Johansson</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Learning an ordering of items based on noisy pairwise comparisons is useful
                    when item-specific labels are difficult to assign, for example, when annotators
                    have to make subjective assessments. Algorithms have been proposed for actively
                    sampling comparisons of items to minimize the number of annotations necessary
                    for learning an accurate ordering. However, many ignore shared structure
                    between items, treating them as unrelated, limiting sample efficiency and
                    precluding generalization to new items. In this work, we study active learning
                    with pairwise preference feedback for ordering items with contextual
                    attributes, both in- and out-of-sample. We give an upper bound on the expected
                    ordering error incurred by active learning strategies under a logistic
                    preference model, in terms of the aleatoric and epistemic uncertainty in
                    comparisons, and propose two algorithms designed to greedily minimize this
                    bound. We evaluate these algorithms in two realistic image ordering tasks,
                    including one with comparisons made by human annotators, and demonstrate
                    superior sample efficiency compared to non-contextual ranking approaches and
                    active preference learning baselines.
                </p>
            </div>
        </dd>
        <dt><a name="item52">[52]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03073"
                    title="Abstract">arXiv:2405.03073</a> (cross-list from math.OC) [<a href="/pdf/2405.03073"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03073" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Convergence and Complexity Guarantee for Inexact First-order
                    Riemannian Optimization Algorithms
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Li%2C+Y">Yuchen Li</a>,
                    <a href="/search/math?searchtype=author&amp;query=Balzano%2C+L">Laura Balzano</a>,
                    <a href="/search/math?searchtype=author&amp;query=Needell%2C+D">Deanna Needell</a>,
                    <a href="/search/math?searchtype=author&amp;query=Lyu%2C+H">Hanbaek Lyu</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 23 pages, 5 figures. ICML 2024. arXiv admin note: text
                    overlap with <a href="/abs/2312.10330">arXiv:2312.10330</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control
                        (math.OC)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">We analyze inexact Riemannian gradient descent (RGD) where Riemannian
                    gradients and retractions are inexactly (and cheaply) computed. Our focus is on
                    understanding when inexact RGD converges and what is the complexity in the
                    general nonconvex and constrained setting. We answer these questions in a
                    general framework of tangential Block Majorization-Minimization (tBMM). We
                    establish that tBMM converges to an <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-39"
                                style="width: 0.524em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.408em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.334em, 1000.41em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-40"><span class="mi" id="MathJax-Span-41"
                                                style="font-family: MathJax_Math-italic;">ϵ</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-11">\epsilon</script>-stationary point within
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-12-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-42"
                                style="width: 3.591em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.954em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.16em, 1002.84em, 2.549em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-43"><span class="mi" id="MathJax-Span-44"
                                                style="font-family: MathJax_Math-italic;">O</span><span class="mo"
                                                id="MathJax-Span-45" style="font-family: MathJax_Main;">(</span><span
                                                class="msubsup" id="MathJax-Span-46"><span
                                                    style="display: inline-block; position: relative; width: 1.392em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.417em, 1000.41em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-47"
                                                            style="font-family: MathJax_Math-italic;">ϵ</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.408em;"><span
                                                            class="texatom" id="MathJax-Span-48"><span class="mrow"
                                                                id="MathJax-Span-49"><span class="mo"
                                                                    id="MathJax-Span-50"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span
                                                                    class="mn" id="MathJax-Span-51"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span><span
                                                class="mo" id="MathJax-Span-52"
                                                style="font-family: MathJax_Main;">)</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.462em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-12">O(\epsilon^{-2})</script> iterations. Under a mild
                    assumption, the results still hold
                    when the subproblem is solved inexactly in each iteration provided the total
                    optimality gap is bounded. Our general analysis applies to a wide range of
                    classical algorithms with Riemannian constraints including inexact RGD and
                    proximal gradient method on Stiefel manifolds. We numerically validate that
                    tBMM shows improved performance over existing methods when applied to various
                    problems, including nonnegative tensor decomposition with Riemannian
                    constraints, regularized nonnegative matrix factorization, and low-rank matrix
                    recovery problems.
                </p>
            </div>
        </dd>
        <dt><a name="item53">[53]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03236"
                    title="Abstract">arXiv:2405.03236</a> (cross-list from cs.LG) [<a href="/pdf/2405.03236"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03236" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Federated Reinforcement Learning with Constraint
                    Heterogeneity
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hao Jin</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Liangyu Zhang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhihua Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">We study a Federated Reinforcement Learning (FedRL) problem with constraint
                    heterogeneity. In our setting, we aim to solve a reinforcement learning problem
                    with multiple constraints while <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-53"
                                style="width: 1.045em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.87em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-54"><span class="mi" id="MathJax-Span-55"
                                                style="font-family: MathJax_Math-italic;">N<span
                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.061em;"></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-13">N</script> training agents are located in <span
                        class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-14-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-56"
                                style="width: 1.045em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.87em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-57"><span class="mi" id="MathJax-Span-58"
                                                style="font-family: MathJax_Math-italic;">N<span
                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.061em;"></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-14">N</script>
                    different environments with limited access to the constraint signals and they
                    are expected to collaboratively learn a policy satisfying all constraint
                    signals. Such learning problems are prevalent in scenarios of Large Language
                    Model (LLM) fine-tuning and healthcare applications. To solve the problem, we
                    propose federated primal-dual policy optimization methods based on traditional
                    policy gradient methods. Specifically, we introduce <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-59"
                                style="width: 1.045em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.871em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.87em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-60"><span class="mi" id="MathJax-Span-61"
                                                style="font-family: MathJax_Math-italic;">N<span
                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.061em;"></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-15">N</script> local Lagrange
                    functions for agents to perform local policy updates, and these agents are then
                    scheduled to periodically communicate on their local policies. Taking natural
                    policy gradient (NPG) and proximal policy optimization (PPO) as policy
                    optimization methods, we mainly focus on two instances of our algorithms, ie,
                    {FedNPG} and {FedPPO}. We show that FedNPG achieves global convergence with an
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-16-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-62"
                                style="width: 4.98em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 4.112em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1004em, 2.549em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-63"><span class="texatom"
                                                id="MathJax-Span-64"><span class="mrow" id="MathJax-Span-65"><span
                                                        class="munderover" id="MathJax-Span-66"><span
                                                            style="display: inline-block; position: relative; width: 0.755em; height: 0px;"><span
                                                                style="position: absolute; clip: rect(3.128em, 1000.75em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                                    class="mi" id="MathJax-Span-67"
                                                                    style="font-family: MathJax_Math-italic;">O</span><span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                                style="position: absolute; clip: rect(3.533em, 1000.41em, 3.938em, -999.997em); top: -4.627em; left: 0.234em;"><span
                                                                    class="mo" id="MathJax-Span-68"
                                                                    style="font-family: MathJax_Main;">~</span><span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span></span><span
                                                class="mo" id="MathJax-Span-69"
                                                style="font-family: MathJax_Main;">(</span><span class="mn"
                                                id="MathJax-Span-70" style="font-family: MathJax_Main;">1</span><span
                                                class="texatom" id="MathJax-Span-71"><span class="mrow"
                                                    id="MathJax-Span-72"><span class="mo" id="MathJax-Span-73"
                                                        style="font-family: MathJax_Main;">/</span></span></span><span
                                                class="msqrt" id="MathJax-Span-74"><span
                                                    style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.7em, 4.17em, -999.997em); top: -3.99em; left: 0.813em;"><span
                                                            class="mrow" id="MathJax-Span-75"><span class="mi"
                                                                id="MathJax-Span-76"
                                                                style="font-family: MathJax_Math-italic;">T<span
                                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.119em;"></span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; clip: rect(3.533em, 1000.75em, 3.938em, -999.997em); top: -4.569em; left: 0.813em;"><span
                                                            style="display: inline-block; position: relative; width: 0.755em; height: 0px;"><span
                                                                style="position: absolute; font-family: MathJax_Main; top: -3.99em; left: -0.055em;">−<span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                                style="position: absolute; font-family: MathJax_Main; top: -3.99em; left: 0.061em;">−<span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; clip: rect(3.012em, 1000.87em, 4.343em, -999.997em); top: -4.048em; left: 0em;"><span
                                                            style="font-family: MathJax_Main;">√</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span><span
                                                class="mo" id="MathJax-Span-77"
                                                style="font-family: MathJax_Main;">)</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.601em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-16">\tilde{O}(1/\sqrt{T})</script> rate, and FedPPO
                    efficiently solves complicated
                    learning tasks with the use of deep neural networks.
                </p>
            </div>
        </dd>
        <dt><a name="item54">[54]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03293"
                    title="Abstract">arXiv:2405.03293</a> (cross-list from astro-ph.IM) [<a href="/pdf/2405.03293"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03293" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Deep Learning and genetic algorithms for cosmological
                    Bayesian inference speed-up
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/astro-ph?searchtype=author&amp;query=G%C3%B3mez-Vargas%2C+I">Isidro
                        Gómez-Vargas</a>,
                    <a href="/search/astro-ph?searchtype=author&amp;query=V%C3%A1zquez%2C+J+A">J. Alberto Vázquez</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods
                        for Astrophysics (astro-ph.IM)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO);
                    Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

                </div>
                <p class="mathjax">In this paper, we present a novel approach to accelerate the Bayesian
                    inference process, focusing specifically on the nested sampling algorithms.
                    Bayesian inference plays a crucial role in cosmological parameter estimation,
                    providing a robust framework for extracting theoretical insights from
                    observational data. However, its computational demands can be substantial,
                    primarily due to the need for numerous likelihood function evaluations. Our
                    proposed method utilizes the power of deep learning, employing feedforward
                    neural networks to approximate the likelihood function dynamically during the
                    Bayesian inference process. Unlike traditional approaches, our method trains
                    neural networks on-the-fly using the current set of live points as training
                    data, without the need for pre-training. This flexibility enables adaptation to
                    various theoretical models and datasets. We perform simple hyperparameter
                    optimization using genetic algorithms to suggest initial neural network
                    architectures for learning each likelihood function. Once sufficient accuracy
                    is achieved, the neural network replaces the original likelihood function. The
                    implementation integrates with nested sampling algorithms and has been
                    thoroughly evaluated using both simple cosmological dark energy models and
                    diverse observational datasets. Additionally, we explore the potential of
                    genetic algorithms for generating initial live points within nested sampling
                    inference, opening up new avenues for enhancing the efficiency and
                    effectiveness of Bayesian inference methods.
                </p>
            </div>
        </dd>
        <dt><a name="item55">[55]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03329"
                    title="Abstract">arXiv:2405.03329</a> (cross-list from cs.LG) [<a href="/pdf/2405.03329"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03329" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Policy Learning for Balancing Short-Term and Long-Term
                    Rewards
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Wu%2C+P">Peng Wu</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Z">Ziyu Shen</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Xie%2C+F">Feng Xie</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhongyao Wang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chunchen Liu</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+Y">Yan Zeng</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Empirical researchers and decision-makers spanning various domains frequently
                    seek profound insights into the long-term impacts of interventions. While the
                    significance of long-term outcomes is undeniable, an overemphasis on them may
                    inadvertently overshadow short-term gains. Motivated by this, this paper
                    formalizes a new framework for learning the optimal policy that effectively
                    balances both long-term and short-term rewards, where some long-term outcomes
                    are allowed to be missing. In particular, we first present the identifiability
                    of both rewards under mild assumptions. Next, we deduce the semiparametric
                    efficiency bounds, along with the consistency and asymptotic normality of their
                    estimators. We also reveal that short-term outcomes, if associated, contribute
                    to improving the estimator of the long-term reward. Based on the proposed
                    estimators, we develop a principled policy learning approach and further derive
                    the convergence rates of regret and estimation errors associated with the
                    learned policy. Extensive experiments are conducted to validate the
                    effectiveness of the proposed method, demonstrating its practical
                    applicability.
                </p>
            </div>
        </dd>
        <dt><a name="item56">[56]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03381"
                    title="Abstract">arXiv:2405.03381</a> (cross-list from cs.CV) [<a href="/pdf/2405.03381"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03381" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Statistical Edge Detection And UDF Learning For Shape
                    Representation
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Foy%2C+V">Virgile Foy</a> (IMT),
                    <a href="/search/cs?searchtype=author&amp;query=Gamboa%2C+F">Fabrice Gamboa</a> (IMT),
                    <a href="/search/cs?searchtype=author&amp;query=Chhaibi%2C+R">Reda Chhaibi</a> (IMT)
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern
                        Recognition (cs.CV)</span>; Machine Learning (cs.LG); Applications (stat.AP)

                </div>
                <p class="mathjax">In the field of computer vision, the numerical encoding of 3D surfaces is
                    crucial. It is classical to represent surfaces with their Signed Distance
                    Functions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like
                    representation learning, surface classification, or surface reconstruction,
                    this function can be learned by a neural network, called Neural Distance
                    Function. This network, and in particular its weights, may serve as a
                    parametric and implicit representation for the surface. The network must
                    represent the surface as accurately as possible. In this paper, we propose a
                    method for learning UDFs that improves the fidelity of the obtained Neural UDF
                    to the original 3D surface. The key idea of our method is to concentrate the
                    learning effort of the Neural UDF on surface edges. More precisely, we show
                    that sampling more training points around surface edges allows better local
                    accuracy of the trained Neural UDF, and thus improves the global expressiveness
                    of the Neural UDF in terms of Hausdorff distance. To detect surface edges, we
                    propose a new statistical method based on the calculation of a <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-78"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.52em, 2.26em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-79"><span class="mi" id="MathJax-Span-80"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-17">p</script>-value at
                    each point on the surface. Our method is shown to detect surface edges more
                    accurately than a commonly used local geometric descriptor.
                </p>
            </div>
        </dd>
        <dt><a name="item57">[57]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03402"
                    title="Abstract">arXiv:2405.03402</a> (cross-list from q-fin.ST) [<a href="/pdf/2405.03402"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03402" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Distributional Reference Class Forecasting of Corporate Sales
                    Growth With Multiple Reference Variables
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/q-fin?searchtype=author&amp;query=Theising%2C+E">Etienne Theising</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 52 pages, 11 figures, 19 tables
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance
                        (q-fin.ST)</span>; Applications (stat.AP)

                </div>
                <p class="mathjax">This paper introduces an approach to reference class selection in
                    distributional forecasting with an application to corporate sales growth rates
                    using several co-variates as reference variables, that are implicit predictors.
                    The method can be used to detect expert or model-based forecasts exposed to
                    (behavioral) bias or to forecast distributions with reference classes. These
                    are sets of similar entities, here firms, and rank based algorithms for their
                    selection are proposed, including an optional preprocessing data dimension
                    reduction via principal components analysis. Forecasts are optimal if they
                    match the underlying distribution as closely as possible. Probability integral
                    transform values rank the forecast capability of different reference variable
                    sets and algorithms in a backtest on a data set of 21,808 US firms over the
                    time period 1950 - 2019. In particular, algorithms on dimension reduced
                    variables perform well using contemporaneous balance sheet and financial market
                    parameters along with past sales growth rates and past operating margins
                    changes. Comparisions of actual analysts' estimates to distributional forecasts
                    and of historic distributional forecasts to realized sales growth illustrate
                    the practical use of the method.
                </p>
            </div>
        </dd>
        <dt><a name="item58">[58]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03449"
                    title="Abstract">arXiv:2405.03449</a> (cross-list from cs.LG) [<a href="/pdf/2405.03449"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03449" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Byzantine-Robust Gossip: Insights from a Dual Approach
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Gaucher%2C+R">Renaud Gaucher</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Hendrikx%2C+H">Hadrien Hendrikx</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Dieuleveut%2C+A">Aymeric Dieuleveut</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 9 pages, 1 figure
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">Distributed approaches have many computational benefits, but they are
                    vulnerable to attacks from a subset of devices transmitting incorrect
                    information. This paper investigates Byzantine-resilient algorithms in a
                    decentralized setting, where devices communicate directly with one another. We
                    leverage the so-called dual approach to design a general robust decentralized
                    optimization method. We provide both global and local clipping rules in the
                    special case of average consensus, with tight convergence guarantees. These
                    clipping rules are practical, and yield results that finely characterize the
                    impact of Byzantine nodes, highlighting for instance a qualitative difference
                    in convergence between global and local clipping thresholds. Lastly, we
                    demonstrate that they can serve as a basis for designing efficient attacks.
                </p>
            </div>
        </dd>
        <dt><a name="item59">[59]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03624"
                    title="Abstract">arXiv:2405.03624</a> (cross-list from cs.LG) [<a href="/pdf/2405.03624"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.03624" title="Download PostScript">ps</a>, <a
                    href="/format/2405.03624" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-81"
                                style="width: 0.558em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.465em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.345em, 1000.42em, 2.086em, -999.998em); top: -1.942em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-82"><span class="texatom"
                                                id="MathJax-Span-83"><span class="mrow" id="MathJax-Span-84"><span
                                                        class="mo" id="MathJax-Span-85"
                                                        style="font-family: MathJax_Math-italic;">ε</span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.947em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.053em; border-left: 0px solid; width: 0px; height: 0.669em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-18">ε</script>-Policy Gradient for Online Pricing
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Szpruch%2C+L">Lukasz Szpruch</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Treetanthiploet%2C+T">Tanut Treetanthiploet</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yufei Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Optimization and Control (math.OC); Statistical Finance (q-fin.ST); Machine
                    Learning (stat.ML)

                </div>
                <p class="mathjax">Combining model-based and model-free reinforcement learning approaches, this
                    paper proposes and analyzes an <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-86"
                                style="width: 0.524em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.408em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.334em, 1000.41em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-87"><span class="mi" id="MathJax-Span-88"
                                                style="font-family: MathJax_Math-italic;">ϵ</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-19">\epsilon</script>-policy gradient algorithm for the
                    online pricing learning task. The algorithm extends <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-89"
                                style="width: 0.524em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.408em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.334em, 1000.41em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-90"><span class="mi" id="MathJax-Span-91"
                                                style="font-family: MathJax_Math-italic;">ϵ</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-20">\epsilon</script>-greedy algorithm
                    by replacing greedy exploitation with gradient descent step and facilitates
                    learning via model inference. We optimize the regret of the proposed algorithm
                    by quantifying the exploration cost in terms of the exploration probability
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-21-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-92"
                                style="width: 0.524em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.408em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.334em, 1000.41em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-93"><span class="mi" id="MathJax-Span-94"
                                                style="font-family: MathJax_Math-italic;">ϵ</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-21">\epsilon</script> and the exploitation cost in terms
                    of the gradient descent
                    optimization and gradient estimation errors. The algorithm achieves an expected
                    regret of order <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-22-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-95"
                                style="width: 3.764em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.102em, 1003.01em, 2.549em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-96"><span class="texatom"
                                                id="MathJax-Span-97"><span class="mrow" id="MathJax-Span-98"><span
                                                        class="mi" id="MathJax-Span-99"
                                                        style="font-family: MathJax_Caligraphic;">O</span></span></span><span
                                                class="mo" id="MathJax-Span-100"
                                                style="font-family: MathJax_Main;">(</span><span class="msqrt"
                                                id="MathJax-Span-101"><span
                                                    style="display: inline-block; position: relative; width: 1.565em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.7em, 4.17em, -999.997em); top: -3.99em; left: 0.813em;"><span
                                                            class="mrow" id="MathJax-Span-102"><span class="mi"
                                                                id="MathJax-Span-103"
                                                                style="font-family: MathJax_Math-italic;">T<span
                                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.119em;"></span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; clip: rect(3.533em, 1000.75em, 3.938em, -999.997em); top: -4.569em; left: 0.813em;"><span
                                                            style="display: inline-block; position: relative; width: 0.755em; height: 0px;"><span
                                                                style="position: absolute; font-family: MathJax_Main; top: -3.99em; left: -0.055em;">−<span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                                style="position: absolute; font-family: MathJax_Main; top: -3.99em; left: 0.061em;">−<span
                                                                    style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; clip: rect(3.012em, 1000.87em, 4.343em, -999.997em); top: -4.048em; left: 0em;"><span
                                                            style="font-family: MathJax_Main;">√</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span><span
                                                class="mo" id="MathJax-Span-104"
                                                style="font-family: MathJax_Main;">)</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.462em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-22">\mathcal{O}(\sqrt{T})</script> (up to a logarithmic
                    factor) over <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-23-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-105"
                                style="width: 0.871em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.697em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.7em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-106"><span class="mi" id="MathJax-Span-107"
                                                style="font-family: MathJax_Math-italic;">T<span
                                                    style="display: inline-block; overflow: hidden; height: 1px; width: 0.119em;"></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-23">T</script>
                    trials.
                </p>
            </div>
        </dd>
        <dt><a name="item60">[60]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.03664"
                    title="Abstract">arXiv:2405.03664</a> (cross-list from cs.LG) [<a href="/pdf/2405.03664"
                    title="Download PDF">pdf</a>, <a href="/format/2405.03664" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A New Robust Partial <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-108"
                                style="width: 0.604em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.512em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.345em, 1000.51em, 2.271em, -999.998em); top: -1.942em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-109"><span class="mi" id="MathJax-Span-110"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.947em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 0.892em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-24">p</script>-Wasserstein-Based Metric for Comparing
                    Distributions
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Raghvendra%2C+S">Sharath Raghvendra</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Shirzadian%2C+P">Pouyan Shirzadian</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kaiyi Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
                <p class="mathjax">The <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-25-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-111"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-112"><span class="mn" id="MathJax-Span-113"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-25">2</script>-Wasserstein distance is sensitive to
                    minor geometric differences
                    between distributions, making it a very powerful dissimilarity metric. However,
                    due to this sensitivity, a small outlier mass can also cause a significant
                    increase in the <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-26-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-114"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-115"><span class="mn" id="MathJax-Span-116"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-26">2</script>-Wasserstein distance between two similar
                    distributions.
                    Similarly, sampling discrepancy can cause the empirical <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-117"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-118"><span class="mn" id="MathJax-Span-119"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-27">2</script>-Wasserstein
                    distance on <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-28-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-120"
                                style="width: 0.697em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.582em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.58em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-121"><span class="mi" id="MathJax-Span-122"
                                                style="font-family: MathJax_Math-italic;">n</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-28">n</script> samples in <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-123"
                                style="width: 1.392em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 1.16em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1001.16em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-124"><span class="msubsup"
                                                id="MathJax-Span-125"><span
                                                    style="display: inline-block; position: relative; width: 1.16em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.7em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="texatom" id="MathJax-Span-126"><span class="mrow"
                                                                id="MathJax-Span-127"><span class="mi"
                                                                    id="MathJax-Span-128"
                                                                    style="font-family: MathJax_AMS;">R</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.395em; left: 0.697em;"><span
                                                            class="mn" id="MathJax-Span-129"
                                                            style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.184em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-29">\mathbb{R}^2</script> to converge to the true
                    distance at a
                    rate of <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-30-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-130"
                                style="width: 2.723em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1002.26em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-131"><span class="msubsup"
                                                id="MathJax-Span-132"><span
                                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.359em, 1000.58em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-133"
                                                            style="font-family: MathJax_Math-italic;">n</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.582em;"><span
                                                            class="texatom" id="MathJax-Span-134"><span class="mrow"
                                                                id="MathJax-Span-135"><span class="mo"
                                                                    id="MathJax-Span-136"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span
                                                                    class="mn" id="MathJax-Span-137"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                                    class="texatom" id="MathJax-Span-138"><span
                                                                        class="mrow" id="MathJax-Span-139"><span
                                                                            class="mo" id="MathJax-Span-140"
                                                                            style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span
                                                                    class="mn" id="MathJax-Span-141"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-30">n^{-1/4}</script>, which is significantly slower
                    than the rate of <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-31-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-142"
                                style="width: 2.723em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1002.26em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-143"><span class="msubsup"
                                                id="MathJax-Span-144"><span
                                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.359em, 1000.58em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-145"
                                                            style="font-family: MathJax_Math-italic;">n</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.582em;"><span
                                                            class="texatom" id="MathJax-Span-146"><span class="mrow"
                                                                id="MathJax-Span-147"><span class="mo"
                                                                    id="MathJax-Span-148"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span
                                                                    class="mn" id="MathJax-Span-149"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                                    class="texatom" id="MathJax-Span-150"><span
                                                                        class="mrow" id="MathJax-Span-151"><span
                                                                            class="mo" id="MathJax-Span-152"
                                                                            style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span
                                                                    class="mn" id="MathJax-Span-153"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">2</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-31">n^{-1/2}</script>
                    for <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-32-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-154"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-155"><span class="mn" id="MathJax-Span-156"
                                                style="font-family: MathJax_Main;">1</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-32">1</script>-Wasserstein distance.
                    <br>We introduce a new family of distances parameterized by <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-157"
                                style="width: 2.896em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.376em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1002.32em, 2.433em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-158"><span class="mi" id="MathJax-Span-159"
                                                style="font-family: MathJax_Math-italic;">k</span><span class="mo"
                                                id="MathJax-Span-160"
                                                style="font-family: MathJax_Main; padding-left: 0.292em;">≥</span><span
                                                class="mn" id="MathJax-Span-161"
                                                style="font-family: MathJax_Main; padding-left: 0.292em;">0</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.205em; border-left: 0px solid; width: 0px; height: 1.115em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-33">k \ge 0</script>, called
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-34-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-162"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-163"><span class="mi" id="MathJax-Span-164"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-34">k</script>-RPW, that is based on computing the
                    partial <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-35-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-165"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-166"><span class="mn" id="MathJax-Span-167"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-35">2</script>-Wasserstein distance. We
                    show that (1) <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-36-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-168"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-169"><span class="mi" id="MathJax-Span-170"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-36">k</script>-RPW satisfies the metric properties, (2)
                    <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-37-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-171"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-172"><span class="mi" id="MathJax-Span-173"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-37">k</script>-RPW is robust to
                    small outlier mass while retaining the sensitivity of <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-174"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-175"><span class="mn" id="MathJax-Span-176"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-38">2</script>-Wasserstein distance
                    to minor geometric differences, and (3) when <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-39-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-177"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-178"><span class="mi" id="MathJax-Span-179"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-39">k</script> is a constant, <span
                        class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-40-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-180"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-181"><span class="mi" id="MathJax-Span-182"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-40">k</script>-RPW
                    distance between empirical distributions on <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-41-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-183"
                                style="width: 0.697em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.582em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.58em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-184"><span class="mi" id="MathJax-Span-185"
                                                style="font-family: MathJax_Math-italic;">n</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.698em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-41">n</script> samples in <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-42-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-186"
                                style="width: 1.392em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 1.16em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1001.16em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-187"><span class="msubsup"
                                                id="MathJax-Span-188"><span
                                                    style="display: inline-block; position: relative; width: 1.16em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.128em, 1000.7em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="texatom" id="MathJax-Span-189"><span class="mrow"
                                                                id="MathJax-Span-190"><span class="mi"
                                                                    id="MathJax-Span-191"
                                                                    style="font-family: MathJax_AMS;">R</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.395em; left: 0.697em;"><span
                                                            class="mn" id="MathJax-Span-192"
                                                            style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.184em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-42">\mathbb{R}^2</script>
                    converges to the true distance at a rate of <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-43-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-193"
                                style="width: 2.723em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1002.26em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-194"><span class="msubsup"
                                                id="MathJax-Span-195"><span
                                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.359em, 1000.58em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-196"
                                                            style="font-family: MathJax_Math-italic;">n</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.582em;"><span
                                                            class="texatom" id="MathJax-Span-197"><span class="mrow"
                                                                id="MathJax-Span-198"><span class="mo"
                                                                    id="MathJax-Span-199"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span
                                                                    class="mn" id="MathJax-Span-200"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                                    class="texatom" id="MathJax-Span-201"><span
                                                                        class="mrow" id="MathJax-Span-202"><span
                                                                            class="mo" id="MathJax-Span-203"
                                                                            style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span
                                                                    class="mn" id="MathJax-Span-204"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">3</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-43">n^{-1/3}</script>, which is faster than
                    the convergence rate of <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-44-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-205"
                                style="width: 2.723em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(-0.055em, 1002.26em, 1.16em, -999.997em); top: -0.981em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-206"><span class="msubsup"
                                                id="MathJax-Span-207"><span
                                                    style="display: inline-block; position: relative; width: 2.26em; height: 0px;"><span
                                                        style="position: absolute; clip: rect(3.359em, 1000.58em, 4.17em, -999.997em); top: -3.99em; left: 0em;"><span
                                                            class="mi" id="MathJax-Span-208"
                                                            style="font-family: MathJax_Math-italic;">n</span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span><span
                                                        style="position: absolute; top: -4.337em; left: 0.582em;"><span
                                                            class="texatom" id="MathJax-Span-209"><span class="mrow"
                                                                id="MathJax-Span-210"><span class="mo"
                                                                    id="MathJax-Span-211"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span
                                                                    class="mn" id="MathJax-Span-212"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span
                                                                    class="texatom" id="MathJax-Span-213"><span
                                                                        class="mrow" id="MathJax-Span-214"><span
                                                                            class="mo" id="MathJax-Span-215"
                                                                            style="font-size: 70.7%; font-family: MathJax_Main;">/</span></span></span><span
                                                                    class="mn" id="MathJax-Span-216"
                                                                    style="font-size: 70.7%; font-family: MathJax_Main;">4</span></span></span><span
                                                            style="display: inline-block; width: 0px; height: 3.996em;"></span></span></span></span></span><span
                                            style="display: inline-block; width: 0px; height: 0.987em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 1.253em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-44">n^{-1/4}</script> for the <span
                        class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-45-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-217"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-218"><span class="mn" id="MathJax-Span-219"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-45">2</script>-Wasserstein distance.
                    <br>Using the partial <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-46-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-220"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.52em, 2.26em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-221"><span class="mi" id="MathJax-Span-222"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-46">p</script>-Wasserstein distance, we extend our
                    distance to any <span class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-47-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-223"
                                style="width: 5.153em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 4.285em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.218em, 1004.17em, 2.549em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-224"><span class="mi" id="MathJax-Span-225"
                                                style="font-family: MathJax_Math-italic;">p</span><span class="mo"
                                                id="MathJax-Span-226"
                                                style="font-family: MathJax_Main; padding-left: 0.292em;">∈</span><span
                                                class="mo" id="MathJax-Span-227"
                                                style="font-family: MathJax_Main; padding-left: 0.292em;">[</span><span
                                                class="mn" id="MathJax-Span-228"
                                                style="font-family: MathJax_Main;">1</span><span class="mo"
                                                id="MathJax-Span-229" style="font-family: MathJax_Main;">,</span><span
                                                class="mi" id="MathJax-Span-230"
                                                style="font-family: MathJax_Main; padding-left: 0.177em;">∞</span><span
                                                class="mo" id="MathJax-Span-231"
                                                style="font-family: MathJax_Main;">]</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.344em; border-left: 0px solid; width: 0px; height: 1.323em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-47">p
    \in [1,\infty]</script>. By setting parameters <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-48-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-232"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.045em, 1000.52em, 2.086em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-233"><span class="mi" id="MathJax-Span-234"
                                                style="font-family: MathJax_Math-italic;">k</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-48">k</script> or <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-49-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-235"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.52em, 2.26em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-236"><span class="mi" id="MathJax-Span-237"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-49">p</script> appropriately, we can reduce
                    our distance to the total variation, <span class="MathJax_Preview"
                        style="display: none;"></span><span class="MathJax" id="MathJax-Element-50-Frame" tabindex="0"
                        style="">
                        <nobr><span class="math" id="MathJax-Span-238"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.52em, 2.26em, -999.997em); top: -1.907em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-239"><span class="mi" id="MathJax-Span-240"
                                                style="font-family: MathJax_Math-italic;">p</span></span><span
                                            style="display: inline-block; width: 0px; height: 1.913em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.274em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-50">p</script>-Wasserstein, and the L\'evy-Prokhorov
                    distances. Experiments show that our distance function achieves higher accuracy
                    in comparison to the <span class="MathJax_Preview" style="display: none;"></span><span
                        class="MathJax" id="MathJax-Element-51-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-241"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-242"><span class="mn" id="MathJax-Span-243"
                                                style="font-family: MathJax_Main;">1</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-51">1</script>-Wasserstein, <span
                        class="MathJax_Preview" style="display: none;"></span><span class="MathJax"
                        id="MathJax-Element-52-Frame" tabindex="0" style="">
                        <nobr><span class="math" id="MathJax-Span-244"
                                style="width: 0.639em; display: inline-block;"><span
                                    style="display: inline-block; position: relative; width: 0.524em; height: 0px; font-size: 120%;"><span
                                        style="position: absolute; clip: rect(1.276em, 1000.47em, 2.318em, -999.997em); top: -2.138em; left: 0em;"><span
                                            class="mrow" id="MathJax-Span-245"><span class="mn" id="MathJax-Span-246"
                                                style="font-family: MathJax_Main;">2</span></span><span
                                            style="display: inline-block; width: 0px; height: 2.144em;"></span></span></span><span
                                    style="display: inline-block; overflow: hidden; vertical-align: -0.066em; border-left: 0px solid; width: 0px; height: 0.976em;"></span></span>
                        </nobr>
                    </span>
                    <script type="math/tex" id="MathJax-Element-52">2</script>-Wasserstein, and TV distances for
                    image retrieval tasks on noisy real-world data sets.
                </p>
            </div>
        </dd>
    </dl>
    <h3>Replacements for Tue, 7 May 24</h3>
    <dl>
        <dt><a name="item61">[61]</a>&nbsp; <span class="list-identifier"><a href="/abs/2008.01503"
                    title="Abstract">arXiv:2008.01503</a> (replaced) [<a href="/pdf/2008.01503"
                    title="Download PDF">pdf</a>, <a href="/format/2008.01503" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Multiple Code Hashing for Efficient Image Retrieval
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Ming-Wei Li</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Q">Qing-Yuan Jiang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wu-Jun Li</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 12 pages, 9 figures, 3 tables
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item62">[62]</a>&nbsp; <span class="list-identifier"><a href="/abs/2009.09538"
                    title="Abstract">arXiv:2009.09538</a> (replaced) [<a href="/pdf/2009.09538"
                    title="Download PDF">pdf</a>, <a href="/format/2009.09538" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Regret Bounds and Reinforcement Learning Exploration of
                    EXP-based Algorithms
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Mengfan Xu</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Klabjan%2C+D">Diego Klabjan</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 40 pages, 8 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item63">[63]</a>&nbsp; <span class="list-identifier"><a href="/abs/2110.13452"
                    title="Abstract">arXiv:2110.13452</a> (replaced) [<a href="/pdf/2110.13452"
                    title="Download PDF">pdf</a>, <a href="/format/2110.13452" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> On the Optimization Landscape of Maximum Mean Discrepancy
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Alon%2C+I">Itai Alon</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Globerson%2C+A">Amir Globerson</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Wiesel%2C+A">Ami Wiesel</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item64">[64]</a>&nbsp; <span class="list-identifier"><a href="/abs/2111.11694"
                    title="Abstract">arXiv:2111.11694</a> (replaced) [<a href="/pdf/2111.11694"
                    title="Download PDF">pdf</a>, <a href="/ps/2111.11694" title="Download PostScript">ps</a>, <a
                    href="/format/2111.11694" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> MARS via LASSO
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Ki%2C+D">Dohyeong Ki</a>,
                    <a href="/search/math?searchtype=author&amp;query=Fang%2C+B">Billy Fang</a>,
                    <a href="/search/math?searchtype=author&amp;query=Guntuboyina%2C+A">Adityanand Guntuboyina</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item65">[65]</a>&nbsp; <span class="list-identifier"><a href="/abs/2202.05420"
                    title="Abstract">arXiv:2202.05420</a> (replaced) [<a href="/pdf/2202.05420"
                    title="Download PDF">pdf</a>, <a href="/format/2202.05420" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A Characterization of Semi-Supervised Adversarially-Robust
                    PAC Learnability
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Attias%2C+I">Idan Attias</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Hanneke%2C+S">Steve Hanneke</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Mansour%2C+Y">Yishay Mansour</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> NeurIPS 2022 camera-ready
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item66">[66]</a>&nbsp; <span class="list-identifier"><a href="/abs/2203.07831"
                    title="Abstract">arXiv:2203.07831</a> (replaced) [<a href="/pdf/2203.07831"
                    title="Download PDF">pdf</a>, <a href="/format/2203.07831" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Graph Convolutional Neural Networks Sensitivity under
                    Probabilistic Error Model
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Wang%2C+X">Xinjue Wang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Ollila%2C+E">Esa Ollila</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Vorobyov%2C+S+A">Sergiy A. Vorobyov</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
            </div>
        </dd>
        <dt><a name="item67">[67]</a>&nbsp; <span class="list-identifier"><a href="/abs/2204.04099"
                    title="Abstract">arXiv:2204.04099</a> (replaced) [<a href="/pdf/2204.04099"
                    title="Download PDF">pdf</a>, <a href="/format/2204.04099" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Seeded graph matching for the correlated Gaussian Wigner
                    model via the projected power method
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Araya%2C+E">Ernesto Araya</a>,
                    <a href="/search/math?searchtype=author&amp;query=Braun%2C+G">Guillaume Braun</a>,
                    <a href="/search/math?searchtype=author&amp;query=Tyagi%2C+H">Hemant Tyagi</a>
                </div>
                <div class="list-journal-ref">
                    <span class="descriptor">Journal-ref:</span> Journal of Machine Learning Research 25(5), 1-43, 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item68">[68]</a>&nbsp; <span class="list-identifier"><a href="/abs/2205.05505"
                    title="Abstract">arXiv:2205.05505</a> (replaced) [<a href="/pdf/2205.05505"
                    title="Download PDF">pdf</a>, <a href="/format/2205.05505" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Probability Distribution of Hypervolume Improvement in
                    Bi-objective Bayesian Optimization
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hao Wang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Yang%2C+K">Kaifeng Yang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Affenzeller%2C+M">Michael Affenzeller</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item69">[69]</a>&nbsp; <span class="list-identifier"><a href="/abs/2206.09821"
                    title="Abstract">arXiv:2206.09821</a> (replaced) [<a href="/pdf/2206.09821"
                    title="Download PDF">pdf</a>, <a href="/format/2206.09821" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Exceedance Probability Forecasting via Regression for
                    Significant Wave Height Prediction
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Cerqueira%2C+V">Vitor Cerqueira</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Torgo%2C+L">Luis Torgo</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
            </div>
        </dd>
        <dt><a name="item70">[70]</a>&nbsp; <span class="list-identifier"><a href="/abs/2206.12977"
                    title="Abstract">arXiv:2206.12977</a> (replaced) [<a href="/pdf/2206.12977"
                    title="Download PDF">pdf</a>, <a href="/ps/2206.12977" title="Download PostScript">ps</a>, <a
                    href="/format/2206.12977" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Adversarially Robust PAC Learnability of Real-Valued
                    Functions
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Attias%2C+I">Idan Attias</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Hanneke%2C+S">Steve Hanneke</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> accepted to ICML2023
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item71">[71]</a>&nbsp; <span class="list-identifier"><a href="/abs/2301.02426"
                    title="Abstract">arXiv:2301.02426</a> (replaced) [<a href="/pdf/2301.02426"
                    title="Download PDF">pdf</a>, <a href="/format/2301.02426" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Reversibility of elliptical slice sampling revisited
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Hasenpflug%2C+M">Mareike Hasenpflug</a>,
                    <a href="/search/math?searchtype=author&amp;query=Telezhnikov%2C+V">Viacheslav Telezhnikov</a>,
                    <a href="/search/math?searchtype=author&amp;query=Rudolf%2C+D">Daniel Rudolf</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 25 pages
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item72">[72]</a>&nbsp; <span class="list-identifier"><a href="/abs/2302.10128"
                    title="Abstract">arXiv:2302.10128</a> (replaced) [<a href="/pdf/2302.10128"
                    title="Download PDF">pdf</a>, <a href="/format/2302.10128" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Sketch In, Sketch Out: Accelerating both Learning and
                    Inference for Structured Prediction with Kernels
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Ahmad%2C+T+E">Tamim El Ahmad</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Brogat-Motte%2C+L">Luc Brogat-Motte</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Laforgue%2C+P">Pierre Laforgue</a>,
                    <a href="/search/stat?searchtype=author&amp;query=d%27Alch%C3%A9-Buc%2C+F">Florence d'Alché-Buc</a>
                </div>
                <div class="list-journal-ref">
                    <span class="descriptor">Journal-ref:</span> Proceedings of The 27th International Conference on
                    Artificial
                    Intelligence and Statistics, PMLR 238:109-117, 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
            </div>
        </dd>
        <dt><a name="item73">[73]</a>&nbsp; <span class="list-identifier"><a href="/abs/2302.12728"
                    title="Abstract">arXiv:2302.12728</a> (replaced) [<a href="/pdf/2302.12728"
                    title="Download PDF">pdf</a>, <a href="/ps/2302.12728" title="Download PostScript">ps</a>, <a
                    href="/format/2302.12728" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Statistical Principles for Platform Trials
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Cui%2C+X">Xinping Cui</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Ouyang%2C+E">Emily Ouyang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Liu%2C+Y">Yi Liu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Schneider%2C+J">Jingjing Schneider</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Tian%2C+H">Hong Tian</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Wang%2C+B">Bushi Wang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Hsu%2C+J+C">Jason C. Hsu</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item74">[74]</a>&nbsp; <span class="list-identifier"><a href="/abs/2304.13794"
                    title="Abstract">arXiv:2304.13794</a> (replaced) [<a href="/pdf/2304.13794"
                    title="Download PDF">pdf</a>, <a href="/format/2304.13794" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Hölder regularity and roughness: construction and examples
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Bayraktar%2C+E">Erhan Bayraktar</a>,
                    <a href="/search/math?searchtype=author&amp;query=Das%2C+P">Purba Das</a>,
                    <a href="/search/math?searchtype=author&amp;query=Kim%2C+D">Donghan Kim</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted for publication at Bernoulli
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Probability
                        (math.PR)</span>; Applications (stat.AP)

                </div>
            </div>
        </dd>
        <dt><a name="item75">[75]</a>&nbsp; <span class="list-identifier"><a href="/abs/2305.06262"
                    title="Abstract">arXiv:2305.06262</a> (replaced) [<a href="/pdf/2305.06262"
                    title="Download PDF">pdf</a>, <a href="/format/2305.06262" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Flexible cost-penalized Bayesian model selection: developing
                    inclusion paths with an application to diagnosis of heart disease
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Porter%2C+E+M">Erica M. Porter</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Franck%2C+C+T">Christopher T. Franck</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Adams%2C+S">Stephen Adams</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item76">[76]</a>&nbsp; <span class="list-identifier"><a href="/abs/2306.02568"
                    title="Abstract">arXiv:2306.02568</a> (replaced) [<a href="/pdf/2306.02568"
                    title="Download PDF">pdf</a>, <a href="/format/2306.02568" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Latent Optimal Paths by Gumbel Propagation for Variational
                    Bayesian Dynamic Programming
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Niu%2C+X">Xinlei Niu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Walder%2C+C">Christian Walder</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+J">Jing Zhang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Martin%2C+C+P">Charles Patrick Martin</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted by ICML 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG)

                </div>
            </div>
        </dd>
        <dt><a name="item77">[77]</a>&nbsp; <span class="list-identifier"><a href="/abs/2306.10614"
                    title="Abstract">arXiv:2306.10614</a> (replaced) [<a href="/pdf/2306.10614"
                    title="Download PDF">pdf</a>, <a href="/format/2306.10614" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Identifiable causal inference with noisy treatment and no
                    side information
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=P%C3%B6ll%C3%A4nen%2C+A">Antti Pöllänen</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Marttinen%2C+P">Pekka Marttinen</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 18 pages, 10 figures. Changes consist of polishing the
                    original version. The experiments and results remain the same
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item78">[78]</a>&nbsp; <span class="list-identifier"><a href="/abs/2306.14761"
                    title="Abstract">arXiv:2306.14761</a> (replaced) [<a href="/pdf/2306.14761"
                    title="Download PDF">pdf</a>, <a href="/ps/2306.14761" title="Download PostScript">ps</a>, <a
                    href="/format/2306.14761" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Doubly ranked tests for grouped functional data
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Meyer%2C+M+J">Mark J. Meyer</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item79">[79]</a>&nbsp; <span class="list-identifier"><a href="/abs/2309.10017"
                    title="Abstract">arXiv:2309.10017</a> (replaced) [<a href="/pdf/2309.10017"
                    title="Download PDF">pdf</a>, <a href="/format/2309.10017" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A Change-Point Approach to Estimating the Proportion of False
                    Null Hypotheses in Multiple Testing
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Kostic%2C+A">Anica Kostic</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Fryzlewicz%2C+P">Piotr Fryzlewicz</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item80">[80]</a>&nbsp; <span class="list-identifier"><a href="/abs/2310.00809"
                    title="Abstract">arXiv:2310.00809</a> (replaced) [<a href="/pdf/2310.00809"
                    title="Download PDF">pdf</a>, <a href="/format/2310.00809" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Towards Causal Foundation Model: on Duality between Causal
                    Inference and Attention
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiaqi Zhang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Jennings%2C+J">Joel Jennings</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Hilmkil%2C+A">Agrin Hilmkil</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Pawlowski%2C+N">Nick Pawlowski</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Cheng Zhang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Ma%2C+C">Chao Ma</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning
                    (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item81">[81]</a>&nbsp; <span class="list-identifier"><a href="/abs/2310.02671"
                    title="Abstract">arXiv:2310.02671</a> (replaced) [<a href="/pdf/2310.02671"
                    title="Download PDF">pdf</a>, <a href="/format/2310.02671" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Beyond Stationarity: Convergence Analysis of Stochastic
                    Softmax Policy Gradient Methods
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Klein%2C+S">Sara Klein</a>,
                    <a href="/search/math?searchtype=author&amp;query=Weissmann%2C+S">Simon Weissmann</a>,
                    <a href="/search/math?searchtype=author&amp;query=D%C3%B6ring%2C+L">Leif Döring</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 54 pages, 2 figures, ICLR 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control
                        (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item82">[82]</a>&nbsp; <span class="list-identifier"><a href="/abs/2310.03789"
                    title="Abstract">arXiv:2310.03789</a> (replaced) [<a href="/pdf/2310.03789"
                    title="Download PDF">pdf</a>, <a href="/format/2310.03789" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Grokking as a First Order Phase Transition in Two Layer
                    Networks
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Rubin%2C+N">Noa Rubin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Seroussi%2C+I">Inbar Seroussi</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Ringel%2C+Z">Zohar Ringel</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning
                    (cs.LG)

                </div>
            </div>
        </dd>
        <dt><a name="item83">[83]</a>&nbsp; <span class="list-identifier"><a href="/abs/2310.19041"
                    title="Abstract">arXiv:2310.19041</a> (replaced) [<a href="/pdf/2310.19041"
                    title="Download PDF">pdf</a>, <a href="/format/2310.19041" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> On Linear Separation Capacity of Self-Supervised
                    Representation Learning
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Wang%2C+S">Shulei Wang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

                </div>
            </div>
        </dd>
        <dt><a name="item84">[84]</a>&nbsp; <span class="list-identifier"><a href="/abs/2311.03309"
                    title="Abstract">arXiv:2311.03309</a> (replaced) [<a href="/pdf/2311.03309"
                    title="Download PDF">pdf</a>, <a href="/format/2311.03309" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Neural Structure Learning with Stochastic Differential
                    Equations
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Wang%2C+B">Benjie Wang</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Jennings%2C+J">Joel Jennings</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Gong%2C+W">Wenbo Gong</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> ICLR 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item85">[85]</a>&nbsp; <span class="list-identifier"><a href="/abs/2311.08629"
                    title="Abstract">arXiv:2311.08629</a> (replaced) [<a href="/pdf/2311.08629"
                    title="Download PDF">pdf</a>, <a href="/format/2311.08629" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Soft Phenotyping for Sepsis via EHR Time-aware Soft
                    Clustering
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Jiang%2C+S">Shiyi Jiang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Gai%2C+X">Xin Gai</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Treggiari%2C+M">Miriam Treggiari</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Stead%2C+W+W">William W. Stead</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhao%2C+Y">Yuankang Zhao</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Page%2C+C+D">C. David Page</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+A+R">Anru R. Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item86">[86]</a>&nbsp; <span class="list-identifier"><a href="/abs/2311.10452"
                    title="Abstract">arXiv:2311.10452</a> (replaced) [<a href="/pdf/2311.10452"
                    title="Download PDF">pdf</a>, <a href="/format/2311.10452" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Square-Root Higher-Order Unscented Estimators for Robust
                    Orbit Determination
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Yang%2C+Y">Yang Yang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item87">[87]</a>&nbsp; <span class="list-identifier"><a href="/abs/2312.00032"
                    title="Abstract">arXiv:2312.00032</a> (replaced) [<a href="/pdf/2312.00032"
                    title="Download PDF">pdf</a>, <a href="/format/2312.00032" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> An algorithm for forensic toolmark comparisons
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Cuellar%2C+M">Maria Cuellar</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Gao%2C+S">Sheng Gao</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Hofmann%2C+H">Heike Hofmann</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security
                        (cs.CR)</span>; Machine Learning (cs.LG); Applications (stat.AP)

                </div>
            </div>
        </dd>
        <dt><a name="item88">[88]</a>&nbsp; <span class="list-identifier"><a href="/abs/2312.02959"
                    title="Abstract">arXiv:2312.02959</a> (replaced) [<a href="/pdf/2312.02959"
                    title="Download PDF">pdf</a>, <a href="/format/2312.02959" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Detecting algorithmic bias in medical-AI models using trees
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Smith%2C+J">Jeffrey Smith</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Holder%2C+A">Andre Holder</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Kamaleswaran%2C+R">Rishikesan Kamaleswaran</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Xie%2C+Y">Yao Xie</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 26 pages, 9 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Applications
                    (stat.AP)

                </div>
            </div>
        </dd>
        <dt><a name="item89">[89]</a>&nbsp; <span class="list-identifier"><a href="/abs/2312.07520"
                    title="Abstract">arXiv:2312.07520</a> (replaced) [<a href="/pdf/2312.07520"
                    title="Download PDF">pdf</a>, <a href="/format/2312.07520" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Estimating Counterfactual Matrix Means with Short Panel Data
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/econ?searchtype=author&amp;query=Lei%2C+L">Lihua Lei</a>,
                    <a href="/search/econ?searchtype=author&amp;query=Ross%2C+B">Brad Ross</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 72 pages, 6 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics
                        (econ.EM)</span>; Statistics Theory (math.ST); Methodology (stat.ME)

                </div>
            </div>
        </dd>
        <dt><a name="item90">[90]</a>&nbsp; <span class="list-identifier"><a href="/abs/2312.09884"
                    title="Abstract">arXiv:2312.09884</a> (replaced) [<a href="/pdf/2312.09884"
                    title="Download PDF">pdf</a>, <a href="/ps/2312.09884" title="Download PostScript">ps</a>, <a
                    href="/format/2312.09884" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Investigating the heterogeneity of "study twins"
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=R%C3%B6ver%2C+C">Christian Röver</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Friede%2C+T">Tim Friede</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 18 pages, 5 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item91">[91]</a>&nbsp; <span class="list-identifier"><a href="/abs/2401.10566"
                    title="Abstract">arXiv:2401.10566</a> (replaced) [<a href="/pdf/2401.10566"
                    title="Download PDF">pdf</a>, <a href="/format/2401.10566" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Robust Multi-Modal Density Estimation
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=M%C3%A9sz%C3%A1ros%2C+A">Anna Mészáros</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Schumann%2C+J+F">Julian F. Schumann</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zgonnikov%2C+A">Arkady Zgonnikov</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Kober%2C+J">Jens Kober</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item92">[92]</a>&nbsp; <span class="list-identifier"><a href="/abs/2402.08193"
                    title="Abstract">arXiv:2402.08193</a> (replaced) [<a href="/pdf/2402.08193"
                    title="Download PDF">pdf</a>, <a href="/format/2402.08193" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Gaussian Ensemble Belief Propagation for Efficient Inference
                    in High-Dimensional Systems
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=MacKinlay%2C+D">Dan MacKinlay</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Tsuchida%2C+R">Russell Tsuchida</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Pagendam%2C+D">Dan Pagendam</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Kuhnert%2C+P">Petra Kuhnert</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Under conference submission
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item93">[93]</a>&nbsp; <span class="list-identifier"><a href="/abs/2402.13642"
                    title="Abstract">arXiv:2402.13642</a> (replaced) [<a href="/pdf/2402.13642"
                    title="Download PDF">pdf</a>, <a href="/format/2402.13642" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Adaptive Ridge Approach to Heteroscedastic Regression
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Ho%2C+K+L+K">Ka Long Keith Ho</a>,
                    <a href="/search/math?searchtype=author&amp;query=Masuda%2C+H">Hiroki Masuda</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 25 pages, 3 tables, 7 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item94">[94]</a>&nbsp; <span class="list-identifier"><a href="/abs/2402.17366"
                    title="Abstract">arXiv:2402.17366</a> (replaced) [<a href="/pdf/2402.17366"
                    title="Download PDF">pdf</a>, <a href="/ps/2402.17366" title="Download PostScript">ps</a>, <a
                    href="/format/2402.17366" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> The risks of risk assessment: causal blind spots when using
                    prediction models for treatment decisions
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=van+Geloven%2C+N">Nan van Geloven</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Keogh%2C+R+H">Ruth H Keogh</a>,
                    <a href="/search/stat?searchtype=author&amp;query=van+Amsterdam%2C+W">Wouter van Amsterdam</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Cin%C3%A0%2C+G">Giovanni Cinà</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Krijthe%2C+J+H">Jesse H. Krijthe</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Peek%2C+N">Niels Peek</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Luijken%2C+K">Kim Luijken</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Magliacane%2C+S">Sara Magliacane</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Morzywo%C5%82ek%2C+P">Paweł Morzywołek</a>,
                    <a href="/search/stat?searchtype=author&amp;query=van+Ommen%2C+T">Thijs van Ommen</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Putter%2C+H">Hein Putter</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Sperrin%2C+M">Matthew Sperrin</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Wang%2C+J">Junfeng Wang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Weir%2C+D+L">Daniala L. Weir</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Didelez%2C+V">Vanessa Didelez</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item95">[95]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.00540"
                    title="Abstract">arXiv:2403.00540</a> (replaced) [<a href="/pdf/2403.00540"
                    title="Download PDF">pdf</a>, <a href="/format/2403.00540" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Epsilon-Greedy Thompson Sampling to Bayesian Optimization
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Do%2C+B">Bach Do</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Adebiyi%2C+T">Taiwo Adebiyi</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+R">Ruda Zhang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item96">[96]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.01150"
                    title="Abstract">arXiv:2403.01150</a> (replaced) [<a href="/pdf/2403.01150"
                    title="Download PDF">pdf</a>, <a href="/format/2403.01150" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Singularity and Error Analysis of a Simple Quaternion
                    Estimator
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Peng%2C+C">Caitong Peng</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Choukroun%2C+D">Daniel Choukroun</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology
                        (stat.ME)</span>; Systems and Control (eess.SY)

                </div>
            </div>
        </dd>
        <dt><a name="item97">[97]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.03205"
                    title="Abstract">arXiv:2403.03205</a> (replaced) [<a href="/pdf/2403.03205"
                    title="Download PDF">pdf</a>, <a href="/format/2403.03205" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Finding Super-spreaders in Network Cascades
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Mossel%2C+E">Elchanan Mossel</a>,
                    <a href="/search/math?searchtype=author&amp;query=Sridhar%2C+A">Anirudh Sridhar</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 32 pages, 3 figures. Main updates are (1) a relaxation of
                    graph assumptions and (2) a slight sharpening of previous techniques that allows us to estimate the
                    infection time of high-degree vertices from a single cascade
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Information Theory (cs.IT); Social and Information Networks (cs.SI);
                    Probability (math.PR)

                </div>
            </div>
        </dd>
        <dt><a name="item98">[98]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.04873"
                    title="Abstract">arXiv:2403.04873</a> (replaced) [<a href="/pdf/2403.04873"
                    title="Download PDF">pdf</a>, <a href="/format/2403.04873" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> The SIDO Performance Model for League of Legends
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+A+X">Amy X. Zhang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Naidu%2C+P">Parth Naidu</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item99">[99]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.07236"
                    title="Abstract">arXiv:2403.07236</a> (replaced) [<a href="/pdf/2403.07236"
                    title="Download PDF">pdf</a>, <a href="/format/2403.07236" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Partial Identification of Individual-Level Parameters Using
                    Aggregate Data in a Nonparametric Model
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/econ?searchtype=author&amp;query=Moon%2C+S">Sarah Moon</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics
                        (econ.EM)</span>; Methodology (stat.ME)

                </div>
            </div>
        </dd>
        <dt><a name="item100">[100]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.11003"
                    title="Abstract">arXiv:2403.11003</a> (replaced) [<a href="/pdf/2403.11003"
                    title="Download PDF">pdf</a>, <a href="/format/2403.11003" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Extreme Treatment Effect: Extrapolating Dose-Response
                    Function Into Extreme Treatment Domain
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Bodik%2C+J">Juraj Bodik</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item101">[101]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.15400"
                    title="Abstract">arXiv:2403.15400</a> (replaced) [<a href="/pdf/2403.15400"
                    title="Download PDF">pdf</a>, <a href="/format/2403.15400" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Efficient Weighting Schemes for Auditing Instant-Runoff
                    Voting Elections
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Ek%2C+A">Alexander Ek</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Stark%2C+P+B">Philip B. Stark</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Stuckey%2C+P+J">Peter J. Stuckey</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Vukcevic%2C+D">Damjan Vukcevic</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 15 pages, 4, figures, presented at Voting'24. The current
                    version includes some improved wording and fixes a few errors
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society
                        (cs.CY)</span>; Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT);
                    Applications (stat.AP)

                </div>
            </div>
        </dd>
        <dt><a name="item102">[102]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.08472"
                    title="Abstract">arXiv:2404.08472</a> (replaced) [<a href="/pdf/2404.08472"
                    title="Download PDF">pdf</a>, <a href="/format/2404.08472" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> TSLANet: Rethinking Transformers for Time Series
                    Representation Learning
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Eldele%2C+E">Emadeldeen Eldele</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Ragab%2C+M">Mohamed Ragab</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhenghua Chen</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Min Wu</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiaoli Li</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted in ICML 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item103">[103]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.11579"
                    title="Abstract">arXiv:2404.11579</a> (replaced) [<a href="/pdf/2404.11579"
                    title="Download PDF">pdf</a>, <a href="/format/2404.11579" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Spatial Heterogeneous Additive Partial Linear Model: A Joint
                    Approach of Bivariate Spline and Forest Lasso
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+X">Xin Zhang</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Yu%2C+S">Shan Yu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Zhu%2C+Z">Zhengyuan Zhu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

                </div>
            </div>
        </dd>
        <dt><a name="item104">[104]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.18909"
                    title="Abstract">arXiv:2404.18909</a> (replaced) [<a href="/pdf/2404.18909"
                    title="Download PDF">pdf</a>, <a href="/format/2404.18909" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Sample-Efficient Robust Multi-Agent Reinforcement Learning in
                    the Face of Environmental Uncertainty
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Shi%2C+L">Laixi Shi</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Mazumdar%2C+E">Eric Mazumdar</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Chi%2C+Y">Yuejie Chi</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Wierman%2C+A">Adam Wierman</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Accepted by Conference on Neural Information Processing
                    Systems (NeurIPS), 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Multiagent Systems (cs.MA); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item105">[105]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19495"
                    title="Abstract">arXiv:2404.19495</a> (replaced) [<a href="/pdf/2404.19495"
                    title="Download PDF">pdf</a>, <a href="/ps/2404.19495" title="Download PostScript">ps</a>, <a
                    href="/format/2404.19495" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Percentage Coefficient (bp) -- Effect Size Analysis (Theory
                    Paper 1)
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Zhao%2C+X">Xinshu Zhao</a> (1),
                    <a href="/search/stat?searchtype=author&amp;query=Li%2C+D+M">Dianshi Moses Li</a> (2),
                    <a href="/search/stat?searchtype=author&amp;query=Lai%2C+Z+Z">Ze Zack Lai</a> (1),
                    <a href="/search/stat?searchtype=author&amp;query=Liu%2C+P+L">Piper Liping Liu</a> (3),
                    <a href="/search/stat?searchtype=author&amp;query=Ao%2C+S+H">Song Harris Ao</a> (1),
                    <a href="/search/stat?searchtype=author&amp;query=You%2C+F">Fei You</a> (1) ((1) Department of
                    Communication, Faculty of Social Science, University of Macau, (2) Centre for Empirical Legal
                    Studies, Faculty of Law, University of Macau, (3) School of Media and Communication, Shenzhen
                    University)
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>; Econometrics (econ.EM); Methodology (stat.ME); Other Statistics (stat.OT)

                </div>
            </div>
        </dd>
        <dt><a name="item106">[106]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.00118"
                    title="Abstract">arXiv:2405.00118</a> (replaced) [<a href="/pdf/2405.00118"
                    title="Download PDF">pdf</a>, <a href="/format/2405.00118" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Causal Inference with High-dimensional Discrete Covariates
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Zeng%2C+Z">Zhenghao Zeng</a>,
                    <a href="/search/math?searchtype=author&amp;query=Balakrishnan%2C+S">Sivaraman Balakrishnan</a>,
                    <a href="/search/math?searchtype=author&amp;query=Han%2C+Y">Yanjun Han</a>,
                    <a href="/search/math?searchtype=author&amp;query=Kennedy%2C+E+H">Edward H. Kennedy</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 66 pages, 5 figures
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory
                        (math.ST)</span>; Methodology (stat.ME)

                </div>
            </div>
        </dd>
        <dt><a name="item107">[107]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.00727"
                    title="Abstract">arXiv:2405.00727</a> (replaced) [<a href="/pdf/2405.00727"
                    title="Download PDF">pdf</a>, <a href="/format/2405.00727" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Generalised envelope spectrum-based signal-to-noise
                    objectives: Formulation, optimisation and application for gear fault detection under time-varying
                    speed conditions
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/eess?searchtype=author&amp;query=Schmidt%2C+S">Stephan Schmidt</a>,
                    <a href="/search/eess?searchtype=author&amp;query=Wilke%2C+D+N">Daniel N. Wilke</a>,
                    <a href="/search/eess?searchtype=author&amp;query=Gryllias%2C+K+C">Konstantinos C. Gryllias</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 27 pages, 15 figures, tables 1, submitted MSSP review
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing
                        (eess.SP)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

                </div>
            </div>
        </dd>
        <dt><a name="item108">[108]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.00914"
                    title="Abstract">arXiv:2405.00914</a> (replaced) [<a href="/pdf/2405.00914"
                    title="Download PDF">pdf</a>, <a href="/format/2405.00914" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Accelerated Fully First-Order Methods for Bilevel and Minimax
                    Optimization
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/math?searchtype=author&amp;query=Li%2C+C+J">Chris Junchi Li</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Minor typographical updates. arXiv admin note: text
                    overlap with <a href="/abs/2307.00126">arXiv:2307.00126</a>
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control
                        (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item109">[109]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.01196"
                    title="Abstract">arXiv:2405.01196</a> (replaced) [<a href="/pdf/2405.01196"
                    title="Download PDF">pdf</a>, <a href="/format/2405.01196" title="Other formats">other</a>]</span>
        </dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> Decoupling Feature Extraction and Classification Layers for
                    Calibrated Neural Networks
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/cs?searchtype=author&amp;query=Jordahn%2C+M">Mikkel Jordahn</a>,
                    <a href="/search/cs?searchtype=author&amp;query=Olmos%2C+P+M">Pablo M. Olmos</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> Proceedings of the 41 st International Conference on
                    Machine Learning (ICML) 2024
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning
                        (cs.LG)</span>; Machine Learning (stat.ML)

                </div>
            </div>
        </dd>
        <dt><a name="item110">[110]</a>&nbsp; <span class="list-identifier"><a href="/abs/2405.01986"
                    title="Abstract">arXiv:2405.01986</a> (replaced) [<a href="/pdf/2405.01986"
                    title="Download PDF">pdf</a>, <a href="/ps/2405.01986" title="Download PostScript">ps</a>, <a
                    href="/format/2405.01986" title="Other formats">other</a>]</span></dt>
        <dd>
            <div class="meta">
                <div class="list-title mathjax">
                    <span class="descriptor">Title:</span> A comparison of regression models for static and dynamic
                    prediction of a prognostic outcome during admission in electronic health care records
                </div>
                <div class="list-authors">
                    <span class="descriptor">Authors:</span>
                    <a href="/search/stat?searchtype=author&amp;query=Gao%2C+S">Shan Gao</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Albu%2C+E">Elena Albu</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Putter%2C+H">Hein Putter</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Stijnen%2C+P">Pieter Stijnen</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Rademakers%2C+F">Frank Rademakers</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Cossey%2C+V">Veerle Cossey</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Debaveye%2C+Y">Yves Debaveye</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Janssens%2C+C">Christel Janssens</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Van+Calster%2C+B">Ben Van Calster</a>,
                    <a href="/search/stat?searchtype=author&amp;query=Wynants%2C+L">Laure Wynants</a>
                </div>
                <div class="list-comments mathjax">
                    <span class="descriptor">Comments:</span> 3388 words; 3 figures; 4 tables
                </div>
                <div class="list-subjects">
                    <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications
                        (stat.AP)</span>

                </div>
            </div>
        </dd>
    </dl>
    <ul>
        <li><a href="/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
        <li><a href="#item39">Cross-lists</a></li>
        <li><a href="#item61">Replacements</a></li>
    </ul>
    <small>[ total of 110 entries: <b>1-110</b> ]</small><br>
    <small>[ showing up to 2000 entries per page: <a href="/list/stat/new?skip=0&amp;show=1000">fewer</a> | <font
            color="#999999">more</font> ]</small><br>
</div>